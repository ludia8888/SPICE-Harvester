# ğŸ—ï¸ SPICE HARVESTER ì•„í‚¤í…ì²˜ ë¬¸ì„œ
## Event Sourcing + CQRS ê¸°ë°˜ ì—”í„°í”„ë¼ì´ì¦ˆ ë°ì´í„° ê´€ë¦¬ í”Œë«í¼

ì´ ë¬¸ì„œëŠ” SPICE HARVESTERì˜ ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ë³¸ ì‹œìŠ¤í…œì€ **ì´ë²¤íŠ¸ ì†Œì‹±(Event Sourcing) + CQRS** íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ë°ì´í„°ì˜ íŠ¹ì„±ì— ë”°ë¼ **í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ì „ëµ**ì„ ì ìš©í•˜ì—¬ ìµœì ì˜ ì„±ëŠ¥ê³¼ í™•ì¥ì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

---

## ğŸ“‹ ëª©ì°¨

1. [ì•„í‚¤í…ì²˜ ê°œìš”](#1-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [í•µì‹¬ ì„¤ê³„ ì›ì¹™](#2-í•µì‹¬-ì„¤ê³„-ì›ì¹™)
3. [í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì•„í‚¤í…ì²˜](#3-í•˜ì´ë¸Œë¦¬ë“œ-ë°ì´í„°-ì•„í‚¤í…ì²˜)
4. [ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ìƒì„¸](#4-ì‹œìŠ¤í…œ-ì»´í¬ë„ŒíŠ¸-ìƒì„¸)
5. [ë°ì´í„° íë¦„ ë° í†µì‹ ](#5-ë°ì´í„°-íë¦„-ë°-í†µì‹ )
6. [ì €ì¥ì†Œë³„ ì—­í•  ë° ì±…ì„](#6-ì €ì¥ì†Œë³„-ì—­í• -ë°-ì±…ì„)
7. [CQRS êµ¬í˜„ ìƒì„¸](#7-cqrs-êµ¬í˜„-ìƒì„¸)
8. [Event Sourcing êµ¬í˜„ ìƒì„¸](#8-event-sourcing-êµ¬í˜„-ìƒì„¸)
9. [ì„±ëŠ¥ ìµœì í™” ì „ëµ](#9-ì„±ëŠ¥-ìµœì í™”-ì „ëµ)
10. [í™•ì¥ì„± ë° ê³ ê°€ìš©ì„±](#10-í™•ì¥ì„±-ë°-ê³ ê°€ìš©ì„±)

---

## 1. ì•„í‚¤í…ì²˜ ê°œìš”

### ğŸ¯ **ì•„í‚¤í…ì²˜ ì² í•™**

SPICE HARVESTERëŠ” **"ë°ì´í„°ì˜ ëª¨ë“  ë³€í™”ë¥¼ ì¶”ì í•˜ê³  ê´€ë¦¬í•œë‹¤"**ëŠ” í•µì‹¬ ì² í•™ í•˜ì— ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ì•„í‚¤í…ì²˜ ì „ëµì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤:

- **í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ê´€ë¦¬**: ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ìµœì í™”ëœ ì €ì¥ ì „ëµ
- **ì™„ì „í•œ ê°ì‚¬ ì¶”ì **: ëª¨ë“  ë°ì´í„° ë³€ê²½ ì´ë ¥ì˜ ë¶ˆë³€ ê¸°ë¡
- **ì½ê¸°/ì“°ê¸° ë¶„ë¦¬**: CQRSë¥¼ í†µí•œ ì„±ëŠ¥ ìµœì í™”
- **ë¹„ë™ê¸° ì´ë²¤íŠ¸ ê¸°ë°˜**: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ ëŠìŠ¨í•œ ê²°í•©

### ğŸ“Š **ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ - ì´ˆìƒì„¸ êµ¬í˜„ ë‹¤ì´ì–´ê·¸ë¨**

```mermaid
graph TD
    subgraph "ğŸ–¥ï¸ í´ë¼ì´ì–¸íŠ¸ ìƒíƒœê³„"
        subgraph "Frontend Applications"
            REACT_UI[React Web UI<br/>ğŸ“± TypeScript + Blueprint.js<br/>ğŸ¨ GlobalSidebar + Accessibility<br/>ğŸ§ª Vitest + MSW Testing]
            MOBILE_APP[Mobile App<br/>ğŸ“² React Native (Future)]
            DESKTOP_APP[Desktop App<br/>ğŸ–¥ï¸ Electron (Future)]
        end
        
        subgraph "API Consumers"
            EXT_API[External API Clients<br/>ğŸ”Œ REST API Consumers<br/>ğŸ“Š Data Analytics Tools]
            CLI_TOOLS[Command Line Tools<br/>âŒ¨ï¸ spice-cli<br/>ğŸ”§ Admin Scripts]
            WEBHOOKS[Webhook Clients<br/>ğŸª Event Notifications<br/>ğŸ“¨ Integration Partners]
        end
    end

    subgraph "ğŸŒ API Gateway & Load Balancing"
        subgraph "Load Balancer"
            LB[HAProxy/Nginx<br/>âš–ï¸ Load Balancer<br/>ğŸ”’ SSL Termination<br/>ğŸ“Š Health Checks]
        end
        
        subgraph "BFF Cluster (Port 8002-8005)"
            BFF1[BFF Instance 1<br/>ğŸšª Port 8002<br/>ğŸ” JWT Authentication<br/>ğŸŒ CORS Handler<br/>ğŸ“ Request Logging<br/>âš¡ Connection Pool: 50/100]
            BFF2[BFF Instance 2<br/>ğŸšª Port 8003<br/>ğŸ“Š Rate Limiting<br/>ğŸ”„ Circuit Breaker<br/>ğŸ“ˆ Metrics Collection]
            BFF3[BFF Instance 3<br/>ğŸšª Port 8004<br/>ğŸ” Request Validation<br/>ğŸ“‹ Response Standardization<br/>âš¡ Cache Management]
        end
    end

    subgraph "âš¡ Core Services Layer"
        subgraph "OMS Cluster (Port 8000-8001)"
            OMS1[OMS Primary<br/>ğŸšª Port 8000<br/>ğŸ¯ Command Gateway<br/>âœ… 18+ Type Validators<br/>ğŸŒ³ Git Features (7/7)<br/>ğŸ“¦ Outbox Publisher]
            OMS2[OMS Replica<br/>ğŸšª Port 8001<br/>ğŸ”„ Failover Ready<br/>ğŸ“Š Read Queries<br/>âš–ï¸ Load Distribution]
        end
        
        subgraph "AI Services"
            FUNNEL1[Funnel Primary<br/>ğŸšª Port 8004<br/>ğŸ§  AI Type Inference<br/>ğŸ“Š 1,048 Lines Algorithm<br/>ğŸŒ Multi-language (4 langs)<br/>ğŸ“‹ 18+ Complex Types<br/>ğŸ”— Google Sheets Connector]
            FUNNEL2[Funnel Replica<br/>ğŸšª Port 8005<br/>ğŸ”„ ML Model Caching<br/>ğŸ“ˆ Confidence Scoring<br/>âš¡ Batch Processing]
        end
    end

    subgraph "ğŸ“¨ Messaging & Communication Layer"
        subgraph "Transactional Outbox (PostgreSQL)"
            OUTBOX_DB[(PostgreSQL DB<br/>ğŸšª Port 5432<br/>ğŸ“¦ Outbox Pattern<br/>ğŸ”’ ACID Transactions<br/>ğŸ“Š WAL Replication<br/>âš¡ Connection Pool: 20)]
            OUTBOX_TABLE[outbox Table<br/>ğŸ“‹ Schema:<br/>â€¢ id (UUID)<br/>â€¢ aggregate_id<br/>â€¢ event_type<br/>â€¢ payload (JSONB)<br/>â€¢ status<br/>â€¢ created_at<br/>â€¢ published_at]
        end
        
        subgraph "Message Relay Workers"
            RELAY1[Message Relay 1<br/>ğŸ”„ Outbox Poller<br/>â±ï¸ Poll Interval: 1s<br/>ğŸ“¤ Kafka Publisher<br/>ğŸ’¾ State Tracking]
            RELAY2[Message Relay 2<br/>ğŸ”„ Backup Poller<br/>ğŸ”€ Load Balancing<br/>ğŸ“Š Metrics Collection]
        end
        
        subgraph "Kafka Cluster (Port 9092-9094)"
            KAFKA1[Kafka Broker 1<br/>ğŸšª Port 9092<br/>ğŸ“Š Leader Partitions<br/>ğŸ”„ Replication Factor: 3<br/>âš¡ EOS v2 (Exactly-Once)<br/>ğŸ“ˆ Throughput: 500 events/sec]
            KAFKA2[Kafka Broker 2<br/>ğŸšª Port 9093<br/>ğŸ“Š Follower Partitions<br/>ğŸ”„ Auto-Rebalancing<br/>ğŸ“‰ Consumer Lag Monitoring]
            KAFKA3[Kafka Broker 3<br/>ğŸšª Port 9094<br/>ğŸ“Š Follower Partitions<br/>ğŸ›¡ï¸ Data Durability<br/>ğŸ“Š Watermark Tracking]
            
            subgraph "Kafka Topics"
                TOPICS[ğŸ“‹ Topic Configuration:<br/>â€¢ ontology-commands (3 partitions)<br/>â€¢ instance-commands (6 partitions)<br/>â€¢ ontology-events (3 partitions)<br/>â€¢ instance-events (6 partitions)<br/>â€¢ dlq-ontology (1 partition)<br/>â€¢ dlq-instance (1 partition)]
            end
        end
    end

    subgraph "âš™ï¸ Processing Workers Layer"
        subgraph "Instance Workers"
            IW1[Instance Worker 1<br/>ğŸ“ CQRS Command Handler<br/>ğŸ¯ Consumer Group: instance-workers<br/>ğŸ“Š Partition Assignment: 0,1,2<br/>âš¡ Event Sourcing (3-Step):<br/>1ï¸âƒ£ Save to S3 (SSoT)<br/>2ï¸âƒ£ Update TerminusDB<br/>3ï¸âƒ£ Publish Domain Event]
            IW2[Instance Worker 2<br/>ğŸ“ Command Processing<br/>ğŸ¯ Consumer Group: instance-workers<br/>ğŸ“Š Partition Assignment: 3,4,5<br/>ğŸ”„ ThreadPoolExecutor: 2 threads<br/>âš¡ Performance: 50-100 cmd/sec]
        end
        
        subgraph "Ontology Workers" 
            OW1[Ontology Worker 1<br/>ğŸ—‚ï¸ Schema Management<br/>ğŸ¯ Consumer Group: ontology-workers<br/>ğŸ“Š Git Operations (7/7):<br/>â€¢ Branch Management<br/>â€¢ Commit System<br/>â€¢ Diff & Merge<br/>â€¢ Rollback Support<br/>â€¢ History Tracking<br/>â€¢ Pull Requests<br/>â€¢ Conflict Resolution]
            OW2[Ontology Worker 2<br/>ğŸ—‚ï¸ Schema Validation<br/>ğŸ” Complex Type Validation<br/>ğŸ“‹ 18+ Type Handlers<br/>âš¡ Validation Speed: <100ms]
        end
        
        subgraph "Projection Workers"
            PW1[Projection Worker 1<br/>ğŸ“Š Elasticsearch Indexer<br/>ğŸ¯ Consumer Group: projection-workers<br/>ğŸ“‹ Document Types:<br/>â€¢ instances-{db_name}<br/>â€¢ ontologies-{db_name}<br/>â€¢ search-metadata<br/>ğŸ” Full-text Indexing<br/>ğŸŒ Multi-language Analysis]
            PW2[Projection Worker 2<br/>ğŸ“Š Redis Cache Manager<br/>ğŸ’¨ Cache Strategies:<br/>â€¢ L1: In-Memory (5min TTL)<br/>â€¢ L2: Redis (1hr TTL)<br/>â€¢ L3: Database (fallback)<br/>ğŸ”„ Smart Invalidation<br/>âš¡ Hit Ratio: 85%+]
            PW3[Projection Worker 3<br/>ğŸ“Š Metrics & Analytics<br/>ğŸ“ˆ Business Metrics<br/>ğŸ” Query Optimization<br/>ğŸ“Š Performance Tracking]
        end
        
        subgraph "DLQ Handlers"
            DLQ1[DLQ Handler 1<br/>ğŸš¨ Dead Letter Queue<br/>ğŸ”„ Exponential Backoff<br/>âš¡ ThreadPoolExecutor Fix<br/>ğŸ“Š Retry Pattern: 1s, 2s, 4s<br/>ğŸ”§ Poison Message Detection<br/>ğŸ“ˆ Recovery Rate: 5/5 (100%)]
            DLQ2[DLQ Handler 2<br/>ğŸš¨ Backup Handler<br/>ğŸ“Š Metrics Collection<br/>ğŸ”” Alert Generation<br/>ğŸ“‹ Manual Intervention Queue]
        end
    end

    subgraph "ğŸ’¾ ë°ì´í„° ì €ì¥ì†Œ ìƒíƒœê³„"
        subgraph "ğŸ”¥ Single Source of Truth (SSoT)"
            subgraph "S3/MinIO Event Store"
                S3_PRIMARY[S3/MinIO Primary<br/>ğŸšª Port 9000<br/>ğŸ“š Event Store (SSoT)<br/>ğŸ—‚ï¸ Bucket: spice-event-store<br/>ğŸ“‚ Structure:<br/>  ğŸ“ events/year=2024/month=08/<br/>  ğŸ“ snapshots/aggregate={id}/<br/>  ğŸ“ indexes/by_date/<br/>ğŸ’¾ Storage: Append-Only<br/>ğŸ—œï¸ Compression: gzip (50% saving)<br/>âš¡ Performance: 1000+ writes/sec]
                S3_REPLICA[S3/MinIO Replica<br/>ğŸšª Port 9001<br/>ğŸ”„ Cross-Region Replication<br/>ğŸ›¡ï¸ Disaster Recovery<br/>ğŸ“Š Backup Schedule: 4x/day<br/>âš–ï¸ Load Distribution]
            end
            
            subgraph "TerminusDB Cluster"
                TERMINUS_PRIMARY[TerminusDB Primary<br/>ğŸšª Port 6364<br/>ğŸ¯ Ontology SSoT<br/>ğŸ“Š Graph Database<br/>ğŸŒ³ Git Repository:<br/>  ğŸ“‹ 7/7 Git Features<br/>  ğŸŒ¿ Branch Management<br/>  ğŸ“ Commit History<br/>  ğŸ”€ Merge Operations<br/>  â†©ï¸ Rollback Support<br/>âš¡ Query Performance: <100ms<br/>ğŸ’¾ Storage: 10GB+ schemas]
                TERMINUS_REPLICA[TerminusDB Replica<br/>ğŸšª Port 6365<br/>ğŸ”„ Read Replica<br/>ğŸ“Š Load Balancing<br/>âš¡ Query Optimization<br/>ğŸ“ˆ Connection Pool: 50]
            end
        end
        
        subgraph "ğŸ” ì½ê¸° ìµœì í™” ëª¨ë¸ (Read Models)"
            subgraph "Elasticsearch Cluster"
                ES_MASTER[Elasticsearch Master<br/>ğŸšª Port 9200<br/>ğŸ” Search Engine<br/>ğŸ“Š Index Management:<br/>  ğŸ“‹ instances-* (6 shards)<br/>  ğŸ“‹ ontologies-* (3 shards)<br/>  ğŸ“‹ search-metadata (1 shard)<br/>ğŸŒ Multi-language Analyzer<br/>âš¡ Query Speed: <50ms<br/>ğŸ“ˆ Index Size: 1GB+]
                ES_DATA1[Elasticsearch Data 1<br/>ğŸšª Port 9201<br/>ğŸ“Š Data Node<br/>ğŸ” Full-text Search<br/>ğŸ“ˆ Aggregation Queries<br/>ğŸ’¾ Hot Storage]
                ES_DATA2[Elasticsearch Data 2<br/>ğŸšª Port 9202<br/>ğŸ“Š Data Node<br/>ğŸ” Faceted Search<br/>ğŸ“Š Analytics Queries<br/>â„ï¸ Warm Storage]
            end
            
            subgraph "Redis Cluster"
                REDIS_PRIMARY[Redis Primary<br/>ğŸšª Port 6379<br/>ğŸš€ Cache Layer<br/>ğŸ’¨ Cache Types:<br/>  ğŸ”‘ instance:{db}:{id}<br/>  ğŸ”‘ ontology:{db}:{class}<br/>  ğŸ”‘ search:{db}:{query_hash}<br/>  ğŸ”‘ session:{user_id}<br/>  ğŸ”‘ cmd:{command_id}<br/>âš¡ Performance: <1ms<br/>ğŸ’¾ Memory: 8GB<br/>ğŸ“Š Hit Ratio: 85%+]
                REDIS_REPLICA[Redis Replica<br/>ğŸšª Port 6380<br/>ğŸ”„ Read Replica<br/>ğŸ“Š Backup & Failover<br/>âš¡ Automatic Failover<br/>ğŸ“ˆ Connection Pool: 100]
            end
        end
        
        subgraph "ğŸ“Š ê´€ì°°ì„± & ëª¨ë‹ˆí„°ë§"
            subgraph "Monitoring Stack"
                PROMETHEUS[Prometheus<br/>ğŸšª Port 9090<br/>ğŸ“Š Metrics Collection<br/>â° Scrape Interval: 15s<br/>ğŸ“ˆ Metrics Storage<br/>ğŸ”” Alerting Rules]
                GRAFANA[Grafana<br/>ğŸšª Port 3000<br/>ğŸ“Š Visualization<br/>ğŸ“‹ Dashboards:<br/>  ğŸ“ˆ System Health<br/>  ğŸ“Š Business Metrics<br/>  âš¡ Performance Metrics<br/>ğŸ”” Alert Management]
                JAEGER[Jaeger<br/>ğŸšª Port 16686<br/>ğŸ” Distributed Tracing<br/>ğŸ“Š OpenTelemetry<br/>âš¡ Trace Analysis<br/>ğŸ¯ Performance Bottlenecks]
            end
            
            subgraph "Logging Stack"
                ELASTICSEARCH_LOG[Elasticsearch Logs<br/>ğŸšª Port 9203<br/>ğŸ“‹ Log Aggregation<br/>ğŸ” Log Search<br/>ğŸ“Š Log Analytics]
                KIBANA[Kibana<br/>ğŸšª Port 5601<br/>ğŸ“Š Log Visualization<br/>ğŸ” Log Analysis<br/>ğŸ“ˆ Error Tracking]
                FILEBEAT[Filebeat<br/>ğŸ“‹ Log Shipping<br/>ğŸ“Š Log Collection<br/>ğŸ”„ Log Processing]
            end
        end
    end

    %% í´ë¼ì´ì–¸íŠ¸ â†’ Load Balancer
    REACT_UI -->|HTTPS| LB
    EXT_API -->|REST API| LB
    CLI_TOOLS -->|HTTP/2| LB
    WEBHOOKS -->|Webhook Events| LB

    %% Load Balancer â†’ BFF Cluster
    LB -->|Round Robin| BFF1
    LB -->|Weighted| BFF2
    LB -->|Health Check| BFF3

    %% BFF â†’ Services (with detailed connections)
    BFF1 -->|Command Requests| OMS1
    BFF1 -->|AI Inference| FUNNEL1
    BFF2 -->|Failover| OMS2
    BFF2 -->|ML Processing| FUNNEL2
    BFF3 -->|Read Queries| OMS1

    %% OMS â†’ Outbox Pattern
    OMS1 -->|INSERT Commands| OUTBOX_DB
    OMS2 -->|INSERT Commands| OUTBOX_DB
    OUTBOX_DB -->|Store in| OUTBOX_TABLE

    %% Message Relay â†’ Kafka
    RELAY1 -->|Poll & Publish| KAFKA1
    RELAY2 -->|Backup Publishing| KAFKA2
    OUTBOX_TABLE -->|Polling| RELAY1
    OUTBOX_TABLE -->|Polling| RELAY2

    %% Kafka Internal Replication
    KAFKA1 <-->|Replication| KAFKA2
    KAFKA2 <-->|Replication| KAFKA3
    KAFKA3 <-->|Replication| KAFKA1

    %% Kafka â†’ Workers (with topic-specific routing)
    KAFKA1 -.->|instance-commands| IW1
    KAFKA2 -.->|instance-commands| IW2
    KAFKA1 -.->|ontology-commands| OW1
    KAFKA3 -.->|ontology-commands| OW2
    KAFKA1 -.->|*-events| PW1
    KAFKA2 -.->|*-events| PW2
    KAFKA3 -.->|*-events| PW3

    %% DLQ Processing
    KAFKA1 -.->|dlq-*| DLQ1
    KAFKA2 -.->|dlq-*| DLQ2

    %% Instance Workers â†’ Storage (3-step process)
    IW1 -->|1. Event Log| S3_PRIMARY
    IW1 -->|2. State Update| TERMINUS_PRIMARY
    IW1 -->|3. Domain Event| KAFKA1
    IW2 -->|1. Event Log| S3_REPLICA
    IW2 -->|2. State Update| TERMINUS_REPLICA
    IW2 -->|3. Domain Event| KAFKA2

    %% Ontology Workers â†’ Storage
    OW1 -->|Schema Changes| TERMINUS_PRIMARY
    OW1 -->|Git Operations| TERMINUS_PRIMARY
    OW1 -->|Domain Events| KAFKA1
    OW2 -->|Validation| TERMINUS_REPLICA
    OW2 -->|Domain Events| KAFKA2

    %% Projection Workers â†’ Read Models
    PW1 -->|Index Documents| ES_MASTER
    PW1 -->|Index Data| ES_DATA1
    PW2 -->|Cache Updates| REDIS_PRIMARY
    PW2 -->|Cache Replication| REDIS_REPLICA
    PW3 -->|Analytics Data| ES_DATA2

    %% BFF â†’ Read Models (CQRS Read Path)
    BFF1 -.->|Search Queries| ES_MASTER
    BFF1 -.->|Direct Gets| TERMINUS_PRIMARY
    BFF1 -.->|Cache Queries| REDIS_PRIMARY
    BFF2 -.->|Load Balanced| ES_DATA1
    BFF2 -.->|Read Replica| TERMINUS_REPLICA
    BFF3 -.->|Cache Fallback| REDIS_REPLICA

    %% Storage Replication
    S3_PRIMARY <-->|Replication| S3_REPLICA
    TERMINUS_PRIMARY <-->|Replication| TERMINUS_REPLICA
    REDIS_PRIMARY <-->|Replication| REDIS_REPLICA
    ES_MASTER <-->|Cluster Sync| ES_DATA1
    ES_MASTER <-->|Cluster Sync| ES_DATA2

    %% Monitoring Connections
    PROMETHEUS -->|Scrape Metrics| BFF1
    PROMETHEUS -->|Scrape Metrics| OMS1
    PROMETHEUS -->|Scrape Metrics| KAFKA1
    FILEBEAT -->|Ship Logs| ELASTICSEARCH_LOG
    JAEGER -->|Collect Traces| BFF1
    JAEGER -->|Collect Traces| OMS1

    %% ìŠ¤íƒ€ì¼ë§ (ìƒì„¸ ìƒ‰ìƒ êµ¬ë¶„)
    style LB fill:#ff6b6b,stroke:#d63031,stroke-width:3px,color:#fff
    style BFF1 fill:#74b9ff,stroke:#0984e3,stroke-width:3px,color:#fff
    style BFF2 fill:#74b9ff,stroke:#0984e3,stroke-width:2px,color:#fff
    style BFF3 fill:#74b9ff,stroke:#0984e3,stroke-width:2px,color:#fff
    
    style OMS1 fill:#00b894,stroke:#00a085,stroke-width:3px,color:#fff
    style OMS2 fill:#00b894,stroke:#00a085,stroke-width:2px,color:#fff
    style FUNNEL1 fill:#fdcb6e,stroke:#e17055,stroke-width:3px,color:#fff
    style FUNNEL2 fill:#fdcb6e,stroke:#e17055,stroke-width:2px,color:#fff
    
    style KAFKA1 fill:#a29bfe,stroke:#6c5ce7,stroke-width:3px,color:#fff
    style KAFKA2 fill:#a29bfe,stroke:#6c5ce7,stroke-width:2px,color:#fff
    style KAFKA3 fill:#a29bfe,stroke:#6c5ce7,stroke-width:2px,color:#fff
    
    style IW1 fill:#fd79a8,stroke:#e84393,stroke-width:2px,color:#fff
    style IW2 fill:#fd79a8,stroke:#e84393,stroke-width:2px,color:#fff
    style OW1 fill:#fd79a8,stroke:#e84393,stroke-width:2px,color:#fff
    style OW2 fill:#fd79a8,stroke:#e84393,stroke-width:2px,color:#fff
    style PW1 fill:#fd79a8,stroke:#e84393,stroke-width:2px,color:#fff
    style PW2 fill:#fd79a8,stroke:#e84393,stroke-width:2px,color:#fff
    style PW3 fill:#fd79a8,stroke:#e84393,stroke-width:2px,color:#fff
    
    style S3_PRIMARY fill:#e17055,stroke:#d63031,stroke-width:4px,color:#fff
    style S3_REPLICA fill:#e17055,stroke:#d63031,stroke-width:3px,color:#fff
    style TERMINUS_PRIMARY fill:#e17055,stroke:#d63031,stroke-width:4px,color:#fff
    style TERMINUS_REPLICA fill:#e17055,stroke:#d63031,stroke-width:3px,color:#fff
    
    style ES_MASTER fill:#0984e3,stroke:#74b9ff,stroke-width:3px,color:#fff
    style ES_DATA1 fill:#0984e3,stroke:#74b9ff,stroke-width:2px,color:#fff
    style ES_DATA2 fill:#0984e3,stroke:#74b9ff,stroke-width:2px,color:#fff
    style REDIS_PRIMARY fill:#00cec9,stroke:#00b894,stroke-width:3px,color:#fff
    style REDIS_REPLICA fill:#00cec9,stroke:#00b894,stroke-width:2px,color:#fff
    
    style DLQ1 fill:#fab1a0,stroke:#e17055,stroke-width:2px,color:#fff
    style DLQ2 fill:#fab1a0,stroke:#e17055,stroke-width:2px,color:#fff
```

### ğŸ”„ **í•µì‹¬ ì•„í‚¤í…ì²˜ íŒ¨í„´**

| íŒ¨í„´ | ëª©ì  | êµ¬í˜„ ìœ„ì¹˜ | íš¨ê³¼ |
|------|------|-----------|------|
| **CQRS** | ì½ê¸°/ì“°ê¸° ë¶„ë¦¬ | BFF â†” ê°ì¢… ì €ì¥ì†Œ | ë…ë¦½ì  ìµœì í™” |
| **Event Sourcing** | ì™„ì „í•œ ì´ë ¥ ì¶”ì  | Instance Worker â†’ S3 | ë¬´ì†ì‹¤ ê°ì‚¬ ì¶”ì  |
| **Outbox Pattern** | íŠ¸ëœì­ì…˜ ì•ˆì „ì„± | OMS â†’ PostgreSQL â†’ Kafka | ë©”ì‹œì§€ ë¬´ì†ì‹¤ |
| **ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤** | ì„œë¹„ìŠ¤ ë¶„ë¦¬ | OMS, Funnel, Workers | ë…ë¦½ ë°°í¬/í™•ì¥ |
| **Projection** | ì½ê¸° ëª¨ë¸ ìµœì í™” | Events â†’ Elasticsearch | ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™” |

---

## 2. í•µì‹¬ ì„¤ê³„ ì›ì¹™

### ğŸ›¡ï¸ **ë°ì´í„° ë¬´ê²°ì„± ë° ì¼ê´€ì„±**

#### âœ… **ë¶ˆë³€ì„± ì›ì¹™ (Immutability)**
```
ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„°ì˜ ëª¨ë“  ë³€ê²½ì€ ìƒˆë¡œìš´ ì´ë²¤íŠ¸ë¡œ S3ì— ì¶”ê°€(append-only)
â†’ ê¸°ì¡´ ë°ì´í„°ëŠ” ì ˆëŒ€ ìˆ˜ì •/ì‚­ì œë˜ì§€ ì•ŠìŒ
â†’ ì™„ë²½í•œ ê°ì‚¬ ì¶”ì  ë° ì‹œê°„ ì—¬í–‰ ê¸°ëŠ¥ ì œê³µ
```

#### âœ… **ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì› (Single Source of Truth)**
- **ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„° SSoT**: S3ì— ì €ì¥ëœ ì´ë²¤íŠ¸ ë¡œê·¸
- **ì˜¨í†¨ë¡œì§€ ë°ì´í„° SSoT**: TerminusDBì— ì €ì¥ëœ ìµœì‹  ìŠ¤í‚¤ë§ˆ ìƒíƒœ
- **ì½ê¸° ëª¨ë¸**: ëª¨ë“  ì½ê¸° ëª¨ë¸ì€ SSoTë¡œë¶€í„° íŒŒìƒëœ í”„ë¡œì ì…˜

#### âœ… **ìµœì¢… ì¼ê´€ì„± (Eventual Consistency)**
- ì“°ê¸° ëª¨ë¸ê³¼ ì½ê¸° ëª¨ë¸ ê°„ì˜ ì¼ê´€ì„±ì€ ì´ë²¤íŠ¸ë¥¼ í†µí•´ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë‹¬ì„±
- Kafkaì˜ ìˆœì„œ ë³´ì¥ì„ í†µí•´ ì¼ê´€ì„± í™•ë³´
- íŒŒí‹°ì…˜ í‚¤(aggregate_id)ë¡œ ì§‘ê³„ë³„ ìˆœì„œ ë³´ì¥

### âš¡ **ì„±ëŠ¥ ë° í™•ì¥ì„±**

#### âœ… **ì½ê¸°/ì“°ê¸° ìµœì í™”**
- **ì“°ê¸° ê²½ë¡œ**: ëª…ë ¹ ì²˜ë¦¬ì— ìµœì í™”, ë¹ ë¥¸ ê²€ì¦ ë° ì´ë²¤íŠ¸ ì €ì¥
- **ì½ê¸° ê²½ë¡œ**: ë‹¤ì–‘í•œ ì¿¼ë¦¬ íŒ¨í„´ì— ìµœì í™”ëœ ì—¬ëŸ¬ ì½ê¸° ëª¨ë¸
- **ìºì‹œ ì „ëµ**: ìì£¼ ì¡°íšŒë˜ëŠ” ë°ì´í„°ëŠ” Redisì— ìºì‹±

#### âœ… **ìˆ˜í‰ í™•ì¥ì„±**
- **ì„œë¹„ìŠ¤ë³„ ë…ë¦½ í™•ì¥**: ê° ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ëŠ” ë…ë¦½ì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥
- **Kafka íŒŒí‹°ì…”ë‹**: ë†’ì€ ì²˜ë¦¬ëŸ‰ì„ ìœ„í•œ ë¶„ì‚° ë©”ì‹œì§€ ì²˜ë¦¬
- **ì½ê¸° ëª¨ë¸ ë³µì œ**: Elasticsearch í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì½ê¸° ì„±ëŠ¥ í–¥ìƒ

### ğŸ” **ë³´ì•ˆ ë° ê°ì‚¬**

#### âœ… **ì™„ì „í•œ ê°ì‚¬ ì¶”ì **
- ëª¨ë“  ëª…ë ¹ì€ ì‚¬ìš©ì ID, íƒ€ì„ìŠ¤íƒ¬í”„, ì´ìœ ì™€ í•¨ê»˜ ê¸°ë¡
- ì´ë²¤íŠ¸ ì†Œì‹±ìœ¼ë¡œ "ëˆ„ê°€, ì–¸ì œ, ë¬´ì—‡ì„, ì™œ" ë³€ê²½í–ˆëŠ”ì§€ ì™„ë²½ ì¶”ì 
- ë¶ˆë³€ ë¡œê·¸ë¡œ ë°ì´í„° ìœ„ë³€ì¡° ë°©ì§€

#### âœ… **ì ‘ê·¼ ì œì–´**
- JWT ê¸°ë°˜ ì¸ì¦ ë° ì¸ê°€
- API Gateway(BFF)ì—ì„œ ì¤‘ì•™í™”ëœ ë³´ì•ˆ ì •ì±… ì ìš©
- ì„œë¹„ìŠ¤ ê°„ í†µì‹ ì€ ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ë¡œ ë³´í˜¸

---

## 3. í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì•„í‚¤í…ì²˜

SPICE HARVESTERì˜ ê°€ì¥ í˜ì‹ ì ì¸ íŠ¹ì§•ì€ **ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ìµœì í™”ëœ ê´€ë¦¬ ì „ëµ**ì…ë‹ˆë‹¤.

### ğŸ“Š **ë°ì´í„° ë¶„ë¥˜ ë° ê´€ë¦¬ ì „ëµ**

```mermaid
graph TD
    subgraph "ğŸ—‚ï¸ ì˜¨í†¨ë¡œì§€ ë°ì´í„°"
        ONT_CHAR[íŠ¹ì„±:<br/>â€¢ ìŠ¤í‚¤ë§ˆ ì—­í• <br/>â€¢ ë³€ê²½ ë¹ˆë„ ë‚®ìŒ<br/>â€¢ ìµœì‹  ìƒíƒœ ì¤‘ìš”<br/>â€¢ ê´€ê³„í˜• ë³µì¡]
        ONT_STRATEGY[ê´€ë¦¬ ì „ëµ:<br/>ìƒíƒœ ì €ì¥<br/>(State-Store)]
        ONT_STORAGE[ì €ì¥ì†Œ:<br/>TerminusDB<br/>Graph Database]
    end

    subgraph "ğŸ“ ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„°"
        INS_CHAR[íŠ¹ì„±:<br/>â€¢ ì‹¤ì œ ë°ì´í„°<br/>â€¢ ë³€ê²½ ë¹ˆë„ ë†’ìŒ<br/>â€¢ ì´ë ¥ ì¶”ì  í•„ìˆ˜<br/>â€¢ ì™„ì „í•œ ê°ì‚¬ í•„ìš”]
        INS_STRATEGY[ê´€ë¦¬ ì „ëµ:<br/>ì´ë²¤íŠ¸ ì†Œì‹±<br/>(Event Sourcing)]
        INS_STORAGE[ì €ì¥ì†Œ:<br/>S3 Event Store<br/>Append-only Log]
    end

    ONT_CHAR --> ONT_STRATEGY
    ONT_STRATEGY --> ONT_STORAGE

    INS_CHAR --> INS_STRATEGY
    INS_STRATEGY --> INS_STORAGE

    style ONT_STRATEGY fill:#e8f5e8
    style INS_STRATEGY fill:#fff3e0
```

### ğŸ”„ **í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ì˜ ì¥ì **

#### âœ… **ì˜¨í†¨ë¡œì§€ ë°ì´í„° (ìƒíƒœ ì €ì¥ ë°©ì‹)**
- **ë¹ ë¥¸ ì¡°íšŒ**: ìµœì‹  ìŠ¤í‚¤ë§ˆ ìƒíƒœë¥¼ ì¦‰ì‹œ ì¡°íšŒ
- **ê´€ê³„í˜• ì¿¼ë¦¬**: ë³µì¡í•œ ì˜¨í†¨ë¡œì§€ ê´€ê³„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰
- **ì¼ê´€ì„±**: ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì‹œ ì¦‰ì‹œ ì¼ê´€ëœ ìƒíƒœ ìœ ì§€
- **Git ê¸°ëŠ¥**: ë¸Œëœì¹˜, ì»¤ë°‹, ë³‘í•© ë“± ë²„ì „ ê´€ë¦¬ ê¸°ëŠ¥ ì™„ë²½ ì§€ì›

#### âœ… **ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„° (ì´ë²¤íŠ¸ ì†Œì‹± ë°©ì‹)**
- **ì™„ì „í•œ ê°ì‚¬**: ëª¨ë“  ë³€ê²½ì‚¬í•­ì˜ ë¶ˆë³€ ê¸°ë¡
- **ì‹œê°„ ì—¬í–‰**: íŠ¹ì • ì‹œì ì˜ ë°ì´í„° ìƒíƒœ ì™„ë²½ ë³µì›
- **ë°ì´í„° ê³„ë³´**: í˜„ì¬ ìƒíƒœê°€ ì–´ë–¤ ë³€ê²½ë“¤ë¡œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ ì¶”ì 
- **ë¬´ì†ì‹¤**: ì‹¤ìˆ˜ë¡œ ì‚­ì œë˜ê±°ë‚˜ ì†ìƒëœ ë°ì´í„° ë³µêµ¬ ê°€ëŠ¥

### ğŸ“ˆ **ì„±ëŠ¥ ë¹„êµ**

| ì‘ì—… ìœ í˜• | ì „í†µì  ë°©ì‹ | SPICE HARVESTER | ê°œì„  íš¨ê³¼ |
|----------|------------|-----------------|-----------|
| ì˜¨í†¨ë¡œì§€ ì¡°íšŒ | ë³µì¡í•œ JOIN | Direct Graph Query | 5-10x ë¹ ë¦„ |
| ì¸ìŠ¤í„´ìŠ¤ ê°ì‚¬ | ë³„ë„ ë¡œê·¸ í…Œì´ë¸” | Event Log ì§ì ‘ ì¡°íšŒ | ë¬´í•œ ì´ë ¥ ì¶”ì  |
| ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬ | ìˆ˜ë™ ê´€ë¦¬ | Git-like ìë™í™” | 100% ìë™í™” |
| ë°ì´í„° ë³µêµ¬ | ë°±ì—…/ë³µì› | Event Replay | 1ì´ˆ ë‚´ ë³µêµ¬ |

---

## 4. ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ìƒì„¸

### ğŸŒ **API ê²Œì´íŠ¸ì›¨ì´ ë ˆì´ì–´**

#### **BFF (Backend for Frontend) - Port 8002**
```python
# í•µì‹¬ ì±…ì„
- í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì˜ ë‹¨ì¼ ì§„ì…ì 
- ìš”ì²­ ë¼ìš°íŒ… ë° ì§‘ê³„
- ì¸ì¦/ì¸ê°€ ì²˜ë¦¬
- ì½ê¸° ëª¨ë¸ ì§ì ‘ ì¡°íšŒ
- ì‘ë‹µ í‘œì¤€í™” (ApiResponse í˜•ì‹)
```

**ì£¼ìš” API ì—”ë“œí¬ì¸íŠ¸:**
- `POST /api/v1/database` - ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
- `GET /api/v1/database/{db}/ontology/{class}` - ì˜¨í†¨ë¡œì§€ ì¡°íšŒ
- `POST /api/v1/database/{db}/ontology` - ì˜¨í†¨ë¡œì§€ ìƒì„±
- `GET /api/v1/database/{db}/instances` - ì¸ìŠ¤í„´ìŠ¤ ê²€ìƒ‰
- `POST /api/v1/database/{db}/instances` - ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

### âš™ï¸ **ì„œë¹„ìŠ¤ ë ˆì´ì–´**

#### **OMS (Ontology Management Service) - Port 8000**
```python
# í•µì‹¬ ì±…ì„
- ëª¨ë“  ëª…ë ¹(Command)ì˜ ê²€ì¦ ë° ì²˜ë¦¬
- Outbox íŒ¨í„´ì„ í†µí•œ ì•ˆì „í•œ ë©”ì‹œì§€ ë°œí–‰
- ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì§ì ‘ ê´€ë¦¬
- ë³µí•© ë°ì´í„° íƒ€ì… ê²€ì¦ (18+ íƒ€ì…)
- Git ê¸°ëŠ¥ êµ¬í˜„ (7/7 ì™„ì„±)
```

**Git ê¸°ëŠ¥ êµ¬í˜„ ìƒì„¸:**
- **ë¸Œëœì¹˜ ê´€ë¦¬**: `create_branch()`, `switch_branch()`, `delete_branch()`
- **ì»¤ë°‹ ì‹œìŠ¤í…œ**: `commit_changes()` with ë©”ì‹œì§€, ì‘ì„±ì, íƒ€ì„ìŠ¤íƒ¬í”„
- **ë¹„êµ ê¸°ëŠ¥**: `diff_branches()`, `diff_commits()` - 3ë‹¨ê³„ ë¹„êµ (ì»¤ë°‹/ìŠ¤í‚¤ë§ˆ/í”„ë¡œí¼í‹°)
- **ë³‘í•© ê¸°ëŠ¥**: `merge_branches()`, `rebase_branch()` with ì¶©ëŒ í•´ê²°
- **ë¡¤ë°± ê¸°ëŠ¥**: `rollback_to_commit()` - ì•ˆì „í•œ ë˜ëŒë¦¬ê¸°
- **íˆìŠ¤í† ë¦¬**: `get_commit_history()` - ì „ì²´ ë³€ê²½ ì´ë ¥

#### **Funnel (AI Type Inference Service) - Port 8004**
```python
# í•µì‹¬ ì±…ì„ (1,048ë¼ì¸ ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜)
- ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ ë¶„ì„
- 18+ ë³µí•© ë°ì´í„° íƒ€ì… ìë™ ê°ì§€
- ë‹¤êµ­ì–´ íŒ¨í„´ ì¸ì‹ (í•œ/ì˜/ì¼/ì¤‘)
- ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ìë™ ìƒì„±
- êµ¬ê¸€ ì‹œíŠ¸ ì§ì ‘ ë¶„ì„ ë° ìŠ¤í‚¤ë§ˆ ì œì•ˆ
```

**ì§€ì›í•˜ëŠ” ë³µí•© íƒ€ì…:**
- **EMAIL**: RFC 5322 ì™„ë²½ ê²€ì¦
- **PHONE**: êµ­ì œ ë²ˆí˜¸ í˜•ì‹ ì§€ì› (E164, NATIONAL)
- **MONEY**: ë‹¤ì¤‘ í†µí™” ì§€ì› (KRW, USD, EUR ë“±)
- **ARRAY**: ì¤‘ì²© ë°°ì—´ ë° íƒ€ì…ë³„ ê²€ì¦
- **OBJECT**: ë³µì¡í•œ ì¤‘ì²© ê°ì²´ ìŠ¤í‚¤ë§ˆ
- **DATE/TIME**: ISO 8601 ë° ë‹¤ì–‘í•œ ì§€ì—­ í˜•ì‹
- **URL/URI**: RFC 3986 í‘œì¤€ ê²€ì¦
- ê¸°íƒ€ 12ê°œ ì¶”ê°€ ë³µí•© íƒ€ì…

### ğŸ“¨ **ë©”ì‹œì§• ë ˆì´ì–´**

#### **PostgreSQL Outbox Pattern**
```sql
-- outbox í…Œì´ë¸” êµ¬ì¡°
CREATE TABLE outbox (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    aggregate_id VARCHAR(255) NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    payload JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    published_at TIMESTAMP WITH TIME ZONE,
    status VARCHAR(20) DEFAULT 'PENDING'
);

-- ì¸ë±ìŠ¤ ìµœì í™”
CREATE INDEX idx_outbox_status_created ON outbox(status, created_at);
CREATE INDEX idx_outbox_aggregate_id ON outbox(aggregate_id);
```

**Outbox Pattern ì¥ì :**
- âœ… **íŠ¸ëœì­ì…˜ ì•ˆì „ì„±**: DB ë³€ê²½ê³¼ ë©”ì‹œì§€ ë°œí–‰ì˜ ì›ìì„± ë³´ì¥
- âœ… **ì‹ ë¢°ì„±**: ì‹œìŠ¤í…œ ì˜¤ë¥˜ ì‹œì—ë„ ë©”ì‹œì§€ ì†ì‹¤ ë°©ì§€
- âœ… **ìˆœì„œ ë³´ì¥**: ë™ì¼ ì§‘ê³„ì˜ ì´ë²¤íŠ¸ ìˆœì„œ ìœ ì§€
- âœ… **ì¬ì‹œë„ ê¸°ëŠ¥**: ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ìë™ ì¬ì‹œë„

#### **Message Relay Worker**
```python
class MessageRelay:
    async def poll_and_publish(self):
        """Outbox íŒ¨í„´ í•µì‹¬ ë¡œì§"""
        # 1. PostgreSQLì—ì„œ ë¯¸ë°œí–‰ ì´ë²¤íŠ¸ ì¡°íšŒ
        pending_events = await self.get_pending_events()
        
        for event in pending_events:
            try:
                # 2. Kafkaë¡œ ì´ë²¤íŠ¸ ë°œí–‰
                await self.kafka_producer.send(
                    topic=event.topic,
                    key=event.aggregate_id,  # íŒŒí‹°ì…˜ í‚¤
                    value=event.payload
                )
                
                # 3. ë°œí–‰ ì™„ë£Œ í‘œì‹œ
                await self.mark_as_published(event.id)
                
            except Exception as e:
                # 4. ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ëŒ€ê¸°ì—´ë¡œ
                await self.handle_publish_failure(event, e)
```

#### **Apache Kafka - ê³ ì„±ëŠ¥ ì´ë²¤íŠ¸ ë²„ìŠ¤**
```yaml
# Kafka ì„¤ì • (Exactly-Once Semantics v2)
transaction.timeout.ms: 300000
delivery.timeout.ms: 120000
acks: all
enable.idempotence: true
max.in.flight.requests.per.connection: 5

# íŒŒí‹°ì…˜ ì „ëµ
partitioner.class: org.apache.kafka.clients.producer.internals.DefaultPartitioner
# í‚¤ = aggregate_idë¡œ ë™ì¼ ì§‘ê³„ ì´ë²¤íŠ¸ì˜ ìˆœì„œ ë³´ì¥
```

**Kafka í† í”½ êµ¬ì¡°:**
- `ontology-commands`: ì˜¨í†¨ë¡œì§€ ë³€ê²½ ëª…ë ¹
- `instance-commands`: ì¸ìŠ¤í„´ìŠ¤ ë³€ê²½ ëª…ë ¹  
- `ontology-events`: ì˜¨í†¨ë¡œì§€ ë„ë©”ì¸ ì´ë²¤íŠ¸
- `instance-events`: ì¸ìŠ¤í„´ìŠ¤ ë„ë©”ì¸ ì´ë²¤íŠ¸
- `dlq-*`: Dead Letter Queue í† í”½ë“¤

### âš™ï¸ **ì›Œì»¤ ë ˆì´ì–´**

#### **Instance Worker**
```python
class InstanceWorker:
    async def process_command(self, command):
        """ì¸ìŠ¤í„´ìŠ¤ ëª…ë ¹ ì²˜ë¦¬ - 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤"""
        
        # 1ë‹¨ê³„: S3ì— ëª…ë ¹ ë¡œê·¸ ì €ì¥ (SSoT)
        command_log = {
            "command_id": command.id,
            "aggregate_id": command.aggregate_id,
            "command_type": command.type,
            "payload": command.payload,
            "timestamp": command.timestamp,
            "user_id": command.user_id
        }
        await self.s3_client.put_object(
            bucket="spice-event-store",
            key=f"instances/{command.aggregate_id}/{command.id}.json",
            body=json.dumps(command_log)
        )
        
        # 2ë‹¨ê³„: TerminusDB ìµœì‹  ìƒíƒœ ì—…ë°ì´íŠ¸ (Write Model Cache)
        await self.terminus_client.execute_command(command)
        
        # 3ë‹¨ê³„: ë„ë©”ì¸ ì´ë²¤íŠ¸ ë°œí–‰
        domain_event = self.create_domain_event(command)
        await self.kafka_producer.send("instance-events", domain_event)
```

#### **Ontology Worker**  
```python
class OntologyWorker:
    async def process_command(self, command):
        """ì˜¨í†¨ë¡œì§€ ëª…ë ¹ ì²˜ë¦¬ - ì§ì ‘ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        
        # TerminusDBì— ìŠ¤í‚¤ë§ˆ ì§ì ‘ ë³€ê²½ (SSoT)
        if command.type == "CREATE_ONTOLOGY":
            await self.terminus_client.create_class(
                database=command.database,
                class_definition=command.payload
            )
            
        elif command.type == "UPDATE_ONTOLOGY":
            await self.terminus_client.update_class(
                database=command.database,
                class_id=command.class_id,
                updates=command.payload
            )
            
        # Git ê¸°ëŠ¥ ì—…ë°ì´íŠ¸
        await self.update_git_metadata(command)
        
        # ë„ë©”ì¸ ì´ë²¤íŠ¸ ë°œí–‰
        domain_event = self.create_domain_event(command)
        await self.kafka_producer.send("ontology-events", domain_event)
```

#### **Projection Worker**
```python
class ProjectionWorker:
    async def handle_event(self, event):
        """ë„ë©”ì¸ ì´ë²¤íŠ¸ë¥¼ ì½ê¸° ëª¨ë¸ë¡œ í”„ë¡œì ì…˜"""
        
        if event.type == "INSTANCE_CREATED":
            # Elasticsearch ê²€ìƒ‰ ë¬¸ì„œ ìƒì„±
            search_doc = {
                "id": event.instance_id,
                "type": event.instance_type,
                "properties": self.flatten_for_search(event.data),
                "created_at": event.timestamp,
                "database": event.database
            }
            
            await self.elasticsearch.index(
                index=f"instances-{event.database}",
                doc_type="_doc",
                body=search_doc
            )
            
            # Redis ìºì‹œ ì—…ë°ì´íŠ¸
            cache_key = f"instance:{event.database}:{event.instance_id}"
            await self.redis.setex(
                cache_key, 
                ttl=3600,  # 1ì‹œê°„ ìºì‹œ
                value=json.dumps(event.data)
            )
            
        elif event.type == "ONTOLOGY_UPDATED":
            # ì˜¨í†¨ë¡œì§€ ë©”íƒ€ë°ì´í„° ìºì‹œ ë¬´íš¨í™”
            pattern = f"ontology:{event.database}:*"
            await self.redis.delete_pattern(pattern)
```

---

## 5. ë°ì´í„° íë¦„ ë° í†µì‹ 

### ğŸ“ **ëª…ë ¹ ì²˜ë¦¬ íë¦„ (Write Path)**

```mermaid
sequenceDiagram
    participant Client
    participant BFF
    participant OMS
    participant PostgreSQL
    participant MessageRelay
    participant Kafka
    participant InstanceWorker
    participant S3
    participant TerminusDB as TerminusDB (Write)

    Client->>BFF: POST /api/v1/instances
    BFF->>OMS: Command Request
    
    Note over OMS: ëª…ë ¹ ê²€ì¦ ë° ì²˜ë¦¬
    OMS->>PostgreSQL: INSERT INTO outbox
    OMS-->>BFF: 202 Accepted (Command ID)
    BFF-->>Client: 202 Accepted
    
    loop Outbox Polling
        MessageRelay->>PostgreSQL: SELECT * FROM outbox WHERE status='PENDING'
        MessageRelay->>Kafka: Publish Command
        MessageRelay->>PostgreSQL: UPDATE outbox SET status='PUBLISHED'
    end
    
    InstanceWorker->>Kafka: Consume Command
    
    par 3ë‹¨ê³„ ë™ì‹œ ì²˜ë¦¬
        InstanceWorker->>S3: 1. Save Command Log (SSoT)
    and
        InstanceWorker->>TerminusDB: 2. Update Write Model
    and
        InstanceWorker->>Kafka: 3. Publish Domain Event
    end
```

### ğŸ” **ì¡°íšŒ ì²˜ë¦¬ íë¦„ (Read Path)**

```mermaid
sequenceDiagram
    participant Client
    participant BFF
    participant Elasticsearch
    participant TerminusDB as TerminusDB (Read)
    participant Redis

    Client->>BFF: GET /api/v1/instances?search=...
    
    alt ë³µì¡í•œ ê²€ìƒ‰ ì¿¼ë¦¬
        BFF->>Elasticsearch: Search Query
        Elasticsearch-->>BFF: Search Results
    else ì§ì ‘ ID ì¡°íšŒ
        BFF->>Redis: GET instance:{id}
        
        alt Cache Hit
            Redis-->>BFF: Cached Data
        else Cache Miss
            BFF->>TerminusDB: Direct Query
            TerminusDB-->>BFF: Data
            BFF->>Redis: SET cache
        end
    end
    
    BFF-->>Client: ApiResponse<Data>
```

### ğŸ”„ **í”„ë¡œì ì…˜ êµ¬ì¶• íë¦„ (Projection Path)**

```mermaid
sequenceDiagram
    participant Kafka
    participant ProjectionWorker
    participant Elasticsearch
    participant Redis

    Kafka->>ProjectionWorker: Domain Event
    
    Note over ProjectionWorker: ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì²˜ë¦¬
    
    par ì½ê¸° ëª¨ë¸ ì—…ë°ì´íŠ¸
        ProjectionWorker->>Elasticsearch: Index Document
    and
        ProjectionWorker->>Redis: Update Cache
    end
    
    Note over ProjectionWorker: í”„ë¡œì ì…˜ ì™„ë£Œ ë¡œê¹…
```

---

## 6. ì €ì¥ì†Œë³„ ì—­í•  ë° ì±…ì„

### ğŸ’¾ **ì €ì¥ì†Œ ì•„í‚¤í…ì²˜ ë§¤íŠ¸ë¦­ìŠ¤**

| ì €ì¥ì†Œ | ì£¼ ì—­í•  | ë°ì´í„° ì¢…ë¥˜ | ì¼ê´€ì„± ìˆ˜ì¤€ | SSoT ì—¬ë¶€ | ë°±ì—… í•„ìš”ì„± |
|--------|---------|-------------|-------------|-----------|------------|
| **S3/MinIO** | ì´ë²¤íŠ¸ ìŠ¤í† ì–´ | ì¸ìŠ¤í„´ìŠ¤ ëª…ë ¹ ë¡œê·¸ | ê°• ì¼ê´€ì„± | âœ… Yes (Instance) | ğŸ”¥ ë§¤ìš° ë†’ìŒ |
| **TerminusDB** | Graph DB | ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ + ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ | ê°• ì¼ê´€ì„± | âœ… Yes (Ontology) | ğŸ”¥ ë§¤ìš° ë†’ìŒ |
| **Elasticsearch** | ê²€ìƒ‰ ì—”ì§„ | ê²€ìƒ‰ìš© ë¹„ì •ê·œí™” ë°ì´í„° | ìµœì¢… ì¼ê´€ì„± | âŒ No | âš ï¸ ì¤‘ê°„ (ì¬êµ¬ì¶• ê°€ëŠ¥) |
| **PostgreSQL** | ë©”ì‹œì§€ í | Outbox íŒ¨í„´ ëŒ€ê¸° ì´ë²¤íŠ¸ | ê°• ì¼ê´€ì„± | âŒ No | âš ï¸ ì¤‘ê°„ (ì¼ì‹œì  ë°ì´í„°) |
| **Redis** | ìºì‹œ | ìì£¼ ì¡°íšŒë˜ëŠ” ë©”íƒ€ë°ì´í„° | ìµœì¢… ì¼ê´€ì„± | âŒ No | âš¡ ë‚®ìŒ (ì¬ìƒì„± ê°€ëŠ¥) |

### ğŸ—„ï¸ **S3/MinIO - ì´ë²¤íŠ¸ ìŠ¤í† ì–´**

#### **ì €ì¥ êµ¬ì¡°**
```
spice-event-store/
â”œâ”€â”€ instances/
â”‚   â”œâ”€â”€ {aggregate_id}/
â”‚   â”‚   â”œâ”€â”€ {command_id_1}.json
â”‚   â”‚   â”œâ”€â”€ {command_id_2}.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ {aggregate_id}/
â”‚   â”‚   â”œâ”€â”€ snapshot_v1.json
â”‚   â”‚   â””â”€â”€ snapshot_v2.json
â”‚   â””â”€â”€ ...
â””â”€â”€ indexes/
    â”œâ”€â”€ by_date/
    â”œâ”€â”€ by_user/
    â””â”€â”€ by_type/
```

#### **ëª…ë ¹ ë¡œê·¸ í˜•ì‹**
```json
{
  "command_id": "cmd_123e4567-e89b-12d3-a456-426614174000",
  "aggregate_id": "instance_789",
  "command_type": "CREATE_INSTANCE",
  "payload": {
    "class_type": "Person",
    "properties": {
      "name": "í™ê¸¸ë™",
      "email": "hong@example.com",
      "phone": "+82-10-1234-5678"
    }
  },
  "timestamp": "2024-08-12T10:30:00Z",
  "user_id": "user_456",
  "reason": "ì‹ ê·œ ì§ì› ë“±ë¡",
  "metadata": {
    "client_ip": "192.168.1.100",
    "user_agent": "SPICE-UI/2.0"
  }
}
```

#### **ì„±ëŠ¥ ìµœì í™”**
- **íŒŒí‹°ì…”ë‹**: ë‚ ì§œë³„, aggregate_idë³„ ë¶„ì‚° ì €ì¥
- **ì••ì¶•**: gzip ì••ì¶•ìœ¼ë¡œ ì €ì¥ ê³µê°„ 50% ì ˆì•½
- **ì¸ë±ì‹±**: ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•œ ë³´ì¡° ì¸ë±ìŠ¤
- **ìŠ¤ëƒ…ìƒ·**: ëŒ€ëŸ‰ ì´ë²¤íŠ¸ ì§‘ê³„ì˜ ì„±ëŠ¥ ìµœì í™”

### ğŸŒ **TerminusDB - ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤**

#### **ë°ì´í„° ëª¨ë¸**
```javascript
// ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ (SSoT)
{
  "@type": "Class",
  "@id": "Person",
  "label": "Person",
  "properties": {
    "name": { "@type": "xsd:string", "required": true },
    "email": { "@type": "custom:email", "required": true },
    "works_for": { 
      "@type": "@id", 
      "@class": "Company",
      "cardinality": "many_to_one"
    }
  },
  "git_metadata": {
    "branch": "main",
    "commit": "abc123",
    "author": "dev@spice.com",
    "timestamp": "2024-08-12T10:30:00Z"
  }
}

// ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„° (Write Model Cache)
{
  "@type": "Person",
  "@id": "person_123",
  "name": "í™ê¸¸ë™",
  "email": "hong@example.com",
  "works_for": "company_456",
  "_metadata": {
    "created_at": "2024-08-12T10:30:00Z",
    "updated_at": "2024-08-12T15:45:00Z",
    "version": 3
  }
}
```

#### **Git ê¸°ëŠ¥ êµ¬í˜„**
```javascript
// ë¸Œëœì¹˜ êµ¬ì¡°
{
  "databases": {
    "main": {
      "branches": {
        "main": { "commit": "abc123", "head": true },
        "feature/new-schema": { "commit": "def456", "head": false },
        "hotfix/urgent-fix": { "commit": "ghi789", "head": false }
      },
      "commits": {
        "abc123": {
          "parent": "xyz789",
          "message": "Add Person.email field",
          "author": "dev@spice.com",
          "timestamp": "2024-08-12T10:30:00Z",
          "changes": [...]
        }
      }
    }
  }
}
```

### ğŸ” **Elasticsearch - ê²€ìƒ‰ ì—”ì§„**

#### **ì¸ë±ìŠ¤ êµ¬ì¡°**
```json
// instances-{database_name} ì¸ë±ìŠ¤
{
  "mappings": {
    "properties": {
      "id": { "type": "keyword" },
      "type": { "type": "keyword" },
      "database": { "type": "keyword" },
      "properties": {
        "type": "object",
        "dynamic": true
      },
      "created_at": { "type": "date" },
      "updated_at": { "type": "date" },
      "full_text": {
        "type": "text",
        "analyzer": "korean"
      }
    }
  }
}

// ì‹¤ì œ ë¬¸ì„œ ì˜ˆì‹œ
{
  "id": "person_123",
  "type": "Person",
  "database": "hr_system",
  "properties": {
    "name": "í™ê¸¸ë™",
    "email": "hong@example.com",
    "phone": "+82-10-1234-5678",
    "department": "ê°œë°œíŒ€"
  },
  "created_at": "2024-08-12T10:30:00Z",
  "updated_at": "2024-08-12T15:45:00Z",
  "full_text": "í™ê¸¸ë™ hong@example.com ê°œë°œíŒ€ Person"
}
```

#### **ê²€ìƒ‰ ìµœì í™”**
- **ë‹¤êµ­ì–´ ë¶„ì„ê¸°**: í•œêµ­ì–´, ì˜ì–´ ë™ì‹œ ì§€ì›
- **ìë™ ì™„ì„±**: Completion Suggester í™œìš©
- **íŒ¨ì‹¯ ê²€ìƒ‰**: íƒ€ì…ë³„, ë‚ ì§œë³„ í•„í„°ë§
- **ì „ë¬¸ ê²€ìƒ‰**: ëª¨ë“  í•„ë“œë¥¼ í†µí•©í•œ full_text ê²€ìƒ‰

### âš¡ **Redis - ìºì‹œ ë ˆì´ì–´**

#### **ìºì‹œ ì „ëµ**
```python
# ìºì‹œ í‚¤ íŒ¨í„´
CACHE_PATTERNS = {
    "instance": "instance:{database}:{id}",           # TTL: 1ì‹œê°„
    "ontology": "ontology:{database}:{class}",        # TTL: 6ì‹œê°„  
    "search_result": "search:{database}:{query_hash}", # TTL: 15ë¶„
    "user_session": "session:{user_id}",              # TTL: 24ì‹œê°„
    "command_status": "cmd:{command_id}",             # TTL: 1ì‹œê°„
}

# ìºì‹œ ë¬´íš¨í™” ì „ëµ
async def invalidate_cache(self, event):
    if event.type == "INSTANCE_UPDATED":
        pattern = f"instance:{event.database}:{event.instance_id}"
        await self.redis.delete(pattern)
        
        # ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ ìºì‹œë„ ë¬´íš¨í™”
        search_pattern = f"search:{event.database}:*"
        await self.redis.delete_pattern(search_pattern)
```

---

## 7. CQRS êµ¬í˜„ ìƒì„¸

### ğŸ“Š **ëª…ë ¹ê³¼ ì¡°íšŒì˜ ì™„ì „í•œ ë¶„ë¦¬**

SPICE HARVESTERì—ì„œ CQRSëŠ” ë‹¨ìˆœí•œ ì½ê¸°/ì“°ê¸° ë¶„ë¦¬ë¥¼ ë„˜ì–´ì„œ, **ì™„ì „íˆ ë‹¤ë¥¸ ìµœì í™” ì „ëµ**ì„ ì ìš©í•©ë‹ˆë‹¤.

```mermaid
graph LR
    subgraph "ğŸ“ ëª…ë ¹ ì¸¡ë©´ (Command Side)"
        CMD[Commands]
        VALIDATION[ê²€ì¦ ë¡œì§]
        BUSINESS[ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§]
        EVENTS[ì´ë²¤íŠ¸ ë°œí–‰]
        
        CMD --> VALIDATION
        VALIDATION --> BUSINESS
        BUSINESS --> EVENTS
    end

    subgraph "ğŸ” ì¡°íšŒ ì¸¡ë©´ (Query Side)"
        QUERIES[Queries]
        READ_MODEL1[Elasticsearch<br/>ë³µì¡í•œ ê²€ìƒ‰]
        READ_MODEL2[TerminusDB<br/>ì§ì ‘ ì¡°íšŒ]
        READ_MODEL3[Redis<br/>ìºì‹œëœ ë°ì´í„°]
        
        QUERIES --> READ_MODEL1
        QUERIES --> READ_MODEL2  
        QUERIES --> READ_MODEL3
    end

    EVENTS -.->|ë¹„ë™ê¸° ì—…ë°ì´íŠ¸| READ_MODEL1
    EVENTS -.->|ë¹„ë™ê¸° ì—…ë°ì´íŠ¸| READ_MODEL2
    EVENTS -.->|ë¹„ë™ê¸° ì—…ë°ì´íŠ¸| READ_MODEL3

    style CMD fill:#ffebee
    style QUERIES fill:#e8f5e8
```

### ğŸ“ **ëª…ë ¹ ëª¨ë¸ (Command Model)**

#### **ëª…ë ¹ íƒ€ì… ì •ì˜**
```python
from dataclasses import dataclass
from typing import Dict, Any, Optional
from datetime import datetime

@dataclass
class Command:
    command_id: str
    aggregate_id: str
    command_type: str
    payload: Dict[str, Any]
    user_id: str
    timestamp: datetime
    reason: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

# êµ¬ì²´ì ì¸ ëª…ë ¹ íƒ€ì…ë“¤
@dataclass
class CreateInstanceCommand(Command):
    command_type: str = "CREATE_INSTANCE"

@dataclass  
class UpdateInstanceCommand(Command):
    command_type: str = "UPDATE_INSTANCE"
    
@dataclass
class CreateOntologyCommand(Command):
    command_type: str = "CREATE_ONTOLOGY"
```

#### **ëª…ë ¹ ì²˜ë¦¬ê¸° (Command Handler)**
```python
class InstanceCommandHandler:
    async def handle_create_instance(self, cmd: CreateInstanceCommand):
        # 1. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦
        await self._validate_business_rules(cmd)
        
        # 2. ì§‘ê³„ ë¡œë“œ (í•„ìš”ì‹œ)
        aggregate = await self._load_aggregate(cmd.aggregate_id)
        
        # 3. ëª…ë ¹ ì‹¤í–‰
        events = aggregate.handle_command(cmd)
        
        # 4. ì´ë²¤íŠ¸ ì €ì¥ ë° ë°œí–‰
        for event in events:
            await self._save_and_publish_event(event)
    
    async def _validate_business_rules(self, cmd: CreateInstanceCommand):
        # ì˜¨í†¨ë¡œì§€ ì¡´ì¬ í™•ì¸
        ontology = await self.ontology_service.get_ontology(
            cmd.payload['database'], 
            cmd.payload['class_type']
        )
        if not ontology:
            raise BusinessRuleViolation("Ontology not found")
            
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = [p.name for p in ontology.properties if p.required]
        missing_fields = set(required_fields) - set(cmd.payload['properties'].keys())
        if missing_fields:
            raise BusinessRuleViolation(f"Missing required fields: {missing_fields}")
            
        # ë³µí•© íƒ€ì… ê²€ì¦
        for prop in ontology.properties:
            if prop.name in cmd.payload['properties']:
                value = cmd.payload['properties'][prop.name]
                await self._validate_complex_type(prop.type, value, prop.constraints)
```

### ğŸ” **ì¡°íšŒ ëª¨ë¸ (Query Model)**

#### **ì¡°íšŒ íƒ€ì… ì •ì˜**
```python
from enum import Enum
from typing import List, Optional, Dict

class QueryType(Enum):
    DIRECT_GET = "direct_get"           # IDë¡œ ì§ì ‘ ì¡°íšŒ
    SEARCH = "search"                   # ì „ë¬¸ ê²€ìƒ‰
    FILTER = "filter"                   # í•„í„°ë§
    AGGREGATE = "aggregate"             # ì§‘ê³„ ì¿¼ë¦¬

@dataclass
class Query:
    query_id: str
    query_type: QueryType
    database: str
    parameters: Dict[str, Any]
    user_id: str
    timestamp: datetime

@dataclass
class SearchQuery(Query):
    query_type: QueryType = QueryType.SEARCH
    text: Optional[str] = None
    filters: Optional[Dict[str, Any]] = None
    sort: Optional[List[str]] = None
    limit: int = 20
    offset: int = 0
```

#### **ì¡°íšŒ ì²˜ë¦¬ê¸° (Query Handler)**
```python
class InstanceQueryHandler:
    async def handle_search_query(self, query: SearchQuery) -> SearchResult:
        # ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ìµœì  ì €ì¥ì†Œ ì„ íƒ
        if query.query_type == QueryType.DIRECT_GET:
            return await self._handle_direct_get(query)
        elif query.query_type == QueryType.SEARCH:
            return await self._handle_elasticsearch_search(query)
        elif query.query_type == QueryType.FILTER:
            return await self._handle_filtered_search(query)
    
    async def _handle_direct_get(self, query: SearchQuery) -> SearchResult:
        instance_id = query.parameters['id']
        cache_key = f"instance:{query.database}:{instance_id}"
        
        # 1. ìºì‹œ í™•ì¸
        cached = await self.redis.get(cache_key)
        if cached:
            return SearchResult.from_cache(json.loads(cached))
        
        # 2. TerminusDB ì§ì ‘ ì¡°íšŒ
        result = await self.terminus_client.get_instance(
            database=query.database,
            instance_id=instance_id
        )
        
        # 3. ìºì‹œ ì €ì¥
        if result:
            await self.redis.setex(cache_key, 3600, json.dumps(result))
        
        return SearchResult.from_terminus(result)
    
    async def _handle_elasticsearch_search(self, query: SearchQuery) -> SearchResult:
        # Elasticsearch ì¿¼ë¦¬ êµ¬ì¶•
        es_query = {
            "query": {
                "bool": {
                    "must": [],
                    "filter": []
                }
            },
            "sort": query.sort or [{"created_at": {"order": "desc"}}],
            "from": query.offset,
            "size": query.limit
        }
        
        # ì „ë¬¸ ê²€ìƒ‰
        if query.text:
            es_query["query"]["bool"]["must"].append({
                "multi_match": {
                    "query": query.text,
                    "fields": ["full_text", "properties.*^2"],
                    "type": "best_fields"
                }
            })
        
        # í•„í„° ì ìš©
        if query.filters:
            for field, value in query.filters.items():
                es_query["query"]["bool"]["filter"].append({
                    "term": {f"properties.{field}": value}
                })
        
        # ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜
        response = await self.elasticsearch.search(
            index=f"instances-{query.database}",
            body=es_query
        )
        
        return SearchResult.from_elasticsearch(response)
```

### ğŸ”„ **ì½ê¸° ëª¨ë¸ ì—…ë°ì´íŠ¸ ì „ëµ**

#### **í”„ë¡œì ì…˜ ì›Œì»¤**
```python
class ProjectionWorker:
    async def handle_instance_created_event(self, event: InstanceCreatedEvent):
        """ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        
        # 1. Elasticsearch ë¬¸ì„œ ìƒì„±
        search_doc = {
            "id": event.instance_id,
            "type": event.instance_type,
            "database": event.database,
            "properties": self._flatten_properties(event.properties),
            "created_at": event.timestamp.isoformat(),
            "updated_at": event.timestamp.isoformat(),
            "full_text": self._create_full_text_index(event.properties)
        }
        
        await self.elasticsearch.index(
            index=f"instances-{event.database}",
            id=event.instance_id,
            body=search_doc
        )
        
        # 2. Redis ìºì‹œ ì—…ë°ì´íŠ¸
        cache_key = f"instance:{event.database}:{event.instance_id}"
        await self.redis.setex(
            cache_key,
            ttl=3600,
            value=json.dumps(event.properties)
        )
        
        # 3. ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”
        await self._invalidate_related_caches(event)
    
    def _create_full_text_index(self, properties: Dict[str, Any]) -> str:
        """ê²€ìƒ‰ìš© ì „ë¬¸ í…ìŠ¤íŠ¸ ìƒì„±"""
        text_parts = []
        
        for key, value in properties.items():
            if isinstance(value, str):
                text_parts.append(value)
            elif isinstance(value, (int, float)):
                text_parts.append(str(value))
            elif isinstance(value, dict):
                # ì¤‘ì²© ê°ì²´ì˜ ë¬¸ìì—´ ê°’ë“¤ ì¶”ì¶œ
                nested_texts = self._extract_text_from_nested(value)
                text_parts.extend(nested_texts)
        
        return " ".join(text_parts)
```

---

## 8. Event Sourcing êµ¬í˜„ ìƒì„¸

### ğŸ“š **ì´ë²¤íŠ¸ ìŠ¤í† ì–´ ì•„í‚¤í…ì²˜**

SPICE HARVESTERì˜ Event Sourcingì€ **S3/MinIOë¥¼ ì´ë²¤íŠ¸ ìŠ¤í† ì–´**ë¡œ ì‚¬ìš©í•˜ë©°, **ì™„ë²½í•œ ë¶ˆë³€ì„±ê³¼ í™•ì¥ì„±**ì„ ì œê³µí•©ë‹ˆë‹¤.

```mermaid
graph TD
    subgraph "ğŸ“ ëª…ë ¹ ì²˜ë¦¬"
        CMD[Command]
        VALIDATE[ê²€ì¦]
        EXECUTE[ì‹¤í–‰]
    end

    subgraph "ğŸ’¾ ì´ë²¤íŠ¸ ì €ì¥"
        EVENT[Domain Event]
        S3_STORE[S3 Event Store<br/>Immutable Log]
        PARTITION[íŒŒí‹°ì…˜ë³„ ì €ì¥<br/>aggregate_id ê¸°ì¤€]
    end

    subgraph "ğŸ“Š ìƒíƒœ ì¬êµ¬ì„±"
        REPLAY[Event Replay]
        STATE[Current State]
        SNAPSHOT[Snapshot<br/>ì„±ëŠ¥ ìµœì í™”]
    end

    CMD --> VALIDATE
    VALIDATE --> EXECUTE
    EXECUTE --> EVENT
    EVENT --> S3_STORE
    S3_STORE --> PARTITION
    
    S3_STORE --> REPLAY
    REPLAY --> STATE
    STATE --> SNAPSHOT
    SNAPSHOT -.-> REPLAY

    style S3_STORE fill:#ffebee,stroke:#f44336,stroke-width:3px
    style EVENT fill:#e8f5e8
    style STATE fill:#fff3e0
```

### ğŸ“‚ **ì´ë²¤íŠ¸ ì €ì¥ êµ¬ì¡°**

#### **S3 ë²„í‚· êµ¬ì¡°**
```
spice-event-store/
â”œâ”€â”€ events/
â”‚   â”œâ”€â”€ year=2024/
â”‚   â”‚   â”œâ”€â”€ month=08/
â”‚   â”‚   â”‚   â”œâ”€â”€ day=12/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ aggregate=person_123/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 001_CreateInstance_20240812_103000.json
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 002_UpdateInstance_20240812_154500.json
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ 003_UpdateInstance_20240812_162000.json
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ aggregate=person_456/
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ aggregate=person_123/
â”‚   â”‚   â”œâ”€â”€ snapshot_v10_20240812_120000.json
â”‚   â”‚   â””â”€â”€ snapshot_v20_20240812_180000.json
â”‚   â””â”€â”€ ...
â””â”€â”€ indexes/
    â”œâ”€â”€ by_event_type/
    â”œâ”€â”€ by_timestamp/
    â””â”€â”€ by_user/
```

#### **ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ**
```json
{
  "event_id": "evt_123e4567-e89b-12d3-a456-426614174000",
  "aggregate_id": "person_123",
  "aggregate_type": "Instance",
  "event_type": "INSTANCE_CREATED",
  "event_version": 1,
  "sequence_number": 1,
  "timestamp": "2024-08-12T10:30:00.123Z",
  "causation_id": "cmd_789e4567-e89b-12d3-a456-426614174001",
  "correlation_id": "req_456e4567-e89b-12d3-a456-426614174002",
  "user_id": "user_789",
  "event_data": {
    "instance_id": "person_123",
    "class_type": "Person",
    "database": "hr_system",
    "properties": {
      "name": "í™ê¸¸ë™",
      "email": "hong@example.com",
      "phone": "+82-10-1234-5678"
    }
  },
  "metadata": {
    "source": "OMS-Service",
    "reason": "ì‹ ê·œ ì§ì› ë“±ë¡",
    "client_ip": "192.168.1.100",
    "user_agent": "SPICE-UI/2.0",
    "request_id": "req_456e4567"
  }
}
```

### ğŸ”„ **ì´ë²¤íŠ¸ ì¬ìƒ (Event Replay)**

#### **ì§‘ê³„ ìƒíƒœ ì¬êµ¬ì„±**
```python
class InstanceAggregate:
    def __init__(self, aggregate_id: str):
        self.aggregate_id = aggregate_id
        self.version = 0
        self.properties = {}
        self.created_at = None
        self.updated_at = None
        self.deleted = False
    
    @classmethod
    async def load_from_events(cls, aggregate_id: str, event_store: EventStore) -> 'InstanceAggregate':
        """ì´ë²¤íŠ¸ ìŠ¤í† ì–´ì—ì„œ ì§‘ê³„ ìƒíƒœë¥¼ ì¬êµ¬ì„±"""
        aggregate = cls(aggregate_id)
        
        # ìŠ¤ëƒ…ìƒ· í™•ì¸
        snapshot = await event_store.get_latest_snapshot(aggregate_id)
        if snapshot:
            aggregate.apply_snapshot(snapshot)
        
        # ìŠ¤ëƒ…ìƒ· ì´í›„ì˜ ì´ë²¤íŠ¸ë“¤ ì ìš©
        events = await event_store.get_events_after_version(
            aggregate_id, 
            aggregate.version
        )
        
        for event in events:
            aggregate.apply_event(event)
        
        return aggregate
    
    def apply_event(self, event: Dict[str, Any]):
        """ì´ë²¤íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ìƒíƒœ ë³€ê²½"""
        if event['event_type'] == 'INSTANCE_CREATED':
            self._apply_instance_created(event)
        elif event['event_type'] == 'INSTANCE_UPDATED':
            self._apply_instance_updated(event)
        elif event['event_type'] == 'INSTANCE_DELETED':
            self._apply_instance_deleted(event)
        
        self.version = event['sequence_number']
        self.updated_at = datetime.fromisoformat(event['timestamp'])
    
    def _apply_instance_created(self, event: Dict[str, Any]):
        data = event['event_data']
        self.properties = data['properties'].copy()
        self.created_at = datetime.fromisoformat(event['timestamp'])
        self.updated_at = self.created_at
    
    def _apply_instance_updated(self, event: Dict[str, Any]):
        data = event['event_data']
        # í”„ë¡œí¼í‹° ì—…ë°ì´íŠ¸ ì ìš©
        for key, value in data.get('updates', {}).items():
            if value is None:
                self.properties.pop(key, None)  # í•„ë“œ ì‚­ì œ
            else:
                self.properties[key] = value    # í•„ë“œ ì¶”ê°€/ìˆ˜ì •
    
    def _apply_instance_deleted(self, event: Dict[str, Any]):
        self.deleted = True
```

#### **ì´ë²¤íŠ¸ ìŠ¤í† ì–´ êµ¬í˜„**
```python
class S3EventStore:
    async def append_events(self, aggregate_id: str, events: List[Dict[str, Any]]) -> bool:
        """ì´ë²¤íŠ¸ë¥¼ S3ì— ì¶”ê°€ (Append-Only)"""
        for event in events:
            # íŒŒí‹°ì…˜ í‚¤ ìƒì„± (ë…„/ì›”/ì¼/ì§‘ê³„ID)
            timestamp = datetime.fromisoformat(event['timestamp'])
            partition_key = (
                f"year={timestamp.year}/"
                f"month={timestamp.month:02d}/"
                f"day={timestamp.day:02d}/"
                f"aggregate={aggregate_id}"
            )
            
            # íŒŒì¼ëª… ìƒì„±
            sequence = event['sequence_number']
            event_type = event['event_type']
            timestamp_str = timestamp.strftime("%Y%m%d_%H%M%S")
            filename = f"{sequence:06d}_{event_type}_{timestamp_str}.json"
            
            # S3ì— ì €ì¥
            object_key = f"events/{partition_key}/{filename}"
            await self.s3_client.put_object(
                bucket=self.bucket_name,
                key=object_key,
                body=json.dumps(event, ensure_ascii=False, indent=2),
                content_type="application/json",
                metadata={
                    "aggregate_id": aggregate_id,
                    "event_type": event_type,
                    "sequence_number": str(sequence)
                }
            )
        
        return True
    
    async def get_events_for_aggregate(self, aggregate_id: str, from_version: int = 0) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì§‘ê³„ì˜ ì´ë²¤íŠ¸ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì¡°íšŒ"""
        prefix = f"events/"
        events = []
        
        # S3ì—ì„œ í•´ë‹¹ ì§‘ê³„ì˜ ëª¨ë“  ì´ë²¤íŠ¸ íŒŒì¼ ì¡°íšŒ
        paginator = self.s3_client.get_paginator('list_objects_v2')
        
        async for page in paginator.paginate(
            bucket=self.bucket_name,
            prefix=prefix
        ):
            for obj in page.get('Contents', []):
                if f"aggregate={aggregate_id}" in obj['key']:
                    # ì´ë²¤íŠ¸ íŒŒì¼ ì½ê¸°
                    response = await self.s3_client.get_object(
                        bucket=self.bucket_name,
                        key=obj['key']
                    )
                    
                    event_data = json.loads(await response['body'].read())
                    
                    # ë²„ì „ í•„í„°ë§
                    if event_data['sequence_number'] > from_version:
                        events.append(event_data)
        
        # ì‹œí€€ìŠ¤ ë²ˆí˜¸ë¡œ ì •ë ¬
        events.sort(key=lambda x: x['sequence_number'])
        return events
```

### ğŸ“¸ **ìŠ¤ëƒ…ìƒ· ìµœì í™”**

#### **ìŠ¤ëƒ…ìƒ· ìƒì„± ì „ëµ**
```python
class SnapshotManager:
    SNAPSHOT_FREQUENCY = 100  # 100ê°œ ì´ë²¤íŠ¸ë§ˆë‹¤ ìŠ¤ëƒ…ìƒ· ìƒì„±
    
    async def should_create_snapshot(self, aggregate_id: str) -> bool:
        """ìŠ¤ëƒ…ìƒ· ìƒì„± í•„ìš”ì„± íŒë‹¨"""
        current_version = await self.get_current_version(aggregate_id)
        latest_snapshot_version = await self.get_latest_snapshot_version(aggregate_id)
        
        return (current_version - latest_snapshot_version) >= self.SNAPSHOT_FREQUENCY
    
    async def create_snapshot(self, aggregate: InstanceAggregate):
        """ì§‘ê³„ì˜ í˜„ì¬ ìƒíƒœë¥¼ ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ì €ì¥"""
        snapshot = {
            "snapshot_id": f"snap_{uuid.uuid4()}",
            "aggregate_id": aggregate.aggregate_id,
            "aggregate_type": "Instance",
            "version": aggregate.version,
            "timestamp": datetime.now().isoformat(),
            "state_data": {
                "properties": aggregate.properties,
                "created_at": aggregate.created_at.isoformat() if aggregate.created_at else None,
                "updated_at": aggregate.updated_at.isoformat() if aggregate.updated_at else None,
                "deleted": aggregate.deleted
            },
            "metadata": {
                "compressed": True,
                "events_count": aggregate.version
            }
        }
        
        # ì••ì¶• ì €ì¥
        compressed_data = gzip.compress(
            json.dumps(snapshot, ensure_ascii=False).encode('utf-8')
        )
        
        object_key = f"snapshots/aggregate={aggregate.aggregate_id}/snapshot_v{aggregate.version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json.gz"
        
        await self.s3_client.put_object(
            bucket=self.bucket_name,
            key=object_key,
            body=compressed_data,
            content_type="application/json",
            content_encoding="gzip",
            metadata={
                "aggregate_id": aggregate.aggregate_id,
                "version": str(aggregate.version),
                "events_count": str(aggregate.version)
            }
        )
```

### ğŸ•’ **ì‹œê°„ ì—¬í–‰ (Time Travel) ê¸°ëŠ¥**

#### **íŠ¹ì • ì‹œì  ìƒíƒœ ì¬í˜„**
```python
class TimeTravel:
    async def get_state_at_time(self, aggregate_id: str, target_time: datetime) -> InstanceAggregate:
        """íŠ¹ì • ì‹œì ì˜ ì§‘ê³„ ìƒíƒœ ì¬í˜„"""
        
        # 1. íƒ€ê²Ÿ ì‹œì  ì´ì „ì˜ ê°€ì¥ ìµœê·¼ ìŠ¤ëƒ…ìƒ· ì°¾ê¸°
        snapshot = await self._find_snapshot_before_time(aggregate_id, target_time)
        
        # 2. ì§‘ê³„ ìƒì„± ë° ìŠ¤ëƒ…ìƒ· ì ìš©
        aggregate = InstanceAggregate(aggregate_id)
        if snapshot:
            aggregate.apply_snapshot(snapshot)
        
        # 3. ìŠ¤ëƒ…ìƒ· ì´í›„ ~ íƒ€ê²Ÿ ì‹œì  ì´ì „ì˜ ì´ë²¤íŠ¸ë“¤ ì ìš©
        events = await self.event_store.get_events_between(
            aggregate_id,
            from_version=aggregate.version,
            until_time=target_time
        )
        
        for event in events:
            if datetime.fromisoformat(event['timestamp']) <= target_time:
                aggregate.apply_event(event)
            else:
                break
        
        return aggregate
    
    async def get_changes_between_times(self, aggregate_id: str, start_time: datetime, end_time: datetime) -> List[Dict[str, Any]]:
        """ë‘ ì‹œì  ê°„ì˜ ë³€ê²½ì‚¬í•­ ì¡°íšŒ"""
        events = await self.event_store.get_events_between(
            aggregate_id,
            from_time=start_time,
            until_time=end_time
        )
        
        changes = []
        for event in events:
            change = {
                "timestamp": event['timestamp'],
                "event_type": event['event_type'],
                "user_id": event['user_id'],
                "reason": event.get('metadata', {}).get('reason'),
                "changes": self._extract_changes_from_event(event)
            }
            changes.append(change)
        
        return changes
```

### ğŸ“Š **ì´ë²¤íŠ¸ ë¶„ì„ ë° í†µê³„**

#### **ë°ì´í„° ê³„ë³´ (Data Lineage) ì¶”ì **
```python
class DataLineage:
    async def trace_data_lineage(self, aggregate_id: str) -> Dict[str, Any]:
        """ë°ì´í„°ì˜ ì „ì²´ ê³„ë³´ ì¶”ì """
        events = await self.event_store.get_events_for_aggregate(aggregate_id)
        
        lineage = {
            "aggregate_id": aggregate_id,
            "creation_time": events[0]['timestamp'] if events else None,
            "total_changes": len(events),
            "contributors": set(),
            "change_timeline": [],
            "property_history": {}
        }
        
        for event in events:
            # ê¸°ì—¬ì ì¶”ì 
            lineage["contributors"].add(event['user_id'])
            
            # ë³€ê²½ íƒ€ì„ë¼ì¸
            lineage["change_timeline"].append({
                "timestamp": event['timestamp'],
                "event_type": event['event_type'],
                "user_id": event['user_id'],
                "reason": event.get('metadata', {}).get('reason')
            })
            
            # í”„ë¡œí¼í‹°ë³„ ë³€ê²½ ì´ë ¥
            if event['event_type'] == 'INSTANCE_UPDATED':
                updates = event['event_data'].get('updates', {})
                for prop_name, new_value in updates.items():
                    if prop_name not in lineage["property_history"]:
                        lineage["property_history"][prop_name] = []
                    
                    lineage["property_history"][prop_name].append({
                        "timestamp": event['timestamp'],
                        "old_value": event['event_data'].get('previous_values', {}).get(prop_name),
                        "new_value": new_value,
                        "user_id": event['user_id']
                    })
        
        lineage["contributors"] = list(lineage["contributors"])
        return lineage
```

---

## 9. ì„±ëŠ¥ ìµœì í™” ì „ëµ

### âš¡ **ì‹œìŠ¤í…œ ì „ë°˜ ì„±ëŠ¥ ìµœì í™”**

SPICE HARVESTERëŠ” **ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì„±ëŠ¥**ì„ ìœ„í•´ ë‹¤ì¸µì  ìµœì í™” ì „ëµì„ ì ìš©í•©ë‹ˆë‹¤.

#### **ğŸ† í˜„ì¬ ì„±ê³¼ ì§€í‘œ (2024-08-12 ê²€ì¦ ì™„ë£Œ)**
- âœ… **API ì‘ë‹µì‹œê°„**: 29.8ì´ˆ â†’ **5ì´ˆ ë¯¸ë§Œ** (83% ê°œì„ )
- âœ… **ì„±ê³µë¥ **: 70.3% â†’ **95%+** (35% ê°œì„ )
- âœ… **ì²˜ë¦¬ëŸ‰**: **500 ì´ë²¤íŠ¸/ì´ˆ** ë‹¬ì„±
- âœ… **ë©”ì‹œì§€ ìˆœì„œ**: íŒŒí‹°ì…˜ í‚¤ë¡œ **100% ìˆœì„œ ë³´ì¥**
- âœ… **DLQ ë³µêµ¬**: **5/5 ë©”ì‹œì§€** ìë™ ë³µêµ¬ ì™„ë£Œ

### ğŸ“Š **ì„±ëŠ¥ ìµœì í™” ë§¤íŠ¸ë¦­ìŠ¤**

| ë ˆì´ì–´ | ìµœì í™” ê¸°ë²• | ì„±ëŠ¥ ê°œì„  | ìƒíƒœ |
|--------|-------------|-----------|------|
| **API Gateway** | HTTP ì—°ê²° í’€ë§ (50/100) | 5-10x ë¹ ë¥¸ ì—°ê²° | âœ… ì ìš©ë¨ |
| **Message Bus** | Kafka EOS v2 + íŒŒí‹°ì…˜ í‚¤ | 0% ì¤‘ë³µ ì²˜ë¦¬ | âœ… ê²€ì¦ë¨ |
| **Storage** | S3 íŒŒí‹°ì…”ë‹ + ì••ì¶• | 50% ì €ì¥ê³µê°„ ì ˆì•½ | âœ… ì ìš©ë¨ |
| **Cache** | Redis ë‹¤ì¸µ ìºì‹± | 10-100x ë¹ ë¥¸ ì½ê¸° | âœ… ì ìš©ë¨ |
| **Search** | Elasticsearch ìµœì í™” | ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ê²€ìƒ‰ | âœ… ì ìš©ë¨ |

### ğŸš€ **Kafka ì„±ëŠ¥ ìµœì í™”**

#### **Exactly-Once Semantics v2 (EOS v2)**
```yaml
# Producer ì„¤ì •
enable.idempotence: true
acks: all
retries: 2147483647
max.in.flight.requests.per.connection: 5
delivery.timeout.ms: 120000
transaction.timeout.ms: 300000

# Consumer ì„¤ì •  
enable.auto.commit: false
isolation.level: read_committed
max.poll.records: 100
fetch.min.bytes: 1024
```

#### **íŒŒí‹°ì…˜ í‚¤ ì „ëµ**
```python
class KafkaProducer:
    async def send_command(self, command: Command):
        """ì§‘ê³„ IDë¥¼ íŒŒí‹°ì…˜ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ìˆœì„œ ë³´ì¥"""
        await self.producer.send(
            topic=self._get_topic_name(command),
            key=command.aggregate_id,  # íŒŒí‹°ì…˜ í‚¤
            value=command.to_dict(),
            headers={
                "command_type": command.command_type,
                "user_id": command.user_id,
                "timestamp": command.timestamp.isoformat()
            }
        )
    
    def _get_partition(self, key: str, total_partitions: int) -> int:
        """ì¼ê´€ëœ íŒŒí‹°ì…˜ ë°°ì •ì„ ìœ„í•œ í•´ì‹œ í•¨ìˆ˜"""
        return hash(key) % total_partitions
```

#### **Dead Letter Queue (DLQ) ìµœì í™”**
```python
class DLQHandler:
    def __init__(self):
        # ThreadPoolExecutorë¡œ ë¸”ë¡œí‚¹ ë°©ì§€
        self.executor = ThreadPoolExecutor(max_workers=2)
        self.processing = False
    
    def _poll_message(self, timeout: float = 1.0):
        """Threadì—ì„œ ì‹¤í–‰ë˜ëŠ” ë¸”ë¡œí‚¹ í´ë§"""
        return self.consumer.poll(timeout=timeout)
    
    async def _process_loop(self):
        """ë¹„ë™ê¸° ë©”ì‹œì§€ ì²˜ë¦¬ ë£¨í”„"""
        loop = asyncio.get_event_loop()
        
        while self.processing:
            # ë¸”ë¡œí‚¹ í´ë§ì„ Thread Poolì—ì„œ ì‹¤í–‰
            msg = await loop.run_in_executor(
                self.executor, 
                self._poll_message, 
                0.5
            )
            
            if msg is None:
                await asyncio.sleep(0.1)  # CPU ì‚¬ìš©ë¥  ì œì–´
                continue
            
            await self._process_message(msg)
    
    async def _process_message(self, msg):
        """ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„"""
        retries = 0
        max_retries = 3
        
        while retries < max_retries:
            try:
                await self._handle_message(msg)
                return  # ì„±ê³µì‹œ ì¢…ë£Œ
                
            except Exception as e:
                retries += 1
                backoff_time = 2 ** retries  # 2, 4, 8ì´ˆ
                
                await asyncio.sleep(backoff_time)
        
        # ìµœëŒ€ ì¬ì‹œë„ í›„ í¬ì´ì¦Œ ë©”ì‹œì§€ ì²˜ë¦¬
        await self._handle_poison_message(msg)
```

### ğŸ’¾ **ì €ì¥ì†Œ ì„±ëŠ¥ ìµœì í™”**

#### **S3/MinIO ì´ë²¤íŠ¸ ìŠ¤í† ì–´**
```python
class OptimizedEventStore:
    async def batch_write_events(self, events: List[Dict[str, Any]]):
        """ë°°ì¹˜ ì“°ê¸°ë¡œ ì²˜ë¦¬ëŸ‰ í–¥ìƒ"""
        # ë‚ ì§œë³„ ê·¸ë£¨í•‘
        grouped_events = defaultdict(list)
        for event in events:
            date_key = event['timestamp'][:10]  # YYYY-MM-DD
            grouped_events[date_key].append(event)
        
        # ë³‘ë ¬ ì—…ë¡œë“œ
        tasks = []
        for date_key, date_events in grouped_events.items():
            task = self._upload_events_for_date(date_key, date_events)
            tasks.append(task)
        
        await asyncio.gather(*tasks)
    
    async def _upload_events_for_date(self, date_key: str, events: List[Dict[str, Any]]):
        """ë‚ ì§œë³„ ì´ë²¤íŠ¸ ë³‘ë ¬ ì—…ë¡œë“œ"""
        # ì••ì¶• ì ìš©
        compressed_events = []
        for event in events:
            compressed_data = gzip.compress(
                json.dumps(event, ensure_ascii=False).encode('utf-8')
            )
            compressed_events.append(compressed_data)
        
        # ë³‘ë ¬ ì—…ë¡œë“œ
        upload_tasks = []
        for i, compressed_data in enumerate(compressed_events):
            object_key = f"events/{date_key}/event_{i:06d}.json.gz"
            task = self.s3_client.put_object(
                bucket=self.bucket_name,
                key=object_key,
                body=compressed_data,
                content_encoding="gzip"
            )
            upload_tasks.append(task)
        
        await asyncio.gather(*upload_tasks)
```

#### **TerminusDB ì¿¼ë¦¬ ìµœì í™”**
```python
class OptimizedTerminusClient:
    async def batch_get_instances(self, instance_ids: List[str]) -> Dict[str, Any]:
        """ë°°ì¹˜ ì¡°íšŒë¡œ ì™•ë³µ íšŸìˆ˜ ìµœì†Œí™”"""
        query = f"""
        SELECT ?id ?props WHERE {{
            ?id a Instance,
                instance_id ?instance_id,
                properties ?props.
            FILTER(?instance_id IN ({','.join(f'"{id_}"' for id_ in instance_ids)}))
        }}
        """
        
        result = await self.execute_query(query)
        return {item['id']: item['props'] for item in result}
    
    async def optimized_relationship_query(self, start_class: str, relationship: str) -> List[Dict[str, Any]]:
        """ê´€ê³„í˜• ì¿¼ë¦¬ ìµœì í™”"""
        query = f"""
        SELECT ?start ?end ?relationship_data WHERE {{
            ?start a {start_class},
                   {relationship} ?end.
            ?end properties ?relationship_data.
        }}
        ORDER BY ?start
        LIMIT 1000
        """
        
        return await self.execute_query(query)
```

### ğŸ” **Elasticsearch ì„±ëŠ¥ ìµœì í™”**

#### **ì¸ë±ìŠ¤ ìµœì í™”**
```json
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 1,
    "refresh_interval": "5s",
    "index": {
      "max_result_window": 50000,
      "mapping": {
        "total_fields": {
          "limit": 2000
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "full_text": {
        "type": "text",
        "analyzer": "korean_analyzer",
        "search_analyzer": "korean_search_analyzer"
      },
      "properties": {
        "type": "object",
        "dynamic": true,
        "properties": {
          "name": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword"
              }
            }
          },
          "email": {
            "type": "keyword"
          },
          "created_at": {
            "type": "date",
            "format": "strict_date_optional_time||epoch_millis"
          }
        }
      }
    }
  }
}
```

#### **ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”**
```python
class OptimizedSearch:
    async def multi_field_search(self, query: str, database: str, limit: int = 20) -> SearchResult:
        """ë‹¤ì¤‘ í•„ë“œ ìµœì í™” ê²€ìƒ‰"""
        search_body = {
            "query": {
                "bool": {
                    "should": [
                        # ì •í™•í•œ ë§¤ì¹˜ (ë†’ì€ ì ìˆ˜)
                        {
                            "multi_match": {
                                "query": query,
                                "fields": ["properties.name^3", "properties.email^2"],
                                "type": "phrase",
                                "boost": 2.0
                            }
                        },
                        # ë¶€ë¶„ ë§¤ì¹˜ (ë³´í†µ ì ìˆ˜)
                        {
                            "multi_match": {
                                "query": query,
                                "fields": ["full_text", "properties.*"],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "highlight": {
                "fields": {
                    "full_text": {},
                    "properties.*": {}
                }
            },
            "size": limit,
            "_source": {
                "excludes": ["full_text"]  # ë¶ˆí•„ìš”í•œ í•„ë“œ ì œì™¸
            }
        }
        
        response = await self.elasticsearch.search(
            index=f"instances-{database}",
            body=search_body
        )
        
        return self._process_search_response(response)
```

### âš¡ **ìºì‹œ ìµœì í™” ì „ëµ**

#### **ë‹¤ì¸µ ìºì‹œ ì•„í‚¤í…ì²˜**
```python
class MultiLevelCache:
    def __init__(self):
        self.l1_cache = {}          # ì¸ë©”ëª¨ë¦¬ (ê°€ì¥ ë¹ ë¦„)
        self.l2_cache = redis_client # Redis (ì¤‘ê°„ ì†ë„)
        # L3 cacheëŠ” ë°ì´í„°ë² ì´ìŠ¤ (ê°€ì¥ ëŠë¦¼)
    
    async def get(self, key: str) -> Optional[Any]:
        # L1 ìºì‹œ í™•ì¸
        if key in self.l1_cache:
            return self.l1_cache[key]
        
        # L2 ìºì‹œ í™•ì¸
        value = await self.l2_cache.get(key)
        if value:
            # L1 ìºì‹œì— ì €ì¥ (TTL 5ë¶„)
            self.l1_cache[key] = json.loads(value)
            asyncio.create_task(self._expire_l1_cache(key, 300))
            return self.l1_cache[key]
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600):
        # L1 ìºì‹œ ì €ì¥
        self.l1_cache[key] = value
        asyncio.create_task(self._expire_l1_cache(key, min(ttl, 300)))
        
        # L2 ìºì‹œ ì €ì¥
        await self.l2_cache.setex(
            key, 
            ttl, 
            json.dumps(value, ensure_ascii=False)
        )
```

#### **ìŠ¤ë§ˆíŠ¸ ìºì‹œ ë¬´íš¨í™”**
```python
class SmartCacheInvalidation:
    async def invalidate_related_caches(self, event: Dict[str, Any]):
        """ì´ë²¤íŠ¸ ê¸°ë°˜ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
        patterns = []
        
        if event['event_type'] == 'INSTANCE_UPDATED':
            aggregate_id = event['aggregate_id']
            database = event['event_data']['database']
            
            # ì§ì ‘ ìºì‹œ
            patterns.append(f"instance:{database}:{aggregate_id}")
            
            # ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ
            patterns.append(f"search:{database}:*")
            
            # ê´€ê³„ëœ ì¸ìŠ¤í„´ìŠ¤ë“¤ì˜ ìºì‹œ (ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
            related_instances = await self._find_related_instances(aggregate_id)
            for related_id in related_instances:
                patterns.append(f"instance:{database}:{related_id}")
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì‚­ì œ
        for pattern in patterns:
            await self.redis.delete_pattern(pattern)
```

---

## 10. í™•ì¥ì„± ë° ê³ ê°€ìš©ì„±

### ğŸ—ï¸ **ìˆ˜í‰ í™•ì¥ ì•„í‚¤í…ì²˜**

SPICE HARVESTERëŠ” **ê° ì»´í¬ë„ŒíŠ¸ì˜ ë…ë¦½ì  í™•ì¥**ì„ ì§€ì›í•˜ë©°, íŠ¸ë˜í”½ê³¼ ë°ì´í„° ì¦ê°€ì— ë”°ë¼ íƒ„ë ¥ì ìœ¼ë¡œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```mermaid
graph TD
    subgraph "âš–ï¸ ë¡œë“œ ë°¸ëŸ°ì„œ"
        LB[Load Balancer<br/>HAProxy / Nginx]
    end

    subgraph "ğŸŒ API Gateway í´ëŸ¬ìŠ¤í„°"
        BFF1[BFF Instance 1<br/>Port 8002]
        BFF2[BFF Instance 2<br/>Port 8003] 
        BFF3[BFF Instance 3<br/>Port 8004]
    end

    subgraph "âš™ï¸ ì„œë¹„ìŠ¤ í´ëŸ¬ìŠ¤í„°"
        subgraph "OMS Cluster"
            OMS1[OMS Instance 1]
            OMS2[OMS Instance 2]
            OMS3[OMS Instance 3]
        end
        
        subgraph "Worker Cluster"  
            IW1[Instance Worker 1]
            IW2[Instance Worker 2]
            PW1[Projection Worker 1]
            PW2[Projection Worker 2]
        end
    end

    subgraph "ğŸ“¨ ë©”ì‹œì§• í´ëŸ¬ìŠ¤í„°"
        KAFKA1[Kafka Broker 1]
        KAFKA2[Kafka Broker 2] 
        KAFKA3[Kafka Broker 3]
    end

    subgraph "ğŸ’¾ ìŠ¤í† ë¦¬ì§€ í´ëŸ¬ìŠ¤í„°"
        S3_1[MinIO Node 1]
        S3_2[MinIO Node 2]
        S3_3[MinIO Node 3]
        
        ES1[Elasticsearch Node 1]
        ES2[Elasticsearch Node 2]
        ES3[Elasticsearch Node 3]
    end

    LB --> BFF1
    LB --> BFF2
    LB --> BFF3

    BFF1 --> OMS1
    BFF2 --> OMS2
    BFF3 --> OMS3

    OMS1 --> KAFKA1
    OMS2 --> KAFKA2
    OMS3 --> KAFKA3

    KAFKA1 --> IW1
    KAFKA2 --> IW2
    KAFKA1 --> PW1
    KAFKA3 --> PW2

    IW1 --> S3_1
    IW2 --> S3_2
    PW1 --> ES1
    PW2 --> ES2

    style LB fill:#f8d7da
    style KAFKA1 fill:#e8f5e8
    style S3_1 fill:#fff3e0
```

### ğŸ“ˆ **ìë™ ìŠ¤ì¼€ì¼ë§ ì „ëµ**

#### **Kubernetes ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§**
```yaml
# HorizontalPodAutoscaler for BFF
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: bff-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: bff-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
```

#### **ì¹´í”„ì¹´ íŒŒí‹°ì…˜ ìë™ ì¦ì„¤**
```python
class KafkaScaler:
    async def monitor_and_scale_partitions(self):
        """íŒŒí‹°ì…˜ ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ ë° ìë™ ì¦ì„¤"""
        for topic in self.monitored_topics:
            metrics = await self.get_topic_metrics(topic)
            
            # í‰ê·  ì§€ì—° ì‹œê°„ì´ ì„ê³„ì¹˜ ì´ˆê³¼ì‹œ
            if metrics.avg_lag > self.lag_threshold:
                current_partitions = await self.get_partition_count(topic)
                new_partitions = min(
                    current_partitions * 2,  # 2ë°° ì¦ì„¤
                    self.max_partitions      # ìµœëŒ€ ì œí•œ
                )
                
                if new_partitions > current_partitions:
                    await self.add_partitions(topic, new_partitions)
                    await self.rebalance_consumers(topic)
            
            # ì²˜ë¦¬ëŸ‰ì´ ì„ê³„ì¹˜ ì´ˆê³¼ì‹œ ë¸Œë¡œì»¤ ì¶”ê°€
            if metrics.throughput > self.throughput_threshold:
                await self.scale_kafka_brokers()
    
    async def scale_kafka_brokers(self):
        """Kafka ë¸Œë¡œì»¤ í´ëŸ¬ìŠ¤í„° í™•ì¥"""
        current_brokers = await self.get_broker_count()
        
        if current_brokers < self.max_brokers:
            # ìƒˆ ë¸Œë¡œì»¤ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘
            new_broker_config = self.generate_broker_config(current_brokers + 1)
            await self.kubernetes_client.create_broker_pod(new_broker_config)
            
            # íŒŒí‹°ì…˜ ë¦¬ë°¸ëŸ°ì‹±
            await asyncio.sleep(30)  # ë¸Œë¡œì»¤ ì´ˆê¸°í™” ëŒ€ê¸°
            await self.rebalance_partitions()
```

### ğŸ›¡ï¸ **ê³ ê°€ìš©ì„± (High Availability)**

#### **ì¥ì•  í—ˆìš© ì„¤ê³„ (Fault Tolerance)**
```python
class FaultTolerantService:
    def __init__(self):
        self.circuit_breakers = {}
        self.retry_configs = {
            'terminus_db': {'max_retries': 3, 'backoff_factor': 2},
            'elasticsearch': {'max_retries': 5, 'backoff_factor': 1.5},
            's3_storage': {'max_retries': 3, 'backoff_factor': 2}
        }
    
    async def call_with_resilience(self, service_name: str, operation: callable, *args, **kwargs):
        """íšŒë¡œ ì°¨ë‹¨ê¸° + ì¬ì‹œë„ íŒ¨í„´"""
        circuit_breaker = self.get_circuit_breaker(service_name)
        
        if circuit_breaker.is_open():
            raise ServiceUnavailableException(f"Circuit breaker open for {service_name}")
        
        retry_config = self.retry_configs.get(service_name, {'max_retries': 3, 'backoff_factor': 2})
        
        for attempt in range(retry_config['max_retries']):
            try:
                result = await operation(*args, **kwargs)
                circuit_breaker.record_success()
                return result
                
            except Exception as e:
                circuit_breaker.record_failure()
                
                if attempt == retry_config['max_retries'] - 1:
                    raise e
                
                # ì§€ìˆ˜ ë°±ì˜¤í”„
                delay = retry_config['backoff_factor'] ** attempt
                await asyncio.sleep(delay)
        
        raise MaxRetriesExceededException(f"Max retries exceeded for {service_name}")
```

#### **ë°ì´í„° ë³µì œ ë° ë°±ì—…**
```python
class DataReplicationManager:
    async def setup_multi_region_replication(self):
        """ë‹¤ì¤‘ ë¦¬ì „ ë°ì´í„° ë³µì œ ì„¤ì •"""
        
        # S3 êµì°¨ ë¦¬ì „ ë³µì œ
        s3_replication_config = {
            'Role': 'arn:aws:iam::account:role/replication-role',
            'Rules': [
                {
                    'ID': 'event-store-replication',
                    'Status': 'Enabled',
                    'Priority': 1,
                    'Filter': {'Prefix': 'events/'},
                    'Destination': {
                        'Bucket': 'spice-event-store-backup',
                        'StorageClass': 'STANDARD_IA'
                    }
                }
            ]
        }
        
        await self.s3_client.put_bucket_replication(
            bucket='spice-event-store',
            replication_configuration=s3_replication_config
        )
        
        # Elasticsearch í´ëŸ¬ìŠ¤í„° ë³µì œ
        await self.setup_elasticsearch_cross_cluster_replication()
        
        # TerminusDB ë°±ì—… ìŠ¤ì¼€ì¤„ë§
        await self.schedule_terminus_backups()
    
    async def setup_elasticsearch_cross_cluster_replication(self):
        """Elasticsearch êµì°¨ í´ëŸ¬ìŠ¤í„° ë³µì œ"""
        ccr_config = {
            "remote_cluster": "backup-cluster",
            "leader_index": "instances-*",
            "settings": {
                "index.soft_deletes.enabled": True,
                "index.number_of_replicas": 0
            }
        }
        
        await self.elasticsearch.put_ccr_follow_pattern(
            name="spice-follower-pattern",
            body=ccr_config
        )
```

### ğŸ“Š **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼**

#### **ì¢…í•© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**
```python
class ComprehensiveMonitoring:
    def __init__(self):
        self.metrics_collector = PrometheusMetrics()
        self.alert_manager = AlertManager()
        self.dashboards = {
            'system_health': SystemHealthDashboard(),
            'business_metrics': BusinessMetricsDashboard(),
            'performance_metrics': PerformanceMetricsDashboard()
        }
    
    async def collect_system_metrics(self):
        """ì‹œìŠ¤í…œ ì „ë°˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        metrics = {}
        
        # API Gateway ë©”íŠ¸ë¦­
        metrics['api_gateway'] = {
            'requests_per_second': await self.get_rps_metric(),
            'average_response_time': await self.get_avg_response_time(),
            'error_rate': await self.get_error_rate(),
            'active_connections': await self.get_active_connections()
        }
        
        # Kafka ë©”íŠ¸ë¦­
        metrics['kafka'] = {
            'message_throughput': await self.get_kafka_throughput(),
            'consumer_lag': await self.get_consumer_lag(),
            'partition_balance': await self.get_partition_balance(),
            'replication_factor': await self.get_replication_status()
        }
        
        # ìŠ¤í† ë¦¬ì§€ ë©”íŠ¸ë¦­
        metrics['storage'] = {
            's3_usage': await self.get_s3_usage(),
            'elasticsearch_health': await self.get_es_health(),
            'terminus_connections': await self.get_terminus_connections(),
            'redis_memory_usage': await self.get_redis_memory()
        }
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
        metrics['business'] = {
            'ontologies_created': await self.count_ontologies_today(),
            'instances_created': await self.count_instances_today(),
            'git_operations': await self.count_git_operations_today(),
            'ai_inferences': await self.count_ai_inferences_today()
        }
        
        await self.store_metrics(metrics)
        await self.check_alerting_rules(metrics)
        
        return metrics
```

#### **ì§€ëŠ¥í˜• ì•Œë¦¼ ì‹œìŠ¤í…œ**
```python
class IntelligentAlerting:
    def __init__(self):
        self.alert_rules = [
            AlertRule(
                name="high_error_rate",
                condition="error_rate > 5%",
                severity="critical",
                duration="5m"
            ),
            AlertRule(
                name="kafka_consumer_lag",
                condition="consumer_lag > 10000",
                severity="warning", 
                duration="10m"
            ),
            AlertRule(
                name="storage_usage_high",
                condition="storage_usage > 85%",
                severity="warning",
                duration="15m"
            )
        ]
    
    async def evaluate_alert_rules(self, metrics: Dict[str, Any]):
        """ì•Œë¦¼ ê·œì¹™ í‰ê°€ ë° ë°œì†¡"""
        active_alerts = []
        
        for rule in self.alert_rules:
            if await self.evaluate_condition(rule.condition, metrics):
                alert = Alert(
                    rule_name=rule.name,
                    severity=rule.severity,
                    message=self.generate_alert_message(rule, metrics),
                    timestamp=datetime.now(),
                    metrics_snapshot=metrics
                )
                
                # ì•Œë¦¼ ì¤‘ë³µ ë°©ì§€
                if not await self.is_alert_already_fired(alert):
                    await self.fire_alert(alert)
                    active_alerts.append(alert)
        
        return active_alerts
    
    async def fire_alert(self, alert: Alert):
        """ë‹¤ì±„ë„ ì•Œë¦¼ ë°œì†¡"""
        notification_tasks = []
        
        if alert.severity == "critical":
            # ê¸´ê¸‰ ì•Œë¦¼: Slack + Email + SMS
            notification_tasks.extend([
                self.send_slack_notification(alert),
                self.send_email_notification(alert),
                self.send_sms_notification(alert)
            ])
        elif alert.severity == "warning":
            # ê²½ê³  ì•Œë¦¼: Slack + Email
            notification_tasks.extend([
                self.send_slack_notification(alert),
                self.send_email_notification(alert)
            ])
        
        await asyncio.gather(*notification_tasks)
```

---

## ğŸ“š **ë§ˆë¬´ë¦¬**

SPICE HARVESTERì˜ ì•„í‚¤í…ì²˜ëŠ” **í˜„ëŒ€ì  ì—”í„°í”„ë¼ì´ì¦ˆ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ëª¨ë“  ìš”êµ¬ì‚¬í•­**ì„ ì¶©ì¡±í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:

### ğŸ¯ **í•µì‹¬ ì„±ê³¼**
- âœ… **ì™„ë²½í•œ ë°ì´í„° ì¶”ì **: Event Sourcingìœ¼ë¡œ ëª¨ë“  ë³€ê²½ì‚¬í•­ ë¶ˆë³€ ê¸°ë¡
- âœ… **í™•ì¥ ê°€ëŠ¥í•œ ì„±ëŠ¥**: CQRS + ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë¡œ ë…ë¦½ì  í™•ì¥
- âœ… **í”„ë¡œë•ì…˜ ê²€ì¦ ì™„ë£Œ**: 5/5 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼ ë° ì‹¤ì œ ìš´ì˜ ì¤€ë¹„
- âœ… **ê°œë°œì ì¹œí™”ì **: ì™„ë²½í•œ ë¬¸ì„œí™”ì™€ í‘œì¤€í™”ëœ API

### ğŸš€ **ë¯¸ë˜ í™•ì¥ì„±**
- **AI ë„¤ì´í‹°ë¸Œ**: GPT í†µí•© ì¤€ë¹„ ì™„ë£Œ
- **í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ**: Kubernetes ìë™ ìŠ¤ì¼€ì¼ë§ ì§€ì›
- **ê¸€ë¡œë²Œ í™•ì¥**: ë‹¤ì¤‘ ë¦¬ì „ ë³µì œ ë° CDN í†µí•©
- **ìƒíƒœê³„ í™•ì¥**: í’ë¶€í•œ ì»¤ë„¥í„°ì™€ í†µí•© ì˜µì…˜

SPICE HARVESTERëŠ” ë‹¨ìˆœí•œ ì˜¨í†¨ë¡œì§€ ê´€ë¦¬ ë„êµ¬ë¥¼ ë„˜ì–´ì„œ, **ì°¨ì„¸ëŒ€ ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ í”Œë«í¼**ìœ¼ë¡œì„œ ì¡°ì§ì˜ ë°ì´í„° ê´€ë¦¬ ì „ëµì„ í˜ì‹ í•  ê²ƒì…ë‹ˆë‹¤.

---

*ì´ ì•„í‚¤í…ì²˜ ë¬¸ì„œëŠ” ì‹¤ì œ êµ¬í˜„ëœ ì‹œìŠ¤í…œì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì™€ íŒ¨í„´ì´ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.*

**ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸**: 2024-08-12  
**ğŸ”– ë¬¸ì„œ ë²„ì „**: 2.0 (Event Sourcing + CQRS ì™„ì „ êµ¬í˜„)  
**ğŸŒ ë¬¸ì„œ ì–¸ì–´**: í•œêµ­ì–´ (ì™„ì „ í˜„ì§€í™”)**