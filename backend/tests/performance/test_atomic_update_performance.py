"""
ì›ìì  ì—…ë°ì´íŠ¸ ì„±ëŠ¥ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸
ëŒ€ìš©ëŸ‰ ë°ì´í„°ì™€ ë†’ì€ ë¶€í•˜ ìƒí™©ì—ì„œì˜ ì›ìì  ì—…ë°ì´íŠ¸ ì„±ëŠ¥ ê²€ì¦

ì´ í…ŒìŠ¤íŠ¸ëŠ” ë‹¤ìŒì„ ê²€ì¦í•©ë‹ˆë‹¤:
1. ëŒ€ìš©ëŸ‰ ì˜¨í†¨ë¡œì§€ ì²˜ë¦¬ ì„±ëŠ¥
2. ë™ì‹œ ì—…ë°ì´íŠ¸ ë¶€í•˜ í…ŒìŠ¤íŠ¸
3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ëˆ„ìˆ˜ ê²€ì¦
4. íŠ¸ëœì­ì…˜ ì²˜ë¦¬ ì„±ëŠ¥
5. ë°±ì—…/ë³µì› ì„±ëŠ¥
"""

import pytest

# No need for sys.path.insert - using proper spice_harvester package imports
import asyncio
import time
import psutil
import gc
from typing import List, Dict, Any
from unittest.mock import AsyncMock, patch

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
import os
from oms.services.async_terminus import (
    AsyncTerminusService,
    AsyncDatabaseError,
    AsyncOntologyNotFoundError
)
from shared.models.config import ConnectionConfig

@pytest.fixture
def performance_connection_config():
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ì—°ê²° ì„¤ì •"""
    return ConnectionConfig(
        server_url="http://localhost:6363",
        user="admin",
        account="admin",
        key="test_key",
        timeout=60,  # ê¸´ íƒ€ì„ì•„ì›ƒ
        retry_attempts=1,  # ì¬ì‹œë„ ìµœì†Œí™”
        retry_delay=0.1
    )

@pytest.fixture
def terminus_service(performance_connection_config):
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© TerminusDB ì„œë¹„ìŠ¤"""
    return AsyncTerminusService(performance_connection_config)

@pytest.fixture
def large_ontology_data():
    """ëŒ€ìš©ëŸ‰ ì˜¨í†¨ë¡œì§€ ë°ì´í„°"""
    return {
        "id": "LargePerformanceClass",
        "label": {"ko": "ëŒ€ìš©ëŸ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤", "en": "Large Performance Test Class"},
        "properties": [
            {"name": f"property_{i}", "type": "xsd:string"}
            for i in range(1000)  # 1000ê°œì˜ ì†ì„±
        ],
        "relationships": [
            {"predicate": f"relation_{i}", "target": f"Target_{i % 10}"}
            for i in range(100)  # 100ê°œì˜ ê´€ê³„
        ]
    }

class TestPerformanceBasics:
    """ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_single_update_performance(self, terminus_service, large_ontology_data):
        """ë‹¨ì¼ ì—…ë°ì´íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        db_name = "performance_test_db"
        
        # ì—…ë°ì´íŠ¸ ë°ì´í„° ì¤€ë¹„
        update_data = {
            "label": {"ko": "ì—…ë°ì´íŠ¸ëœ ëŒ€ìš©ëŸ‰ í´ë˜ìŠ¤", "en": "Updated Large Class"},
            "properties": [
                {"name": f"property_{i}", "type": "xsd:string"}
                for i in range(1200)  # 1200ê°œë¡œ ì¦ê°€
            ]
        }
        
        with patch.object(terminus_service, 'ensure_db_exists', return_value=None):
            with patch.object(terminus_service, 'get_ontology', return_value=large_ontology_data):
                with patch.object(terminus_service, '_make_request', return_value={"success": True}):
                    
                    # ì„±ëŠ¥ ì¸¡ì •
                    start_time = time.time()
                    memory_before = psutil.Process().memory_info().rss
                    
                    result = await terminus_service.update_ontology_atomic_patch(
                        db_name, "LargePerformanceClass", update_data
                    )
                    
                    end_time = time.time()
                    memory_after = psutil.Process().memory_info().rss
                    
                    # ì„±ëŠ¥ ê²€ì¦
                    execution_time = end_time - start_time
                    memory_used = memory_after - memory_before
                    
                    print(f"ğŸ” ë‹¨ì¼ ì—…ë°ì´íŠ¸ ì„±ëŠ¥:")
                    print(f"   ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
                    print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used / 1024 / 1024:.2f}MB")
                    
                    # ì„±ëŠ¥ ì„ê³„ê°’ ê²€ì¦
                    assert execution_time < 5.0, f"ì—…ë°ì´íŠ¸ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {execution_time:.3f}ì´ˆ"
                    assert memory_used < 100 * 1024 * 1024, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤: {memory_used / 1024 / 1024:.2f}MB"
                    
                    # ê²°ê³¼ ê²€ì¦
                    assert result["id"] == "LargePerformanceClass"
                    assert result["method"] == "atomic_patch"
    
    @pytest.mark.asyncio
    async def test_transaction_performance(self, terminus_service, large_ontology_data):
        """íŠ¸ëœì­ì…˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        db_name = "performance_test_db"
        
        update_data = {
            "description": {"ko": "íŠ¸ëœì­ì…˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸", "en": "Transaction Performance Test"}
        }
        
        with patch.object(terminus_service, 'ensure_db_exists', return_value=None):
            with patch.object(terminus_service, 'get_ontology', return_value=large_ontology_data):
                with patch.object(terminus_service, '_create_backup_before_update', return_value={"backup_id": "backup_123"}):
                    with patch.object(terminus_service, '_begin_transaction', return_value="tx_456"):
                        with patch.object(terminus_service, '_make_request', return_value={"success": True}):
                            with patch.object(terminus_service, '_commit_transaction', return_value=None):
                                
                                # ì„±ëŠ¥ ì¸¡ì •
                                start_time = time.time()
                                
                                result = await terminus_service.update_ontology_atomic_transaction(
                                    db_name, "LargePerformanceClass", update_data
                                )
                                
                                end_time = time.time()
                                execution_time = end_time - start_time
                                
                                print(f"ğŸ” íŠ¸ëœì­ì…˜ ì„±ëŠ¥:")
                                print(f"   ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
                                
                                # íŠ¸ëœì­ì…˜ì€ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
                                assert execution_time < 10.0, f"íŠ¸ëœì­ì…˜ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {execution_time:.3f}ì´ˆ"
                                
                                # ê²°ê³¼ ê²€ì¦
                                assert result["method"] == "atomic_transaction"

class TestConcurrencyPerformance:
    """ë™ì‹œì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_concurrent_updates_performance(self, terminus_service):
        """ë™ì‹œ ì—…ë°ì´íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        db_name = "concurrent_test_db"
        
        # ì „ì—­ ëª¨í‚¹ ì„¤ì • - ëª¨ë“  ë©”ì†Œë“œë¥¼ í•œë²ˆì— ëª¨í‚¹
        with patch.object(terminus_service, 'ensure_db_exists', return_value=None):
            with patch.object(terminus_service, '_make_request', return_value={"success": True}):
                # get_ontologyë¥¼ ì „ì—­ìœ¼ë¡œ ëª¨í‚¹í•˜ì—¬ ëª¨ë“  íƒœìŠ¤í¬ì— ì ìš©
                with patch.object(terminus_service, 'get_ontology') as mock_get_ontology:
                    
                    # ë™ì‹œ ì—…ë°ì´íŠ¸ ë°ì´í„° ì¤€ë¹„
                    update_tasks = []
                    for i in range(20):  # 20ê°œì˜ ë™ì‹œ ì—…ë°ì´íŠ¸
                        update_data = {
                            "label": {"ko": f"ë™ì‹œ ì—…ë°ì´íŠ¸ {i}", "en": f"Concurrent Update {i}"},
                            "properties": [
                                {"name": f"prop_{i}_{j}", "type": "xsd:string"}
                                for j in range(50)  # ê°ê° 50ê°œì˜ ì†ì„±
                            ]
                        }
                        
                        sample_ontology = {
                            "id": f"ConcurrentClass_{i}",
                            "label": {"ko": f"ë™ì‹œ í´ë˜ìŠ¤ {i}", "en": f"Concurrent Class {i}"},
                            "properties": [
                                {"name": f"prop_{i}_{j}", "type": "xsd:string"}
                                for j in range(30)  # ê¸°ì¡´ 30ê°œ ì†ì„±
                            ]
                        }
                        
                        # ê° íƒœìŠ¤í¬ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ëª¨í‚¹ëœ ë°˜í™˜ê°’ ì„¤ì •
                        mock_get_ontology.return_value = sample_ontology
                        
                        task = terminus_service.update_ontology_atomic_patch(
                            db_name, f"ConcurrentClass_{i}", update_data
                        )
                        update_tasks.append(task)
                    
                    # ë™ì‹œ ì‹¤í–‰
                    start_time = time.time()
                    memory_before = psutil.Process().memory_info().rss
                    
                    results = await asyncio.gather(*update_tasks, return_exceptions=True)
                    
                    end_time = time.time()
                    memory_after = psutil.Process().memory_info().rss
                    
                    # ì„±ëŠ¥ ë¶„ì„
                    execution_time = end_time - start_time
                    memory_used = memory_after - memory_before
                    
                    success_count = sum(1 for r in results if not isinstance(r, Exception))
                    error_count = len(results) - success_count
                    
                    print(f"ğŸ” ë™ì‹œ ì—…ë°ì´íŠ¸ ì„±ëŠ¥:")
                    print(f"   ì´ ì‘ì—… ìˆ˜: {len(update_tasks)}")
                    print(f"   ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {error_count}")
                    print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
                    print(f"   í‰ê·  ì‘ì—… ì‹œê°„: {execution_time / len(update_tasks):.3f}ì´ˆ")
                    print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used / 1024 / 1024:.2f}MB")
                    
                    # ì„±ëŠ¥ ì„ê³„ê°’ ê²€ì¦
                    assert execution_time < 15.0, f"ë™ì‹œ ì—…ë°ì´íŠ¸ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {execution_time:.3f}ì´ˆ"
                    assert success_count >= len(update_tasks) * 0.8, f"ì„±ê³µë¥ ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤: {success_count}/{len(update_tasks)}"
    
    @pytest.mark.asyncio
    async def test_load_stress_test(self, terminus_service):
        """ë¶€í•˜ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        db_name = "stress_test_db"
        
        # ë†’ì€ ë¶€í•˜ ìƒí™© ì‹œë®¬ë ˆì´ì…˜
        stress_tasks = []
        for batch in range(5):  # 5ê°œ ë°°ì¹˜
            for i in range(10):  # ê° ë°°ì¹˜ì— 10ê°œ ì‘ì—…
                update_data = {
                    "label": {"ko": f"ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ {batch}-{i}", "en": f"Stress Test {batch}-{i}"},
                    "properties": [
                        {"name": f"stress_prop_{batch}_{i}_{j}", "type": "xsd:string"}
                        for j in range(20)
                    ]
                }
                
                sample_ontology = {
                    "id": f"StressClass_{batch}_{i}",
                    "label": {"ko": f"ìŠ¤íŠ¸ë ˆìŠ¤ í´ë˜ìŠ¤", "en": f"Stress Class"},
                    "properties": []
                }
                
                with patch.object(terminus_service, 'ensure_db_exists', return_value=None):
                    with patch.object(terminus_service, 'get_ontology', return_value=sample_ontology):
                        with patch.object(terminus_service, '_make_request', return_value={"success": True}):
                            
                            task = terminus_service.update_ontology_atomic_patch(
                                db_name, f"StressClass_{batch}_{i}", update_data
                            )
                            stress_tasks.append(task)
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        start_time = time.time()
        cpu_before = psutil.cpu_percent(interval=None)
        memory_before = psutil.Process().memory_info().rss
        
        results = await asyncio.gather(*stress_tasks, return_exceptions=True)
        
        end_time = time.time()
        cpu_after = psutil.cpu_percent(interval=None)
        memory_after = psutil.Process().memory_info().rss
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ë¶„ì„
        execution_time = end_time - start_time
        memory_used = memory_after - memory_before
        cpu_usage = cpu_after - cpu_before
        
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        error_count = len(results) - success_count
        
        print(f"ğŸ” ë¶€í•˜ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸:")
        print(f"   ì´ ì‘ì—… ìˆ˜: {len(stress_tasks)}")
        print(f"   ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {error_count}")
        print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
        print(f"   ì²˜ë¦¬ëŸ‰: {len(stress_tasks) / execution_time:.2f} ì‘ì—…/ì´ˆ")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used / 1024 / 1024:.2f}MB")
        print(f"   CPU ì‚¬ìš©ë¥  ë³€í™”: {cpu_usage:.1f}%")
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì„ê³„ê°’ ê²€ì¦
        assert execution_time < 30.0, f"ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {execution_time:.3f}ì´ˆ"
        assert success_count >= len(stress_tasks) * 0.7, f"ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤: {success_count}/{len(stress_tasks)}"
        assert memory_used < 500 * 1024 * 1024, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤: {memory_used / 1024 / 1024:.2f}MB"

class TestMemoryManagement:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_memory_leak_detection(self, terminus_service):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸"""
        db_name = "memory_test_db"
        
        # ê¸°ì¤€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        initial_memory = psutil.Process().memory_info().rss
        
        # ë°˜ë³µ ì—…ë°ì´íŠ¸ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸
        for iteration in range(100):  # 100ë²ˆ ë°˜ë³µ
            update_data = {
                "label": {"ko": f"ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ {iteration}", "en": f"Memory Test {iteration}"},
                "properties": [
                    {"name": f"mem_prop_{iteration}_{j}", "type": "xsd:string"}
                    for j in range(10)
                ]
            }
            
            sample_ontology = {
                "id": f"MemoryClass_{iteration}",
                "label": {"ko": "ë©”ëª¨ë¦¬ í´ë˜ìŠ¤", "en": "Memory Class"},
                "properties": []
            }
            
            with patch.object(terminus_service, 'ensure_db_exists', return_value=None):
                with patch.object(terminus_service, 'get_ontology', return_value=sample_ontology):
                    with patch.object(terminus_service, '_make_request', return_value={"success": True}):
                        
                        await terminus_service.update_ontology_atomic_patch(
                            db_name, f"MemoryClass_{iteration}", update_data
                        )
            
            # 10ë²ˆë§ˆë‹¤ ë©”ëª¨ë¦¬ ì²´í¬
            if iteration % 10 == 0:
                gc.collect()
                current_memory = psutil.Process().memory_info().rss
                memory_growth = current_memory - initial_memory
                
                print(f"ğŸ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ë°˜ë³µ {iteration}):")
                print(f"   í˜„ì¬ ë©”ëª¨ë¦¬: {current_memory / 1024 / 1024:.2f}MB")
                print(f"   ì¦ê°€ëŸ‰: {memory_growth / 1024 / 1024:.2f}MB")
                
                # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ ì„ê³„ê°’ ê²€ì¦ (100MB ì´í•˜)
                assert memory_growth < 100 * 1024 * 1024, f"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬: {memory_growth / 1024 / 1024:.2f}MB ì¦ê°€"
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì²´í¬
        gc.collect()
        final_memory = psutil.Process().memory_info().rss
        total_growth = final_memory - initial_memory
        
        print(f"ğŸ” ìµœì¢… ë©”ëª¨ë¦¬ ë¶„ì„:")
        print(f"   ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory / 1024 / 1024:.2f}MB")
        print(f"   ìµœì¢… ë©”ëª¨ë¦¬: {final_memory / 1024 / 1024:.2f}MB")
        print(f"   ì´ ì¦ê°€ëŸ‰: {total_growth / 1024 / 1024:.2f}MB")
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê²€ì¦ (50MB ì´í•˜ ì¦ê°€ëŠ” ì •ìƒ)
        assert total_growth < 50 * 1024 * 1024, f"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€: {total_growth / 1024 / 1024:.2f}MB ì¦ê°€"
    
    @pytest.mark.asyncio
    async def test_backup_restore_memory_usage(self, terminus_service):
        """ë°±ì—…/ë³µì› ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        db_name = "backup_memory_test_db"
        
        # ëŒ€ìš©ëŸ‰ ì˜¨í†¨ë¡œì§€ ë°ì´í„°
        large_ontology = {
            "id": "LargeBackupClass",
            "label": {"ko": "ëŒ€ìš©ëŸ‰ ë°±ì—… í´ë˜ìŠ¤", "en": "Large Backup Class"},
            "properties": [
                {"name": f"backup_prop_{i}", "type": "xsd:string"}
                for i in range(500)  # 500ê°œì˜ ì†ì„±
            ]
        }
        
        with patch.object(terminus_service, 'get_ontology', return_value=large_ontology):
            # ë°±ì—… ìƒì„± ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            memory_before_backup = psutil.Process().memory_info().rss
            
            backup_data = await terminus_service._create_backup_before_update(db_name, "LargeBackupClass")
            
            memory_after_backup = psutil.Process().memory_info().rss
            backup_memory_used = memory_after_backup - memory_before_backup
            
            print(f"ğŸ” ë°±ì—… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
            print(f"   ë°±ì—… ë©”ëª¨ë¦¬: {backup_memory_used / 1024 / 1024:.2f}MB")
            
            # ë°±ì—… ë³µì› ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            with patch.object(terminus_service, '_make_request', return_value={"success": True}):
                memory_before_restore = psutil.Process().memory_info().rss
                
                restore_success = await terminus_service._restore_from_backup(backup_data)
                
                memory_after_restore = psutil.Process().memory_info().rss
                restore_memory_used = memory_after_restore - memory_before_restore
                
                print(f"ğŸ” ë³µì› ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
                print(f"   ë³µì› ë©”ëª¨ë¦¬: {restore_memory_used / 1024 / 1024:.2f}MB")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„ê³„ê°’ ê²€ì¦
                assert backup_memory_used < 20 * 1024 * 1024, f"ë°±ì—… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤: {backup_memory_used / 1024 / 1024:.2f}MB"
                assert restore_memory_used < 20 * 1024 * 1024, f"ë³µì› ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤: {restore_memory_used / 1024 / 1024:.2f}MB"
                
                # ë³µì› ì„±ê³µ ê²€ì¦
                assert restore_success is True

class TestScalabilityLimits:
    """í™•ì¥ì„± í•œê³„ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_maximum_property_count(self, terminus_service):
        """ìµœëŒ€ ì†ì„± ê°œìˆ˜ í…ŒìŠ¤íŠ¸"""
        db_name = "scalability_test_db"
        
        # ë§¤ìš° ë§ì€ ì†ì„±ì„ ê°€ì§„ ì˜¨í†¨ë¡œì§€
        max_properties = 5000  # 5000ê°œì˜ ì†ì„±
        
        large_ontology = {
            "id": "MaxPropertiesClass",
            "label": {"ko": "ìµœëŒ€ ì†ì„± í´ë˜ìŠ¤", "en": "Max Properties Class"},
            "properties": [
                {"name": f"max_prop_{i}", "type": "xsd:string"}
                for i in range(max_properties)
            ]
        }
        
        update_data = {
            "properties": [
                {"name": f"updated_prop_{i}", "type": "xsd:string"}
                for i in range(max_properties + 100)  # 100ê°œ ì¶”ê°€
            ]
        }
        
        with patch.object(terminus_service, 'ensure_db_exists', return_value=None):
            with patch.object(terminus_service, 'get_ontology', return_value=large_ontology):
                with patch.object(terminus_service, '_make_request', return_value={"success": True}):
                    
                    # ìµœëŒ€ ì†ì„± ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •
                    start_time = time.time()
                    memory_before = psutil.Process().memory_info().rss
                    
                    result = await terminus_service.update_ontology_atomic_patch(
                        db_name, "MaxPropertiesClass", update_data
                    )
                    
                    end_time = time.time()
                    memory_after = psutil.Process().memory_info().rss
                    
                    execution_time = end_time - start_time
                    memory_used = memory_after - memory_before
                    
                    print(f"ğŸ” ìµœëŒ€ ì†ì„± ì²˜ë¦¬ ì„±ëŠ¥:")
                    print(f"   ì†ì„± ê°œìˆ˜: {max_properties + 100}")
                    print(f"   ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
                    print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used / 1024 / 1024:.2f}MB")
                    
                    # í™•ì¥ì„± ì„ê³„ê°’ ê²€ì¦
                    assert execution_time < 30.0, f"ëŒ€ìš©ëŸ‰ ì†ì„± ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {execution_time:.3f}ì´ˆ"
                    assert memory_used < 200 * 1024 * 1024, f"ëŒ€ìš©ëŸ‰ ì†ì„± ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤: {memory_used / 1024 / 1024:.2f}MB"
                    
                    # ê²°ê³¼ ê²€ì¦
                    assert result["id"] == "MaxPropertiesClass"
                    assert result["method"] == "atomic_patch"
    
    @pytest.mark.asyncio
    async def test_deep_relationship_chains(self, terminus_service):
        """ê¹Šì€ ê´€ê³„ ì²´ì¸ í…ŒìŠ¤íŠ¸"""
        db_name = "deep_relationship_test_db"
        
        # ê¹Šì€ ê´€ê³„ ì²´ì¸ì„ ê°€ì§„ ì˜¨í†¨ë¡œì§€ë“¤
        chain_depth = 100
        
        ontology_chain = []
        for i in range(chain_depth):
            ontology = {
                "id": f"ChainClass_{i}",
                "label": {"ko": f"ì²´ì¸ í´ë˜ìŠ¤ {i}", "en": f"Chain Class {i}"},
                "relationships": [
                    {"predicate": "next", "target": f"ChainClass_{i+1}"}
                ] if i < chain_depth - 1 else []
            }
            ontology_chain.append(ontology)
        
        # ì²´ì¸ ì¤‘ê°„ ë…¸ë“œ ì—…ë°ì´íŠ¸
        middle_index = chain_depth // 2
        update_data = {
            "label": {"ko": f"ì—…ë°ì´íŠ¸ëœ ì²´ì¸ í´ë˜ìŠ¤ {middle_index}", "en": f"Updated Chain Class {middle_index}"},
            "properties": [
                {"name": f"chain_prop_{i}", "type": "xsd:string"}
                for i in range(10)
            ]
        }
        
        with patch.object(terminus_service, 'ensure_db_exists', return_value=None):
            with patch.object(terminus_service, 'get_ontology', return_value=ontology_chain[middle_index]):
                with patch.object(terminus_service, '_make_request', return_value={"success": True}):
                    
                    # ê¹Šì€ ê´€ê³„ ì²´ì¸ ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •
                    start_time = time.time()
                    
                    result = await terminus_service.update_ontology_atomic_patch(
                        db_name, f"ChainClass_{middle_index}", update_data
                    )
                    
                    end_time = time.time()
                    execution_time = end_time - start_time
                    
                    print(f"ğŸ” ê¹Šì€ ê´€ê³„ ì²´ì¸ ì²˜ë¦¬ ì„±ëŠ¥:")
                    print(f"   ì²´ì¸ ê¹Šì´: {chain_depth}")
                    print(f"   ì—…ë°ì´íŠ¸ ë…¸ë“œ: {middle_index}")
                    print(f"   ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
                    
                    # ê´€ê³„ ì²´ì¸ ì²˜ë¦¬ ì„ê³„ê°’ ê²€ì¦
                    assert execution_time < 10.0, f"ê¹Šì€ ê´€ê³„ ì²´ì¸ ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {execution_time:.3f}ì´ˆ"
                    
                    # ê²°ê³¼ ê²€ì¦
                    assert result["id"] == f"ChainClass_{middle_index}"
                    assert result["method"] == "atomic_patch"

if __name__ == "__main__":
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    pytest.main([__file__, "-v", "-s"])  # -s ì˜µì…˜ìœ¼ë¡œ print ì¶œë ¥ í™œì„±í™”