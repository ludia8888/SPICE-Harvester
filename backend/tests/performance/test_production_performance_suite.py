"""
í”„ë¡œë•ì…˜ ë ˆë²¨ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
ëŒ€ìš©ëŸ‰ í˜ì´ë¡œë“œ, ë™ì‹œ ìš”ì²­, ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë“± ëª¨ë“  ì„±ëŠ¥ ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦

think ultra: ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì„±ëŠ¥ ë³‘ëª© ì§€ì  í…ŒìŠ¤íŠ¸
NO SIMULATION - ì‹¤ì œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì™€ ì‹¤ì œ ë™ì‹œ ìš”ì²­ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
"""

import asyncio
import httpx
import json
import time
import psutil
import threading
import statistics
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import random
import string
import gc
import sys
import tracemalloc
from test_config import TestConfig

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì„¤ì •
OMS_BASE_URL = TestConfig.get_oms_base_url()
BFF_BASE_URL = TestConfig.get_bff_base_url()
PERFORMANCE_TEST_TIMEOUT = 60
MAX_PAYLOAD_SIZE = 10 * 1024 * 1024  # 10MB
MAX_CONCURRENT_USERS = 100

logger = logging.getLogger(__name__)


class ProductionPerformanceTestSuite:
    """í”„ë¡œë•ì…˜ ë ˆë²¨ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        self.test_db_name = f"perf_test_{int(time.time())}"
        self.performance_results = {}
        self.resource_metrics = {}
        self.error_counts = {}
        self.response_times = []
        
        # ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì • (í”„ë¡œë•ì…˜ ê¸°ì¤€)
        self.thresholds = {
            "max_response_time": 5.0,  # 5ì´ˆ
            "min_throughput": 10,      # ì´ˆë‹¹ 10 ìš”ì²­
            "max_error_rate": 0.05,    # 5% ì´í•˜
            "max_memory_mb": 512,      # 512MB ì´í•˜
            "max_cpu_percent": 80      # 80% ì´í•˜
        }
    
    async def setup_performance_test_environment(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        print("ğŸš€ í”„ë¡œë•ì…˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •")
        
        # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
        tracemalloc.start()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        async with httpx.AsyncClient(timeout=PERFORMANCE_TEST_TIMEOUT) as client:
            response = await client.post(
                f"{OMS_BASE_URL}/api/v1/database/create",
                json={"name": self.test_db_name, "description": "Performance test database"}
            )
            if response.status_code not in [200, 409]:
                raise Exception(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ DB ìƒì„± ì‹¤íŒ¨: {response.status_code}")
        
        print(f"âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ: {self.test_db_name}")
    
    async def cleanup_performance_test_environment(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬"""
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                await client.delete(f"{OMS_BASE_URL}/api/v1/database/{self.test_db_name}")
            print("âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def monitor_system_resources(self, duration: int = 60):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
        start_time = time.time()
        cpu_samples = []
        memory_samples = []
        
        def collect_metrics():
            while time.time() - start_time < duration:
                cpu_samples.append(psutil.cpu_percent(interval=1))
                memory_samples.append(psutil.virtual_memory().used / 1024 / 1024)  # MB
                time.sleep(1)
        
        monitor_thread = threading.Thread(target=collect_metrics)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        return monitor_thread, cpu_samples, memory_samples

    # =============================================================================
    # 1. ëŒ€ìš©ëŸ‰ í˜ì´ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    # =============================================================================
    
    async def test_large_payload_performance(self):
        """ëŒ€ìš©ëŸ‰ í˜ì´ë¡œë“œ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“¦ ëŒ€ìš©ëŸ‰ í˜ì´ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        
        # ëŒ€ìš©ëŸ‰ ì˜¨í†¨ë¡œì§€ ë°ì´í„° ìƒì„± (ì ì§„ì ìœ¼ë¡œ í¬ê¸° ì¦ê°€)
        payload_sizes = [1, 10, 100, 500, 1000]  # ì†ì„± ê°œìˆ˜
        
        async with httpx.AsyncClient(timeout=PERFORMANCE_TEST_TIMEOUT) as client:
            for size in payload_sizes:
                # ëŒ€ìš©ëŸ‰ ì˜¨í†¨ë¡œì§€ ìƒì„±
                large_ontology = {
                    "id": f"LargeOntology{size}",
                    "label": f"Large Ontology with {size} properties",
                    "description": "Performance test ontology with many properties",
                    "properties": {}
                }
                
                # ë§ì€ ì†ì„± ì¶”ê°€
                for i in range(size):
                    large_ontology["properties"][f"property_{i}"] = "string"
                
                # í˜ì´ë¡œë“œ í¬ê¸° ê³„ì‚°
                payload_json = json.dumps(large_ontology)
                payload_size_mb = len(payload_json.encode('utf-8')) / 1024 / 1024
                
                if payload_size_mb > 10:  # 10MB ì´ˆê³¼ ì‹œ ì¤‘ë‹¨
                    print(f"âš ï¸ í˜ì´ë¡œë“œ í¬ê¸° í•œê³„ ë„ë‹¬: {payload_size_mb:.2f}MB")
                    break
                
                # ì„±ëŠ¥ ì¸¡ì •
                start_time = time.time()
                response = await client.post(
                    f"{OMS_BASE_URL}/api/v1/ontology/{self.test_db_name}/create",
                    json=large_ontology
                )
                end_time = time.time()
                
                response_time = end_time - start_time
                
                self.performance_results[f"large_payload_{size}"] = {
                    "payload_size_mb": payload_size_mb,
                    "properties_count": size,
                    "response_time": response_time,
                    "status_code": response.status_code,
                    "success": response.status_code == 200
                }
                
                print(f"  ğŸ“Š {size} ì†ì„±: {payload_size_mb:.2f}MB, {response_time:.2f}s, {response.status_code}")
                
                # ì„ê³„ê°’ í™•ì¸
                if response_time > self.thresholds["max_response_time"]:
                    print(f"âš ï¸ ì‘ë‹µ ì‹œê°„ ì„ê³„ê°’ ì´ˆê³¼: {response_time:.2f}s > {self.thresholds['max_response_time']}s")
        
        print("âœ… ëŒ€ìš©ëŸ‰ í˜ì´ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    # =============================================================================
    # 2. ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸
    # =============================================================================
    
    async def test_concurrent_user_load(self):
        """ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ‘¥ ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸")
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor_thread, cpu_samples, memory_samples = self.monitor_system_resources(60)
        
        concurrent_levels = [10, 25, 50, 75, 100]
        
        for concurrent_users in concurrent_levels:
            print(f"  ğŸ”„ ë™ì‹œ ì‚¬ìš©ì {concurrent_users}ëª… í…ŒìŠ¤íŠ¸")
            
            # ë™ì‹œ ìš”ì²­ ìƒì„±
            tasks = []
            start_time = time.time()
            
            async with httpx.AsyncClient(timeout=PERFORMANCE_TEST_TIMEOUT) as client:
                for user_id in range(concurrent_users):
                    ontology_data = {
                        "id": f"ConcurrentUser{concurrent_users}_{user_id}",
                        "label": f"Concurrent Test User {user_id}",
                        "properties": {
                            "user_id": "string",
                            "timestamp": "string",
                            "test_data": "string"
                        }
                    }
                    
                    task = client.post(
                        f"{OMS_BASE_URL}/api/v1/ontology/{self.test_db_name}/create",
                        json=ontology_data
                    )
                    tasks.append(task)
                
                # ëª¨ë“  ìš”ì²­ ë™ì‹œ ì‹¤í–‰
                responses = await asyncio.gather(*tasks, return_exceptions=True)
                end_time = time.time()
                
                # ê²°ê³¼ ë¶„ì„
                total_time = end_time - start_time
                successful_responses = sum(1 for r in responses if hasattr(r, 'status_code') and r.status_code == 200)
                duplicate_responses = sum(1 for r in responses if hasattr(r, 'status_code') and r.status_code == 409)
                error_responses = sum(1 for r in responses if not hasattr(r, 'status_code') or r.status_code >= 500)
                
                throughput = len(responses) / total_time
                error_rate = error_responses / len(responses)
                
                self.performance_results[f"concurrent_{concurrent_users}"] = {
                    "concurrent_users": concurrent_users,
                    "total_requests": len(responses),
                    "successful": successful_responses,
                    "duplicates": duplicate_responses,
                    "errors": error_responses,
                    "total_time": total_time,
                    "throughput": throughput,
                    "error_rate": error_rate
                }
                
                print(f"    ğŸ“ˆ ì²˜ë¦¬ëŸ‰: {throughput:.2f} req/s, ì—ëŸ¬ìœ¨: {error_rate:.2%}")
                
                # ì„ê³„ê°’ í™•ì¸
                if throughput < self.thresholds["min_throughput"]:
                    print(f"âš ï¸ ì²˜ë¦¬ëŸ‰ ì„ê³„ê°’ ë¯¸ë‹¬: {throughput:.2f} < {self.thresholds['min_throughput']}")
                
                if error_rate > self.thresholds["max_error_rate"]:
                    print(f"âš ï¸ ì—ëŸ¬ìœ¨ ì„ê³„ê°’ ì´ˆê³¼: {error_rate:.2%} > {self.thresholds['max_error_rate']:.2%}")
                
                # ì‹œìŠ¤í…œ ë¶€í•˜ ì¡°ì ˆì„ ìœ„í•œ ëŒ€ê¸°
                await asyncio.sleep(2)
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„
        monitor_thread.join(timeout=5)
        if cpu_samples and memory_samples:
            avg_cpu = statistics.mean(cpu_samples)
            max_cpu = max(cpu_samples)
            avg_memory = statistics.mean(memory_samples)
            max_memory = max(memory_samples)
            
            self.resource_metrics["load_test"] = {
                "avg_cpu_percent": avg_cpu,
                "max_cpu_percent": max_cpu,
                "avg_memory_mb": avg_memory,
                "max_memory_mb": max_memory
            }
            
            print(f"  ğŸ’» CPU ì‚¬ìš©ë¥ : í‰ê·  {avg_cpu:.1f}%, ìµœëŒ€ {max_cpu:.1f}%")
            print(f"  ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: í‰ê·  {avg_memory:.1f}MB, ìµœëŒ€ {max_memory:.1f}MB")
        
        print("âœ… ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    # =============================================================================
    # 3. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (í•œê³„ì  ì°¾ê¸°)
    # =============================================================================
    
    async def test_stress_breaking_point(self):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ - ì‹œìŠ¤í…œ í•œê³„ì  ì°¾ê¸°"""
        print("\nğŸ’¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (í•œê³„ì  ì°¾ê¸°)")
        
        max_concurrent = 200
        increment = 20
        
        for concurrent_level in range(20, max_concurrent + 1, increment):
            print(f"  ğŸ”¥ ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ {concurrent_level}")
            
            tasks = []
            error_count = 0
            timeout_count = 0
            
            start_time = time.time()
            
            try:
                async with httpx.AsyncClient(timeout=30) as client:
                    for i in range(concurrent_level):
                        ontology_data = {
                            "id": f"StressTest{concurrent_level}_{i}",
                            "label": f"Stress Test {i}",
                            "properties": {"stress_prop": "string"}
                        }
                        
                        task = client.post(
                            f"{OMS_BASE_URL}/api/v1/ontology/{self.test_db_name}/create",
                            json=ontology_data
                        )
                        tasks.append(task)
                    
                    # ëª¨ë“  ìš”ì²­ ì‹¤í–‰
                    responses = await asyncio.gather(*tasks, return_exceptions=True)
                    end_time = time.time()
                    
                    # ê²°ê³¼ ë¶„ì„
                    for response in responses:
                        if isinstance(response, Exception):
                            if "timeout" in str(response).lower():
                                timeout_count += 1
                            else:
                                error_count += 1
                        elif hasattr(response, 'status_code') and response.status_code >= 500:
                            error_count += 1
                    
                    total_time = end_time - start_time
                    success_count = len(responses) - error_count - timeout_count
                    throughput = len(responses) / total_time
                    error_rate = (error_count + timeout_count) / len(responses)
                    
                    self.performance_results[f"stress_{concurrent_level}"] = {
                        "stress_level": concurrent_level,
                        "success_count": success_count,
                        "error_count": error_count,
                        "timeout_count": timeout_count,
                        "throughput": throughput,
                        "error_rate": error_rate,
                        "total_time": total_time
                    }
                    
                    print(f"    ğŸ“Š ì„±ê³µ: {success_count}, ì—ëŸ¬: {error_count}, íƒ€ì„ì•„ì›ƒ: {timeout_count}")
                    print(f"    âš¡ ì²˜ë¦¬ëŸ‰: {throughput:.2f} req/s, ì—ëŸ¬ìœ¨: {error_rate:.2%}")
                    
                    # ì‹œìŠ¤í…œ í•œê³„ì  íŒë‹¨ (ì—ëŸ¬ìœ¨ 50% ì´ìƒ)
                    if error_rate > 0.5:
                        print(f"ğŸš¨ ì‹œìŠ¤í…œ í•œê³„ì  ë„ë‹¬: ë™ì‹œ ìš”ì²­ {concurrent_level}, ì—ëŸ¬ìœ¨ {error_rate:.2%}")
                        break
                    
                    # íšŒë³µ ì‹œê°„
                    await asyncio.sleep(3)
                    
            except Exception as e:
                print(f"ğŸ’¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                break
        
        print("âœ… ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    # =============================================================================
    # 4. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸
    # =============================================================================
    
    async def test_memory_leak_detection(self):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§  ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸")
        
        initial_memory = psutil.virtual_memory().used / 1024 / 1024
        memory_snapshots = [initial_memory]
        
        # 1000ê°œì˜ ìš”ì²­ì„ 10íšŒ ë°˜ë³µ (ì´ 10,000 ìš”ì²­)
        iterations = 10
        requests_per_iteration = 100
        
        for iteration in range(iterations):
            print(f"  ğŸ”„ ë°˜ë³µ {iteration + 1}/{iterations}")
            
            tasks = []
            async with httpx.AsyncClient(timeout=30) as client:
                for i in range(requests_per_iteration):
                    ontology_data = {
                        "id": f"MemoryTest{iteration}_{i}",
                        "label": f"Memory Test {iteration}_{i}",
                        "properties": {"test_prop": "string"}
                    }
                    
                    task = client.post(
                        f"{OMS_BASE_URL}/api/v1/ontology/{self.test_db_name}/create",
                        json=ontology_data
                    )
                    tasks.append(task)
                
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            current_memory = psutil.virtual_memory().used / 1024 / 1024
            memory_snapshots.append(current_memory)
            
            print(f"    ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {current_memory:.1f}MB (+{current_memory - initial_memory:.1f}MB)")
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            gc.collect()
            
            # ì ì‹œ ëŒ€ê¸°
            await asyncio.sleep(2)
        
        # ë©”ëª¨ë¦¬ ì¦ê°€ ì¶”ì„¸ ë¶„ì„
        memory_increase = memory_snapshots[-1] - memory_snapshots[0]
        memory_growth_rate = memory_increase / iterations
        
        self.resource_metrics["memory_leak_test"] = {
            "initial_memory_mb": initial_memory,
            "final_memory_mb": memory_snapshots[-1],
            "total_increase_mb": memory_increase,
            "growth_rate_mb_per_iteration": memory_growth_rate,
            "memory_snapshots": memory_snapshots
        }
        
        print(f"  ğŸ“ˆ ì´ ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.1f}MB")
        print(f"  ğŸ“Š ë°˜ë³µë‹¹ ì¦ê°€ìœ¨: {memory_growth_rate:.2f}MB/iteration")
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì„ê³„ê°’ í™•ì¸
        if memory_growth_rate > 5:  # ë°˜ë³µë‹¹ 5MB ì´ìƒ ì¦ê°€ ì‹œ ê²½ê³ 
            print(f"âš ï¸ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬: ë°˜ë³µë‹¹ {memory_growth_rate:.2f}MB ì¦ê°€")
        else:
            print("âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•ˆì •ì ")
        
        print("âœ… ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    # =============================================================================
    # 5. ì‘ë‹µ ì‹œê°„ ë¶„í¬ ë¶„ì„
    # =============================================================================
    
    async def test_response_time_distribution(self):
        """ì‘ë‹µ ì‹œê°„ ë¶„í¬ ë¶„ì„"""
        print("\nâ±ï¸ ì‘ë‹µ ì‹œê°„ ë¶„í¬ ë¶„ì„")
        
        response_times = []
        sample_size = 500
        
        async with httpx.AsyncClient(timeout=30) as client:
            for i in range(sample_size):
                ontology_data = {
                    "id": f"ResponseTimeTest{i}",
                    "label": f"Response Time Test {i}",
                    "properties": {"test_prop": "string"}
                }
                
                start_time = time.time()
                try:
                    response = await client.post(
                        f"{OMS_BASE_URL}/api/v1/ontology/{self.test_db_name}/create",
                        json=ontology_data
                    )
                    end_time = time.time()
                    
                    if response.status_code in [200, 409]:  # ì„±ê³µ ë˜ëŠ” ì¤‘ë³µ
                        response_times.append(end_time - start_time)
                
                except Exception as e:
                    end_time = time.time()
                    response_times.append(end_time - start_time)  # íƒ€ì„ì•„ì›ƒë„ í¬í•¨
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ (100ê°œë§ˆë‹¤)
                if (i + 1) % 100 == 0:
                    print(f"    ì§„í–‰: {i + 1}/{sample_size}")
        
        # í†µê³„ ë¶„ì„
        if response_times:
            min_time = min(response_times)
            max_time = max(response_times)
            avg_time = statistics.mean(response_times)
            median_time = statistics.median(response_times)
            p95_time = sorted(response_times)[int(len(response_times) * 0.95)]
            p99_time = sorted(response_times)[int(len(response_times) * 0.99)]
            
            self.performance_results["response_time_analysis"] = {
                "sample_size": len(response_times),
                "min_time": min_time,
                "max_time": max_time,
                "avg_time": avg_time,
                "median_time": median_time,
                "p95_time": p95_time,
                "p99_time": p99_time,
                "times_over_threshold": sum(1 for t in response_times if t > self.thresholds["max_response_time"])
            }
            
            print(f"  ğŸ“Š ì‘ë‹µ ì‹œê°„ í†µê³„:")
            print(f"    ìµœì†Œ: {min_time:.3f}s")
            print(f"    ìµœëŒ€: {max_time:.3f}s")
            print(f"    í‰ê· : {avg_time:.3f}s")
            print(f"    ì¤‘ê°„ê°’: {median_time:.3f}s")
            print(f"    95%ile: {p95_time:.3f}s")
            print(f"    99%ile: {p99_time:.3f}s")
            
            slow_requests = sum(1 for t in response_times if t > self.thresholds["max_response_time"])
            slow_percentage = slow_requests / len(response_times) * 100
            print(f"    ì„ê³„ê°’({self.thresholds['max_response_time']}s) ì´ˆê³¼: {slow_requests}ê±´ ({slow_percentage:.1f}%)")
        
        print("âœ… ì‘ë‹µ ì‹œê°„ ë¶„í¬ ë¶„ì„ ì™„ë£Œ")

    # =============================================================================
    # 6. ì¥ì‹œê°„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
    # =============================================================================
    
    async def test_long_running_stability(self):
        """ì¥ì‹œê°„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ•’ ì¥ì‹œê°„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (5ë¶„ê°„)")
        
        duration_seconds = 300  # 5ë¶„
        request_interval = 1    # 1ì´ˆë§ˆë‹¤ ìš”ì²­
        
        start_time = time.time()
        request_count = 0
        error_count = 0
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor_thread, cpu_samples, memory_samples = self.monitor_system_resources(duration_seconds)
        
        async with httpx.AsyncClient(timeout=30) as client:
            while time.time() - start_time < duration_seconds:
                try:
                    ontology_data = {
                        "id": f"LongRunTest{request_count}",
                        "label": f"Long Running Test {request_count}",
                        "properties": {"test_prop": "string"}
                    }
                    
                    response = await client.post(
                        f"{OMS_BASE_URL}/api/v1/ontology/{self.test_db_name}/create",
                        json=ontology_data
                    )
                    
                    if response.status_code not in [200, 409]:
                        error_count += 1
                    
                    request_count += 1
                    
                    # ì§„í–‰ ìƒí™© í‘œì‹œ (30ì´ˆë§ˆë‹¤)
                    elapsed = time.time() - start_time
                    if request_count % 30 == 0:
                        print(f"    ì§„í–‰: {elapsed:.0f}s/{duration_seconds}s, ìš”ì²­: {request_count}, ì—ëŸ¬: {error_count}")
                    
                    await asyncio.sleep(request_interval)
                    
                except Exception as e:
                    error_count += 1
                    await asyncio.sleep(request_interval)
        
        total_time = time.time() - start_time
        avg_throughput = request_count / total_time
        error_rate = error_count / request_count if request_count > 0 else 1
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„
        monitor_thread.join(timeout=5)
        resource_stability = {
            "cpu_stability": statistics.stdev(cpu_samples) if len(cpu_samples) > 1 else 0,
            "memory_stability": statistics.stdev(memory_samples) if len(memory_samples) > 1 else 0
        }
        
        self.performance_results["long_running_stability"] = {
            "duration_seconds": total_time,
            "total_requests": request_count,
            "error_count": error_count,
            "error_rate": error_rate,
            "avg_throughput": avg_throughput,
            "resource_stability": resource_stability
        }
        
        print(f"  ğŸ“Š ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"    ì´ ìš”ì²­: {request_count}")
        print(f"    ì—ëŸ¬: {error_count} ({error_rate:.2%})")
        print(f"    í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_throughput:.2f} req/s")
        print(f"    CPU ì•ˆì •ì„±: {resource_stability['cpu_stability']:.2f}% í‘œì¤€í¸ì°¨")
        print(f"    ë©”ëª¨ë¦¬ ì•ˆì •ì„±: {resource_stability['memory_stability']:.2f}MB í‘œì¤€í¸ì°¨")
        
        print("âœ… ì¥ì‹œê°„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    # =============================================================================
    # ë©”ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°
    # =============================================================================
    
    async def run_comprehensive_performance_tests(self):
        """ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            await self.setup_performance_test_environment()
            
            # ìˆœì°¨ì ìœ¼ë¡œ ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            performance_test_methods = [
                self.test_large_payload_performance,
                self.test_concurrent_user_load,
                self.test_stress_breaking_point,
                self.test_memory_leak_detection,
                self.test_response_time_distribution,
                self.test_long_running_stability
            ]
            
            for test_method in performance_test_methods:
                try:
                    await test_method()
                    # í…ŒìŠ¤íŠ¸ ê°„ íšŒë³µ ì‹œê°„
                    await asyncio.sleep(5)
                except Exception as e:
                    print(f"âŒ {test_method.__name__} ì‹¤íŒ¨: {e}")
                    self.performance_results[test_method.__name__] = {"error": str(e)}
            
            # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_performance_report()
            
        finally:
            await self.cleanup_performance_test_environment()
    
    def generate_performance_report(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "="*80)
        print("ğŸ í”„ë¡œë•ì…˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸")
        print("="*80)
        
        # ì„±ëŠ¥ ì„ê³„ê°’ ìœ„ë°˜ ì‚¬í•­ í™•ì¸
        violations = []
        
        for test_name, results in self.performance_results.items():
            if isinstance(results, dict) and "error" not in results:
                # ì‘ë‹µ ì‹œê°„ í™•ì¸
                if "response_time" in results and results["response_time"] > self.thresholds["max_response_time"]:
                    violations.append(f"{test_name}: ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ ({results['response_time']:.2f}s)")
                
                # ì²˜ë¦¬ëŸ‰ í™•ì¸
                if "throughput" in results and results["throughput"] < self.thresholds["min_throughput"]:
                    violations.append(f"{test_name}: ì²˜ë¦¬ëŸ‰ ë¶€ì¡± ({results['throughput']:.2f} req/s)")
                
                # ì—ëŸ¬ìœ¨ í™•ì¸
                if "error_rate" in results and results["error_rate"] > self.thresholds["max_error_rate"]:
                    violations.append(f"{test_name}: ì—ëŸ¬ìœ¨ ì´ˆê³¼ ({results['error_rate']:.2%})")
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
        for metric_name, metric_data in self.resource_metrics.items():
            if "max_cpu_percent" in metric_data and metric_data["max_cpu_percent"] > self.thresholds["max_cpu_percent"]:
                violations.append(f"{metric_name}: CPU ì‚¬ìš©ë¥  ì´ˆê³¼ ({metric_data['max_cpu_percent']:.1f}%)")
            
            if "max_memory_mb" in metric_data and metric_data["max_memory_mb"] > self.thresholds["max_memory_mb"]:
                violations.append(f"{metric_name}: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼ ({metric_data['max_memory_mb']:.1f}MB)")
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í•­ëª©: {len(self.performance_results)}")
        print(f"ğŸ’» ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§: {len(self.resource_metrics)}")
        
        if violations:
            print(f"\nâš ï¸ ì„±ëŠ¥ ì„ê³„ê°’ ìœ„ë°˜ ì‚¬í•­: {len(violations)}")
            for violation in violations:
                print(f"  - {violation}")
        else:
            print("\nâœ… ëª¨ë“  ì„±ëŠ¥ ì„ê³„ê°’ í†µê³¼")
        
        # ìƒì„¸ ê²°ê³¼
        print("\nğŸ“ˆ ìƒì„¸ ì„±ëŠ¥ ê²°ê³¼:")
        for test_name, results in self.performance_results.items():
            print(f"\n  {test_name}:")
            if isinstance(results, dict):
                for key, value in results.items():
                    if isinstance(value, (int, float)):
                        if key.endswith("_time"):
                            print(f"    {key}: {value:.3f}s")
                        elif key.endswith("_rate"):
                            print(f"    {key}: {value:.2%}")
                        elif key.endswith("_mb"):
                            print(f"    {key}: {value:.1f}MB")
                        else:
                            print(f"    {key}: {value}")
                    else:
                        print(f"    {key}: {value}")
        
        # ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        performance_score = max(0, 100 - len(violations) * 10)
        print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥ ì ìˆ˜: {performance_score}/100")
        
        if performance_score >= 80:
            print("ğŸš€ í”„ë¡œë•ì…˜ ì„±ëŠ¥ ê¸°ì¤€ ë§Œì¡±!")
        elif performance_score >= 60:
            print("âš ï¸ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥")
        else:
            print("ğŸš¨ ì„±ëŠ¥ ê°œì„  í•„ìˆ˜")


# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
async def run_production_performance_tests():
    """í”„ë¡œë•ì…˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    test_suite = ProductionPerformanceTestSuite()
    await test_suite.run_comprehensive_performance_tests()


if __name__ == "__main__":
    # ì‹¤ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(run_production_performance_tests())