"""
ğŸ”¥ THINK ULTRA! Funnel Data Processor Service
Data Connectorì™€ OMS/BFF ì‚¬ì´ì˜ ë°ì´í„° ì²˜ë¦¬ ë ˆì´ì–´
"""

from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

import httpx

from shared.models.google_sheets import GoogleSheetPreviewRequest, GoogleSheetPreviewResponse

from funnel.services.type_inference import FunnelTypeInferenceService
from shared.config.service_config import ServiceConfig
from shared.models.type_inference import (
    ColumnAnalysisResult,
    DatasetAnalysisRequest,
    DatasetAnalysisResponse,
    FunnelPreviewResponse,
)


class FunnelDataProcessor:
    """
    ğŸ”¥ THINK ULTRA! ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

    Architecture Flow:
    1. Data Connector (Google Sheets, CSV, etc.) â†’ Raw Data
    2. Funnel â†’ Type Inference & Data Processing
    3. OMS/BFF â†’ Schema Generation & Storage
    """

    def __init__(self):
        self.type_inference_service = FunnelTypeInferenceService

    async def process_google_sheets_preview(
        self,
        sheet_url: str,
        worksheet_name: Optional[str] = None,
        api_key: Optional[str] = None,
        connection_id: Optional[str] = None,
        infer_types: bool = True,
        include_complex_types: bool = False,
    ) -> FunnelPreviewResponse:
        """
        Google Sheets ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  íƒ€ì…ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.

        Args:
            sheet_url: Google Sheets URL
            worksheet_name: ì›Œí¬ì‹œíŠ¸ ì´ë¦„
            api_key: Google API í‚¤
            infer_types: íƒ€ì… ì¶”ë¡  ì—¬ë¶€
            include_complex_types: ë³µí•© íƒ€ì… ê²€ì‚¬ ì—¬ë¶€

        Returns:
            íƒ€ì…ì´ ì¶”ë¡ ëœ preview ì‘ë‹µ
        """
        # 1. Google Sheets connectorë¥¼ í†µí•´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        preview_request = GoogleSheetPreviewRequest(
            sheet_url=sheet_url,
            worksheet_name=worksheet_name,
            api_key=api_key,
            connection_id=connection_id,
        )

        # Google Sheets service í˜¸ì¶œ (ì‹¤ì œë¡œëŠ” ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ì²˜ë¦¬)
        # ì—¬ê¸°ì„œëŠ” ì§ì ‘ HTTP ìš”ì²­ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{ServiceConfig.get_bff_url()}/api/v1/data-connectors/google-sheets/preview",
                json=preview_request.model_dump(),
            )

            if response.status_code != 200:
                raise Exception(f"Failed to fetch Google Sheets data: {response.text}")

            sheets_preview = GoogleSheetPreviewResponse(**response.json())

        # 2. íƒ€ì… ì¶”ë¡  ìˆ˜í–‰
        inferred_schema = None
        if infer_types and sheets_preview.sample_rows:
            analysis_results = self.type_inference_service.analyze_dataset(
                data=sheets_preview.sample_rows,
                columns=sheets_preview.columns,
                include_complex_types=include_complex_types,
            )
            inferred_schema = analysis_results

        # 3. Funnel response ìƒì„±
        return FunnelPreviewResponse(
            source_metadata={
                "type": "google_sheets",
                "sheet_id": sheets_preview.sheet_id,
                "sheet_title": sheets_preview.sheet_title,
                "worksheet_title": sheets_preview.worksheet_title,
            },
            columns=sheets_preview.columns,
            sample_data=sheets_preview.sample_rows,
            inferred_schema=inferred_schema,
            total_rows=sheets_preview.total_rows,
            preview_rows=len(sheets_preview.sample_rows),
        )

    async def analyze_dataset(self, request: DatasetAnalysisRequest) -> DatasetAnalysisResponse:
        """
        ë°ì´í„°ì…‹ì„ ë¶„ì„í•˜ê³  íƒ€ì…ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.

        Args:
            request: ë°ì´í„°ì…‹ ë¶„ì„ ìš”ì²­

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        # íƒ€ì… ì¶”ë¡  ìˆ˜í–‰
        analysis_results = self.type_inference_service.analyze_dataset(
            data=request.data,
            columns=request.columns,
            sample_size=request.sample_size,
            include_complex_types=request.include_complex_types,
        )

        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            "total_columns": len(request.columns),
            "analyzed_rows": min(len(request.data), request.sample_size or len(request.data)),
            "include_complex_types": request.include_complex_types,
            "analysis_version": "1.0",
        }

        return DatasetAnalysisResponse(
            columns=analysis_results, analysis_metadata=metadata, timestamp=datetime.now(timezone.utc)
        )

    def generate_schema_suggestion(
        self, analysis_results: List[ColumnAnalysisResult], class_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

        Args:
            analysis_results: ì»¬ëŸ¼ ë¶„ì„ ê²°ê³¼
            class_name: í´ë˜ìŠ¤ ì´ë¦„ (ì„ íƒì‚¬í•­)

        Returns:
            OMSì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìŠ¤í‚¤ë§ˆ ì œì•ˆ
        """
        properties = []

        for result in analysis_results:
            # ë†’ì€ confidenceì˜ íƒ€ì…ë§Œ ì‚¬ìš©
            if result.inferred_type.confidence >= 0.7:
                data_type = result.inferred_type.type
            else:
                data_type = "xsd:string"  # ê¸°ë³¸ê°’

            property_def = {
                "name": self._normalize_property_name(result.column_name),
                "type": data_type,
                "label": {"ko": result.column_name},
                "description": {
                    "ko": f"{result.inferred_type.reason} (ì‹ ë¢°ë„: {result.inferred_type.confidence:.0%})"
                },
                # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°(required íŒë‹¨ ë¶ˆê°€)ëŠ” Falseë¡œ ë‘ 
                "required": result.total_count > 0 and result.null_count == 0,
                "metadata": {
                    "inferred_confidence": result.inferred_type.confidence,
                    "total_count": result.total_count,
                    "non_empty_count": result.non_empty_count,
                    "unique_count": result.unique_count,
                    "null_count": result.null_count,
                    "unique_ratio": result.unique_ratio,
                    "null_ratio": result.null_ratio,
                },
            }

            # íƒ€ì…ë³„ ì¶”ê°€ ì œì•½ì¡°ê±´
            if (
                data_type == "xsd:string"
                and result.non_empty_count > 0
                and result.unique_count == result.non_empty_count
            ):
                property_def["constraints"] = {"unique": True}
            elif data_type in ["xsd:integer", "xsd:decimal"]:
                # Prefer engine-provided numeric stats when available
                meta = result.inferred_type.metadata or {}
                min_val = meta.get("min")
                max_val = meta.get("max")

                if min_val is None or max_val is None:
                    # Fallback to sample values (limited) when metadata is missing
                    numeric_values = []
                    for val in result.sample_values:
                        try:
                            numeric_values.append(float(val))
                        except (ValueError, TypeError):
                            continue
                    if numeric_values:
                        min_val = min(numeric_values)
                        max_val = max(numeric_values)

                if min_val is not None and max_val is not None:
                    property_def["constraints"] = {"min": min_val, "max": max_val}

            # ë³µí•© íƒ€ì…(ì˜ˆ: enum/phone ë“±)ì˜ ì¶”ë¡  ë©”íƒ€ë°ì´í„°ì—ì„œ ì œì•½ì¡°ê±´ ì œì•ˆì´ ìˆìœ¼ë©´ ë°˜ì˜
            suggested = (result.inferred_type.metadata or {}).get("suggested_constraints")
            if isinstance(suggested, dict) and suggested:
                property_def.setdefault("constraints", {})
                property_def["constraints"].update(suggested)

            properties.append(property_def)

        # í´ë˜ìŠ¤ ì´ë¦„ ìƒì„±
        if not class_name:
            class_name = "GeneratedClass" + datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")

        return {
            "id": self._generate_class_id(class_name),
            "label": {"ko": class_name},
            "description": {
                "ko": f"Funnel ì„œë¹„ìŠ¤ì—ì„œ ìë™ ìƒì„±ëœ ìŠ¤í‚¤ë§ˆ (ì»¬ëŸ¼ {len(properties)}ê°œ)"
            },
            "properties": properties,
            "metadata": {
                "generated_by": "funnel",
                "generation_date": datetime.now(timezone.utc).isoformat(),
                "confidence_scores": {
                    result.column_name: result.inferred_type.confidence
                    for result in analysis_results
                },
            },
        }

    def _normalize_property_name(self, column_name: str) -> str:
        """ì»¬ëŸ¼ ì´ë¦„ì„ ì†ì„± ì´ë¦„ìœ¼ë¡œ ì •ê·œí™”"""
        # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
        normalized = column_name.replace(" ", "_").replace("-", "_")
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        normalized = "".join(c for c in normalized if c.isalnum() or c == "_")
        # ì†Œë¬¸ìë¡œ ë³€í™˜
        normalized = normalized.lower()
        # ìˆ«ìë¡œ ì‹œì‘í•˜ë©´ ì•ì— 'col_' ì¶”ê°€
        if normalized and normalized[0].isdigit():
            normalized = "col_" + normalized
        # ë¹ˆ ë¬¸ìì—´ì´ë©´ ê¸°ë³¸ê°’
        if not normalized:
            normalized = "column"
        return normalized

    def _generate_class_id(self, class_name: str) -> str:
        """í´ë˜ìŠ¤ ID ìƒì„±"""
        # CamelCaseë¡œ ë³€í™˜
        parts = class_name.replace("_", " ").replace("-", " ").split()
        return "".join(word.capitalize() for word in parts)
