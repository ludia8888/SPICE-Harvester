"""
ðŸ”¥ THINK ULTRA! Funnel Data Processor Service
Data Connectorì™€ OMS/BFF ì‚¬ì´ì˜ ë°ì´í„° ì²˜ë¦¬ ë ˆì´ì–´
"""

from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

import httpx

from shared.models.google_sheets import GoogleSheetPreviewRequest, GoogleSheetPreviewResponse

from funnel.services.risk_assessor import assess_dataset_risks
from funnel.services.schema_utils import normalize_property_name
from funnel.services.type_inference import FunnelTypeInferenceService
from shared.config.service_config import ServiceConfig
from shared.models.type_inference import (
    ColumnAnalysisResult,
    DatasetAnalysisRequest,
    DatasetAnalysisResponse,
    FunnelPreviewResponse,
)


class FunnelDataProcessor:
    """
    ðŸ”¥ THINK ULTRA! ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

    Architecture Flow:
    1. Data Connector (Google Sheets, CSV, etc.) â†’ Raw Data
    2. Funnel â†’ Type Inference & Data Processing
    3. OMS/BFF â†’ Schema Generation & Storage
    """

    def __init__(self):
        self.type_inference_service = FunnelTypeInferenceService

    async def process_google_sheets_preview(
        self,
        sheet_url: str,
        worksheet_name: Optional[str] = None,
        api_key: Optional[str] = None,
        connection_id: Optional[str] = None,
        infer_types: bool = True,
        include_complex_types: bool = False,
    ) -> FunnelPreviewResponse:
        """
        Google Sheets ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  íƒ€ìž…ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.

        Args:
            sheet_url: Google Sheets URL
            worksheet_name: ì›Œí¬ì‹œíŠ¸ ì´ë¦„
            api_key: Google API í‚¤
            infer_types: íƒ€ìž… ì¶”ë¡  ì—¬ë¶€
            include_complex_types: ë³µí•© íƒ€ìž… ê²€ì‚¬ ì—¬ë¶€

        Returns:
            íƒ€ìž…ì´ ì¶”ë¡ ëœ preview ì‘ë‹µ
        """
        # 1. Google Sheets connectorë¥¼ í†µí•´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        preview_request = GoogleSheetPreviewRequest(
            sheet_url=sheet_url,
            worksheet_name=worksheet_name,
            api_key=api_key,
            connection_id=connection_id,
        )

        # Google Sheets service í˜¸ì¶œ (ì‹¤ì œë¡œëŠ” ì˜ì¡´ì„± ì£¼ìž…ìœ¼ë¡œ ì²˜ë¦¬)
        # ì—¬ê¸°ì„œëŠ” ì§ì ‘ HTTP ìš”ì²­ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{ServiceConfig.get_bff_url()}/api/v1/data-connectors/google-sheets/preview",
                json=preview_request.model_dump(),
            )

            if response.status_code != 200:
                raise Exception(f"Failed to fetch Google Sheets data: {response.text}")

            sheets_preview = GoogleSheetPreviewResponse(**response.json())

        # 2. íƒ€ìž… ì¶”ë¡  ìˆ˜í–‰
        inferred_schema = None
        risk_summary: List[Any] = []
        if infer_types and sheets_preview.sample_rows:
            analysis_results = self.type_inference_service.analyze_dataset(
                data=sheets_preview.sample_rows,
                columns=sheets_preview.columns,
                include_complex_types=include_complex_types,
            )
            risk_summary, column_risks, column_profiles = assess_dataset_risks(
                sheets_preview.sample_rows,
                sheets_preview.columns,
                analysis_results,
            )
            inferred_schema = _attach_risks_and_profiles(
                analysis_results, column_risks, column_profiles
            )

        # 3. Funnel response ìƒì„±
        return FunnelPreviewResponse(
            source_metadata={
                "type": "google_sheets",
                "sheet_id": sheets_preview.sheet_id,
                "sheet_title": sheets_preview.sheet_title,
                "worksheet_title": sheets_preview.worksheet_title,
            },
            columns=sheets_preview.columns,
            sample_data=sheets_preview.sample_rows,
            inferred_schema=inferred_schema,
            total_rows=sheets_preview.total_rows,
            preview_rows=len(sheets_preview.sample_rows),
            risk_summary=risk_summary,
        )

    async def analyze_dataset(self, request: DatasetAnalysisRequest) -> DatasetAnalysisResponse:
        """
        ë°ì´í„°ì…‹ì„ ë¶„ì„í•˜ê³  íƒ€ìž…ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.

        Args:
            request: ë°ì´í„°ì…‹ ë¶„ì„ ìš”ì²­

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        # íƒ€ìž… ì¶”ë¡  ìˆ˜í–‰
        analysis_results = self.type_inference_service.analyze_dataset(
            data=request.data,
            columns=request.columns,
            sample_size=request.sample_size,
            include_complex_types=request.include_complex_types,
        )

        risk_summary, column_risks, column_profiles = assess_dataset_risks(
            request.data, request.columns, analysis_results
        )
        analysis_results = _attach_risks_and_profiles(
            analysis_results, column_risks, column_profiles
        )

        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            "total_columns": len(request.columns),
            "analyzed_rows": min(len(request.data), request.sample_size or len(request.data)),
            "include_complex_types": request.include_complex_types,
            "analysis_version": "1.0",
        }

        return DatasetAnalysisResponse(
            columns=analysis_results,
            analysis_metadata=metadata,
            timestamp=datetime.now(timezone.utc),
            risk_summary=risk_summary,
        )

    def generate_schema_suggestion(
        self, analysis_results: List[ColumnAnalysisResult], class_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

        Args:
            analysis_results: ì»¬ëŸ¼ ë¶„ì„ ê²°ê³¼
            class_name: í´ëž˜ìŠ¤ ì´ë¦„ (ì„ íƒì‚¬í•­)

        Returns:
            OMSì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ìŠ¤í‚¤ë§ˆ ì œì•ˆ
        """
        properties = []

        for result in analysis_results:
            # ë†’ì€ confidenceì˜ íƒ€ìž…ë§Œ ì‚¬ìš©
            if result.inferred_type.confidence >= 0.7:
                data_type = result.inferred_type.type
            else:
                data_type = "xsd:string"  # ê¸°ë³¸ê°’

            property_def = {
                "name": normalize_property_name(result.column_name),
                "type": data_type,
                "label": {"ko": result.column_name},
                "description": {
                    "ko": f"{result.inferred_type.reason} (ì‹ ë¢°ë„: {result.inferred_type.confidence:.0%})"
                },
                # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°(required íŒë‹¨ ë¶ˆê°€)ëŠ” Falseë¡œ ë‘ 
                "required": result.total_count > 0 and result.null_count == 0,
                "metadata": {
                    "inferred_confidence": result.inferred_type.confidence,
                    "total_count": result.total_count,
                    "non_empty_count": result.non_empty_count,
                    "unique_count": result.unique_count,
                    "null_count": result.null_count,
                    "unique_ratio": result.unique_ratio,
                    "null_ratio": result.null_ratio,
                },
            }

            # íƒ€ìž…ë³„ ì¶”ê°€ ì œì•½ì¡°ê±´
            if (
                data_type == "xsd:string"
                and result.non_empty_count > 0
                and result.unique_count == result.non_empty_count
            ):
                property_def["constraints"] = {"unique": True}
            elif data_type in ["xsd:integer", "xsd:decimal"]:
                # Prefer engine-provided numeric stats when available
                meta = result.inferred_type.metadata or {}
                min_val = meta.get("min")
                max_val = meta.get("max")

                if min_val is None or max_val is None:
                    # Fallback to sample values (limited) when metadata is missing
                    numeric_values = []
                    for val in result.sample_values:
                        try:
                            numeric_values.append(float(val))
                        except (ValueError, TypeError):
                            continue
                    if numeric_values:
                        min_val = min(numeric_values)
                        max_val = max(numeric_values)

                if min_val is not None and max_val is not None:
                    property_def["constraints"] = {"min": min_val, "max": max_val}

            # ë³µí•© íƒ€ìž…(ì˜ˆ: enum/phone ë“±)ì˜ ì¶”ë¡  ë©”íƒ€ë°ì´í„°ì—ì„œ ì œì•½ì¡°ê±´ ì œì•ˆì´ ìžˆìœ¼ë©´ ë°˜ì˜
            suggested = (result.inferred_type.metadata or {}).get("suggested_constraints")
            if isinstance(suggested, dict) and suggested:
                property_def.setdefault("constraints", {})
                property_def["constraints"].update(suggested)

            properties.append(property_def)

        # í´ëž˜ìŠ¤ ì´ë¦„ ìƒì„±
        if not class_name:
            class_name = "GeneratedClass" + datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")

        return {
            "id": self._generate_class_id(class_name),
            "label": {"ko": class_name},
            "description": {
                "ko": f"Funnel ì„œë¹„ìŠ¤ì—ì„œ ìžë™ ìƒì„±ëœ ìŠ¤í‚¤ë§ˆ (ì»¬ëŸ¼ {len(properties)}ê°œ)"
            },
            "properties": properties,
            "metadata": {
                "generated_by": "funnel",
                "generation_date": datetime.now(timezone.utc).isoformat(),
                "confidence_scores": {
                    result.column_name: result.inferred_type.confidence
                    for result in analysis_results
                },
            },
        }

    def _normalize_property_name(self, column_name: str) -> str:
        """ì»¬ëŸ¼ ì´ë¦„ì„ ì†ì„± ì´ë¦„ìœ¼ë¡œ ì •ê·œí™”"""
        return normalize_property_name(column_name)

    def _generate_class_id(self, class_name: str) -> str:
        """í´ëž˜ìŠ¤ ID ìƒì„±"""
        # CamelCaseë¡œ ë³€í™˜
        parts = class_name.replace("_", " ").replace("-", " ").split()
        return "".join(word.capitalize() for word in parts)


def _attach_risks_and_profiles(
    results: List[ColumnAnalysisResult],
    column_risks: Dict[str, List[Any]],
    column_profiles: Dict[str, Any],
) -> List[ColumnAnalysisResult]:
    enriched: List[ColumnAnalysisResult] = []
    for result in results:
        update: Dict[str, Any] = {}
        if result.column_name in column_profiles:
            update["profile"] = column_profiles[result.column_name]
        if result.column_name in column_risks:
            update["risk_flags"] = column_risks[result.column_name]
        if update:
            enriched.append(_copy_model(result, update))
        else:
            enriched.append(result)
    return enriched


def _copy_model(model: Any, update: Dict[str, Any]) -> Any:
    if hasattr(model, "model_copy"):
        return model.model_copy(update=update)
    return model.copy(update=update)
