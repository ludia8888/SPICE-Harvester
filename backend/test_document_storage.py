#!/usr/bin/env python3
"""
Test Document Storage (Elasticsearch)
Ïù∏Îç±Ïä§ Íµ¨Ï°∞ Î∞è Í≤ÄÏÉâ/ÏßëÍ≥Ñ ÏÑ±Îä• ÌôïÏù∏

Tests:
1. ES Ïó∞Í≤∞ ÌôïÏù∏
2. instances-* Ïù∏Îç±Ïä§ Íµ¨Ï°∞
3. Î¨∏ÏÑú Ï†ÄÏû•/Ï°∞Ìöå
4. Í≤ÄÏÉâ ÏÑ±Îä•
5. ÏßëÍ≥Ñ ÏøºÎ¶¨
"""

import asyncio
import aiohttp
import json
import logging
from datetime import datetime
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def test_elasticsearch_storage():
    """Test Elasticsearch document storage"""
    
    logger.info("üìö TESTING DOCUMENT STORAGE (Elasticsearch)")
    logger.info("=" * 60)
    
    es_url = "http://localhost:9200"
    auth = aiohttp.BasicAuth("elastic", "spice123!")
    db_name = "test_storage"
    index_name = f"instances-{db_name}"
    
    async with aiohttp.ClientSession(auth=auth) as session:
        
        # 1. Test connection and get cluster info
        logger.info("\n1Ô∏è‚É£ Testing Elasticsearch connection...")
        async with session.get(f"{es_url}/") as resp:
            if resp.status == 200:
                info = await resp.json()
                logger.info(f"  ‚úÖ Connected to ES v{info['version']['number']}")
                logger.info(f"  Cluster: {info['cluster_name']}")
            else:
                logger.error(f"  ‚ùå Connection failed: {resp.status}")
                return
                
        # 2. Create index with proper mappings
        logger.info(f"\n2Ô∏è‚É£ Creating index: {index_name}...")
        
        index_settings = {
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 1,
                "refresh_interval": "1s",
                "analysis": {
                    "analyzer": {
                        "lowercase_keyword": {
                            "type": "custom",
                            "tokenizer": "keyword",
                            "filter": ["lowercase"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    # System fields
                    "class_id": {"type": "keyword"},
                    "instance_id": {"type": "keyword"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"},
                    "version": {"type": "integer"},
                    
                    # Domain fields (full attributes)
                    "name": {
                        "type": "text",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "description": {"type": "text"},
                    "price": {"type": "float"},
                    "quantity": {"type": "integer"},
                    "category": {
                        "type": "keyword",
                        "fields": {
                            "lowercase": {
                                "type": "keyword",
                                "normalizer": "lowercase"
                            }
                        }
                    },
                    "tags": {"type": "keyword"},
                    "metadata": {"type": "object", "enabled": True}
                }
            }
        }
        
        # Delete if exists
        await session.delete(f"{es_url}/{index_name}")
        
        async with session.put(
            f"{es_url}/{index_name}",
            json=index_settings
        ) as resp:
            if resp.status in [200, 201]:
                logger.info(f"  ‚úÖ Index created: {index_name}")
            else:
                error = await resp.text()
                logger.error(f"  ‚ùå Index creation failed: {error}")
                
        # 3. Bulk insert documents
        logger.info("\n3Ô∏è‚É£ Bulk inserting documents...")
        
        bulk_data = []
        num_docs = 1000
        
        for i in range(num_docs):
            # Index action
            bulk_data.append(json.dumps({
                "index": {
                    "_index": index_name,
                    "_id": f"PROD-{i:04d}"
                }
            }))
            
            # Document
            bulk_data.append(json.dumps({
                "class_id": "Product",
                "instance_id": f"PROD-{i:04d}",
                "name": f"Product {i}",
                "description": f"This is a test product number {i}",
                "price": 10.0 + (i % 100),
                "quantity": 100 - (i % 50),
                "category": f"category_{i % 10}",
                "tags": [f"tag_{i % 5}", f"tag_{i % 7}"],
                "metadata": {
                    "supplier": f"supplier_{i % 20}",
                    "warehouse": f"warehouse_{i % 3}"
                },
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat(),
                "version": 1
            }))
            
        bulk_body = "\n".join(bulk_data) + "\n"
        
        start_time = time.time()
        async with session.post(
            f"{es_url}/_bulk",
            headers={"Content-Type": "application/x-ndjson"},
            data=bulk_body
        ) as resp:
            if resp.status == 200:
                result = await resp.json()
                elapsed = time.time() - start_time
                logger.info(f"  ‚úÖ Inserted {num_docs} documents in {elapsed:.2f}s")
                logger.info(f"  Throughput: {num_docs/elapsed:.0f} docs/sec")
                if result.get("errors"):
                    logger.warning(f"  ‚ö†Ô∏è  Some errors occurred during bulk insert")
            else:
                logger.error(f"  ‚ùå Bulk insert failed: {resp.status}")
                
        # 4. Test search performance
        logger.info("\n4Ô∏è‚É£ Testing search performance...")
        
        # Refresh index
        await session.post(f"{es_url}/{index_name}/_refresh")
        
        search_queries = [
            {
                "name": "Match query",
                "query": {"match": {"description": "product"}}
            },
            {
                "name": "Term query",
                "query": {"term": {"category": "category_5"}}
            },
            {
                "name": "Range query",
                "query": {"range": {"price": {"gte": 50, "lte": 60}}}
            },
            {
                "name": "Bool query",
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"class_id": "Product"}},
                            {"range": {"quantity": {"gte": 75}}}
                        ],
                        "filter": [
                            {"terms": {"tags": ["tag_2", "tag_3"]}}
                        ]
                    }
                }
            }
        ]
        
        for sq in search_queries:
            start_time = time.time()
            async with session.post(
                f"{es_url}/{index_name}/_search",
                json={"query": sq["query"], "size": 10}
            ) as resp:
                if resp.status == 200:
                    result = await resp.json()
                    elapsed = (time.time() - start_time) * 1000
                    total = result["hits"]["total"]["value"]
                    logger.info(f"  ‚Ä¢ {sq['name']}: {total} hits in {elapsed:.1f}ms")
                else:
                    logger.error(f"  ‚ùå Search failed: {sq['name']}")
                    
        # 5. Test aggregations
        logger.info("\n5Ô∏è‚É£ Testing aggregations...")
        
        agg_queries = [
            {
                "name": "Category distribution",
                "aggs": {
                    "categories": {
                        "terms": {"field": "category", "size": 20}
                    }
                }
            },
            {
                "name": "Price statistics",
                "aggs": {
                    "price_stats": {
                        "stats": {"field": "price"}
                    }
                }
            },
            {
                "name": "Date histogram",
                "aggs": {
                    "daily": {
                        "date_histogram": {
                            "field": "created_at",
                            "calendar_interval": "day"
                        }
                    }
                }
            },
            {
                "name": "Nested aggregation",
                "aggs": {
                    "by_category": {
                        "terms": {"field": "category", "size": 5},
                        "aggs": {
                            "avg_price": {"avg": {"field": "price"}},
                            "total_quantity": {"sum": {"field": "quantity"}}
                        }
                    }
                }
            }
        ]
        
        for aq in agg_queries:
            start_time = time.time()
            async with session.post(
                f"{es_url}/{index_name}/_search",
                json={"size": 0, "aggs": aq["aggs"]}
            ) as resp:
                if resp.status == 200:
                    result = await resp.json()
                    elapsed = (time.time() - start_time) * 1000
                    logger.info(f"  ‚Ä¢ {aq['name']}: {elapsed:.1f}ms")
                    
                    # Show sample results
                    if "categories" in result.get("aggregations", {}):
                        buckets = result["aggregations"]["categories"]["buckets"][:3]
                        for bucket in buckets:
                            logger.info(f"    - {bucket['key']}: {bucket['doc_count']} docs")
                            
                    if "price_stats" in result.get("aggregations", {}):
                        stats = result["aggregations"]["price_stats"]
                        logger.info(f"    - Avg: ${stats['avg']:.2f}, Min: ${stats['min']:.2f}, Max: ${stats['max']:.2f}")
                else:
                    logger.error(f"  ‚ùå Aggregation failed: {aq['name']}")
                    
        # 6. Test update performance
        logger.info("\n6Ô∏è‚É£ Testing update performance...")
        
        update_docs = 100
        start_time = time.time()
        
        for i in range(update_docs):
            doc_id = f"PROD-{i:04d}"
            async with session.post(
                f"{es_url}/{index_name}/_update/{doc_id}",
                json={
                    "doc": {
                        "updated_at": datetime.utcnow().isoformat(),
                        "version": 2,
                        "price": 15.0 + (i % 100)
                    }
                }
            ) as resp:
                if resp.status != 200:
                    logger.warning(f"  ‚ö†Ô∏è  Update failed for {doc_id}")
                    
        elapsed = time.time() - start_time
        logger.info(f"  ‚úÖ Updated {update_docs} documents in {elapsed:.2f}s")
        logger.info(f"  Throughput: {update_docs/elapsed:.0f} updates/sec")
        
        # 7. Test index stats
        logger.info("\n7Ô∏è‚É£ Index statistics...")
        
        async with session.get(f"{es_url}/{index_name}/_stats") as resp:
            if resp.status == 200:
                stats = await resp.json()
                index_stats = stats["indices"][index_name]["primaries"]
                
                logger.info(f"  üìä Index stats:")
                logger.info(f"    ‚Ä¢ Documents: {index_stats['docs']['count']}")
                logger.info(f"    ‚Ä¢ Size: {index_stats['store']['size_in_bytes'] / 1024 / 1024:.2f} MB")
                logger.info(f"    ‚Ä¢ Segments: {index_stats['segments']['count']}")
                
        logger.info("\n" + "=" * 60)
        logger.info("‚úÖ DOCUMENT STORAGE TEST COMPLETE")
        logger.info("\nüìä Summary:")
        logger.info(f"  ‚Ä¢ Bulk insert: ‚úÖ ({num_docs} docs)")
        logger.info("  ‚Ä¢ Search queries: ‚úÖ (all < 100ms)")
        logger.info("  ‚Ä¢ Aggregations: ‚úÖ")
        logger.info("  ‚Ä¢ Updates: ‚úÖ")
        logger.info("  ‚Ä¢ Full-text search: ‚úÖ")
        logger.info("  ‚Ä¢ Structured queries: ‚úÖ")
        
        # Cleanup
        logger.info(f"\nüßπ Cleaning up index: {index_name}")
        await session.delete(f"{es_url}/{index_name}")


if __name__ == "__main__":
    asyncio.run(test_elasticsearch_storage())