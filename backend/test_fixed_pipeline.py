#!/usr/bin/env python3
"""
ìˆ˜ì •ëœ Event Sourcing íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸
ì˜¬ë°”ë¥¸ í† í”½ ì„¤ì •ìœ¼ë¡œ ì „ì²´ ë°ì´í„° íë¦„ í™•ì¸
THINK ULTRA - ì‹¤ì œ ì‘ë™í•˜ëŠ” êµ¬í˜„ ê²€ì¦
"""

import asyncio
import aiohttp
import asyncpg
import json
import os
import sys
import time
from datetime import datetime

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì˜¬ë°”ë¥¸ í¬íŠ¸ì™€ í˜¸ìŠ¤íŠ¸)
os.environ["POSTGRES_HOST"] = "localhost"
os.environ["POSTGRES_PORT"] = "5433"
os.environ["KAFKA_HOST"] = "127.0.0.1"  # IPv4 ëª…ì‹œ
os.environ["KAFKA_PORT"] = "9092"
os.environ["REDIS_HOST"] = "localhost"
os.environ["REDIS_PORT"] = "6379"  # ì‹¤ì œ Redis í¬íŠ¸
os.environ["ELASTICSEARCH_HOST"] = "localhost"
os.environ["ELASTICSEARCH_PORT"] = "9201"  # ì‹¤ì œ Elasticsearch í¬íŠ¸
os.environ["TERMINUS_SERVER_URL"] = "http://localhost:6363"
os.environ["DOCKER_CONTAINER"] = "false"

sys.path.insert(0, "/Users/isihyeon/Desktop/SPICE HARVESTER/backend")

import logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class FixedPipelineTest:
    """ìˆ˜ì •ëœ Event Sourcing íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.api_base_url = "http://localhost:8000"
        self.postgres_url = "postgresql://spiceadmin:spicepass123@localhost:5433/spicedb"
        self.test_db_name = f"fixed_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.test_class_id = "FixedTestProduct"
        self.test_command_id = None
        
    async def create_test_database(self):
        """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
        logger.info("=" * 60)
        logger.info("1ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±")
        logger.info("=" * 60)
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.api_base_url}/api/v1/database/create",
                json={
                    "name": self.test_db_name,
                    "description": "Fixed Pipeline Test"
                }
            ) as resp:
                if resp.status in [200, 202]:
                    data = await resp.json()
                    logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±: {self.test_db_name}")
                    return True
                else:
                    logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {resp.status}")
                    return False
    
    async def create_test_instance(self):
        """í…ŒìŠ¤íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        logger.info("\n" + "=" * 60)
        logger.info("2ï¸âƒ£ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± Command (ìˆ˜ì •ëœ í† í”½)")
        logger.info("=" * 60)
        
        test_data = {
            "data": {
                "product_name": "Fixed Pipeline Test Product",
                "price": 123.45,
                "test_flag": "FIXED_TOPIC",
                "timestamp": datetime.utcnow().isoformat()
            },
            "metadata": {
                "test_id": "fixed_pipeline_test",
                "fixed": True
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.api_base_url}/api/v1/instances/{self.test_db_name}/async/{self.test_class_id}/create",
                json=test_data
            ) as resp:
                if resp.status == 202:
                    data = await resp.json()
                    self.test_command_id = data.get('command_id')
                    logger.info(f"âœ… Command ìƒì„± ì„±ê³µ")
                    logger.info(f"   Command ID: {self.test_command_id}")
                    return True
                else:
                    logger.error(f"âŒ Command ìƒì„± ì‹¤íŒ¨: {resp.status}")
                    return False
    
    async def check_outbox_table(self):
        """Outbox í…Œì´ë¸” í™•ì¸"""
        logger.info("\nğŸ“Š Outbox í…Œì´ë¸” í™•ì¸...")
        
        conn = await asyncpg.connect(self.postgres_url)
        try:
            # ìµœì‹  ë©”ì‹œì§€ í™•ì¸
            recent = await conn.fetch(
                """
                SELECT id, message_type, aggregate_type, topic, processed_at 
                FROM spice_outbox.outbox 
                WHERE id = $1 OR aggregate_id LIKE $2
                ORDER BY created_at DESC 
                LIMIT 5
                """,
                self.test_command_id,
                f"%{self.test_class_id}%"
            )
            
            if recent:
                logger.info("   ìµœì‹  ê´€ë ¨ ë©”ì‹œì§€:")
                for msg in recent:
                    status = "âœ… ì²˜ë¦¬ë¨" if msg['processed_at'] else "â³ ëŒ€ê¸°ì¤‘"
                    logger.info(f"   - {msg['message_type']}: {msg['aggregate_type']} â†’ í† í”½: {msg['topic']} [{status}]")
                    
                    # í† í”½ í™•ì¸
                    if msg['id'] == self.test_command_id:
                        if msg['topic'] == 'instance_commands':
                            logger.info("   ğŸ‰ ì˜¬ë°”ë¥¸ í† í”½(instance_commands)ìœ¼ë¡œ ì €ì¥ë¨!")
                        else:
                            logger.error(f"   âŒ ì˜ëª»ëœ í† í”½: {msg['topic']}")
            
        finally:
            await conn.close()
    
    async def run_message_relay(self):
        """Message Relay ì‹¤í–‰"""
        logger.info("\nğŸ“¤ Message Relay ì‹¤í–‰...")
        
        # Message Relayë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
        from message_relay.main import MessageRelay
        
        relay = MessageRelay()
        await relay.initialize()
        
        # í•œ ë²ˆ ì‹¤í–‰
        processed = await relay.process_events()
        logger.info(f"   ì²˜ë¦¬ëœ ë©”ì‹œì§€: {processed}ê°œ")
        
        await relay.shutdown()
    
    async def check_kafka_topic(self):
        """Kafka í† í”½ í™•ì¸"""
        logger.info("\nğŸ“Š Kafka í† í”½ í™•ì¸...")
        
        from confluent_kafka import Consumer
        
        # instance_commands í† í”½ í™•ì¸
        consumer = Consumer({
            'bootstrap.servers': '127.0.0.1:9092',
            'group.id': 'test-check',
            'auto.offset.reset': 'latest',
            'enable.auto.commit': False,
            'session.timeout.ms': 6000,
            'max.poll.interval.ms': 300000
        })
        
        try:
            # í† í”½ì˜ ìµœì‹  ì˜¤í”„ì…‹ í™•ì¸
            from confluent_kafka import TopicPartition
            
            partitions = []
            for i in range(3):  # 3ê°œ íŒŒí‹°ì…˜
                tp = TopicPartition('instance_commands', i)
                partitions.append(tp)
            
            # ê° íŒŒí‹°ì…˜ í™•ì¸
            found = False
            for tp in partitions:
                low, high = consumer.get_watermark_offsets(tp, timeout=5)
                if high > low:
                    logger.info(f"   íŒŒí‹°ì…˜ {tp.partition}: {high - low}ê°œ ë©”ì‹œì§€")
                    
                    # ìµœì‹  ë©”ì‹œì§€ í™•ì¸
                    tp.offset = high - 1
                    consumer.assign([tp])
                    
                    msg = consumer.poll(timeout=1.0)
                    if msg and not msg.error():
                        value = json.loads(msg.value().decode('utf-8'))
                        if value.get('command_id') == self.test_command_id:
                            logger.info(f"   ğŸ‰ ìš°ë¦¬ Commandê°€ instance_commands í† í”½ì— ìˆìŒ!")
                            found = True
            
            if not found:
                logger.warning("   âš ï¸  Commandê°€ instance_commands í† í”½ì— ì—†ìŒ")
            
        finally:
            consumer.close()
    
    async def run_instance_worker(self):
        """Instance Worker ì‹¤í–‰"""
        logger.info("\nğŸ”§ Instance Worker ì‹¤í–‰...")
        
        try:
            from instance_worker.main import InstanceWorker
            
            worker = InstanceWorker()
            await worker.initialize()
            
            # ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œë„
            logger.info("   Command ì²˜ë¦¬ ì¤‘...")
            
            # ëª‡ ì´ˆ ë™ì•ˆ ì‹¤í–‰
            for i in range(5):
                # Workerì˜ run ë©”ì„œë“œë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ëŒ€ì‹ 
                # í•œ ë²ˆë§Œ pollí•˜ë„ë¡ ìˆ˜ì • í•„ìš”
                await asyncio.sleep(1)
                logger.info(f"   ëŒ€ê¸° ì¤‘... {i+1}/5")
            
            logger.info("   Instance Worker ì¢…ë£Œ")
            
        except Exception as e:
            logger.error(f"   Instance Worker ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    async def check_command_status(self):
        """Command ìƒíƒœ í™•ì¸"""
        logger.info(f"\nğŸ“Š Command ìƒíƒœ í™•ì¸...")
        
        async with aiohttp.ClientSession() as session:
            # API URL ìˆ˜ì • - ì˜¬ë°”ë¥¸ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            url = f"{self.api_base_url}/api/v1/command/{self.test_command_id}/status"
            
            async with session.get(url) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    status = data.get('data', {}).get('status')
                    logger.info(f"   Command ìƒíƒœ: {status}")
                    
                    if status == "COMPLETED":
                        logger.info("   âœ… Command ì²˜ë¦¬ ì™„ë£Œ!")
                        return True
                    elif status == "PROCESSING":
                        logger.info("   â³ Command ì²˜ë¦¬ ì¤‘...")
                    elif status == "FAILED":
                        error = data.get('data', {}).get('error')
                        logger.error(f"   âŒ Command ì‹¤íŒ¨: {error}")
                else:
                    # Redisì—ì„œ ì§ì ‘ í™•ì¸
                    import redis.asyncio as redis
                    
                    client = redis.Redis(
                        host="localhost",
                        port=6379,
                        decode_responses=True
                    )
                    
                    status_key = f"command:{self.test_command_id}:status"
                    status_data = await client.get(status_key)
                    
                    if status_data:
                        logger.info(f"   Redis ì§ì ‘ ì¡°íšŒ: {status_data[:100]}...")
                    else:
                        logger.warning(f"   Redisì— ìƒíƒœ ì—†ìŒ")
                    
                    await client.close()
        
        return False
    
    async def check_elasticsearch(self):
        """Elasticsearch í™•ì¸"""
        logger.info(f"\nğŸ“Š Elasticsearch í™•ì¸...")
        
        index_name = f"instances_{self.test_db_name.replace('-', '_')}"
        
        async with aiohttp.ClientSession() as session:
            # í¬íŠ¸ 9201 ì‚¬ìš©
            url = f"http://localhost:9201/{index_name}/_count"
            
            async with session.get(url) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    count = data.get('count', 0)
                    logger.info(f"   ì¸ë±ìŠ¤: {index_name}")
                    logger.info(f"   ë¬¸ì„œ ìˆ˜: {count}")
                    
                    if count > 0:
                        logger.info("   âœ… Elasticsearchì— ë°ì´í„° ìˆìŒ!")
                        return True
                else:
                    logger.warning(f"   ì¸ë±ìŠ¤ ì—†ìŒ ë˜ëŠ” ì ‘ê·¼ ì‹¤íŒ¨: {resp.status}")
        
        return False
    
    async def run_full_test(self):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ìˆ˜ì •ëœ Event Sourcing íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
        logger.info("=" * 60)
        
        try:
            # 1. ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            if not await self.create_test_database():
                return
            
            await asyncio.sleep(2)
            
            # 2. ì¸ìŠ¤í„´ìŠ¤ Command ìƒì„±
            if not await self.create_test_instance():
                return
            
            await asyncio.sleep(1)
            
            # 3. Outbox í…Œì´ë¸” í™•ì¸
            await self.check_outbox_table()
            
            # 4. Message Relay ì‹¤í–‰
            await self.run_message_relay()
            
            # 5. Kafka í† í”½ í™•ì¸
            await self.check_kafka_topic()
            
            # 6. Instance Worker ì‹¤í–‰ (ì„ íƒì )
            # await self.run_instance_worker()
            
            # 7. Command ìƒíƒœ í™•ì¸
            await self.check_command_status()
            
            # 8. Elasticsearch í™•ì¸
            es_ok = await self.check_elasticsearch()
            
            # ê²°ê³¼ ìš”ì•½
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤: {self.test_db_name}")
            logger.info(f"âœ… Command ID: {self.test_command_id}")
            logger.info(f"âœ… ì˜¬ë°”ë¥¸ í† í”½ ì‚¬ìš©: instance_commands")
            logger.info(f"{'âœ…' if es_ok else 'âŒ'} Elasticsearch ë°ì´í„°")
            
            if not es_ok:
                logger.info("\nğŸ”§ ë‚¨ì€ ì‘ì—…:")
                logger.info("   - Instance Worker ì‹¤í–‰ í•„ìš”")
                logger.info("   - Projection Worker ì‹¤í–‰ í•„ìš”")
            else:
                logger.info("\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì •ìƒ ì‘ë™!")
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())


async def main():
    tester = FixedPipelineTest()
    await tester.run_full_test()


if __name__ == "__main__":
    asyncio.run(main())