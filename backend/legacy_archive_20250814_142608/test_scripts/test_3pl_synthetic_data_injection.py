#!/usr/bin/env python3
"""
SPICE HARVESTER 3PL Synthetic Data Injection Tool

ì‹¤ì œ í•©ì„± ë°ì´í„° (37K+ ë ˆì½”ë“œ)ë¥¼ ì‚¬ìš©í•˜ì—¬ SPICE HARVESTER ì‹œìŠ¤í…œì„ 
ì‹¤ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ë¡œ ì™„ì „ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë„êµ¬

Features:
- 14ê°œ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ìë™ ìƒì„±
- 37K+ ë ˆì½”ë“œ ë°°ì¹˜ ì²˜ë¦¬ ì£¼ì…
- ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§
- Event Sourcing íŒŒì´í”„ë¼ì¸ ê²€ì¦
- ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ìë™í™”
"""

import asyncio
import csv
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import urllib.request
import urllib.parse
import urllib.error
import aiohttp
import pandas as pd

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SyntheticDataInjector:
    """3PL í•©ì„± ë°ì´í„° ì£¼ì… ë„êµ¬"""
    
    def __init__(self, base_url: str = "http://localhost:8000", data_dir: str = None):
        self.base_url = base_url
        self.data_dir = Path(data_dir) if data_dir else Path("../test_data/spice_harvester_synthetic_3pl")
        self.db_name = "spice_3pl_synthetic"
        self.session: Optional[aiohttp.ClientSession] = None
        
        # í†µê³„ ì¶”ì 
        self.stats = {
            "ontologies_created": 0,
            "instances_created": 0,
            "errors": 0,
            "start_time": None,
            "processing_times": []
        }
        
        # ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ontology.jsonld ê¸°ë°˜)
        self.ontology_schemas = self._load_ontology_schemas()
        
    def _load_ontology_schemas(self) -> Dict[str, Dict]:
        """ontology.jsonldì—ì„œ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ë° SPICE HARVESTER í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        ontology_file = self.data_dir / "ontology.jsonld"
        
        with open(ontology_file, 'r', encoding='utf-8') as f:
            ontology_data = json.load(f)
        
        schemas = {}
        
        # ê° ì—”í‹°í‹°ë¥¼ SPICE HARVESTER ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
        for entity in ontology_data["entities"]:
            entity_id = entity["@id"]
            properties = []
            
            for prop_name in entity["properties"]:
                # CSV ìƒ˜í”Œ ë°ì´í„°ì—ì„œ íƒ€ì… ì¶”ë¡ 
                prop_type = self._infer_property_type(entity_id, prop_name)
                
                properties.append({
                    "name": prop_name,
                    "type": prop_type,
                    "label": prop_name.replace('_', ' ').title(),
                    "description": f"{entity_id} {prop_name}",
                    "required": prop_name.endswith('_id')  # ID í•„ë“œëŠ” í•„ìˆ˜
                })
            
            schemas[entity_id] = {
                "id": entity_id,
                "label": entity_id,
                "description": f"{entity_id} entity for 3PL synthetic dataset",
                "properties": properties
            }
        
        return schemas
    
    def _infer_property_type(self, entity_id: str, prop_name: str) -> str:
        """CSV ë°ì´í„°ì—ì„œ ì†ì„± íƒ€ì… ì¶”ë¡ """
        # ê¸°ë³¸ íƒ€ì… ë§¤í•‘
        if prop_name.endswith('_id'):
            return "string"
        elif prop_name.endswith('_date') or prop_name.endswith('_datetime') or prop_name.endswith('_at'):
            return "datetime"
        elif prop_name in ['qty', 'quantity', 'line_no', 'weight_g']:
            return "integer"
        elif prop_name in ['unit_cost', 'unit_price', 'cost_per_unit']:
            return "decimal"
        elif prop_name in ['payload']:
            return "json"
        else:
            return "string"
    
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def create_all_ontologies(self) -> bool:
        """ëª¨ë“  ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ìƒì„±"""
        logger.info(f"ğŸ—ï¸ Creating {len(self.ontology_schemas)} ontology classes...")
        
        success_count = 0
        for entity_name, schema in self.ontology_schemas.items():
            try:
                success = await self._create_ontology(entity_name, schema)
                if success:
                    success_count += 1
                    self.stats["ontologies_created"] += 1
                    logger.info(f"âœ… Created ontology: {entity_name}")
                else:
                    logger.error(f"âŒ Failed to create ontology: {entity_name}")
                    self.stats["errors"] += 1
                
                # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"âŒ Error creating ontology {entity_name}: {e}")
                self.stats["errors"] += 1
        
        logger.info(f"ğŸ¯ Ontology creation completed: {success_count}/{len(self.ontology_schemas)} successful")
        return success_count == len(self.ontology_schemas)
    
    async def _create_ontology(self, entity_name: str, schema: Dict) -> bool:
        """ê°œë³„ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ìƒì„±"""
        url = f"{self.base_url}/api/v1/ontology/{self.db_name}/create"
        
        try:
            async with self.session.post(url, json=schema) as response:
                if response.status == 202:  # Event Sourcing ëª¨ë“œ
                    result = await response.json()
                    command_id = result["data"]["command_id"]
                    logger.info(f"ğŸ“ Ontology {entity_name} creation command accepted: {command_id}")
                    return True
                else:
                    error_text = await response.text()
                    logger.error(f"âŒ Failed to create {entity_name}: {response.status} - {error_text}")
                    return False
                    
        except Exception as e:
            logger.error(f"âŒ Exception creating {entity_name}: {e}")
            return False
    
    async def inject_all_data(self) -> bool:
        """ëª¨ë“  CSV ë°ì´í„° ì£¼ì… (ì˜ì¡´ì„± ìˆœì„œ ê³ ë ¤)"""
        logger.info("ğŸš€ Starting massive data injection (37K+ records)...")
        self.stats["start_time"] = time.time()
        
        # ì˜ì¡´ì„± ìˆœì„œëŒ€ë¡œ ë°ì´í„° ì£¼ì…
        injection_order = [
            ("Client", "clients.csv"),
            ("Supplier", "suppliers.csv"), 
            ("Warehouse", "warehouses.csv"),
            ("Location", "locations.csv"),
            ("Product", "products.csv"),
            ("SKU", "skus.csv"),
            ("Inbound", "inbounds.csv"),
            ("Receipt", "receipts.csv"),
            ("Order", "orders.csv"),
            ("OrderItem", "order_items.csv"),
            ("Tracking", "tracking.csv"),
            ("InventorySnapshot", "inventory_snapshots.csv"),
            ("Return", "returns.csv"),
            ("Exception", "exceptions.csv"),
            ("Event", "events.csv"),
            ("Outbox", "outbox.csv")
        ]
        
        total_success = 0
        total_records = 0
        
        for entity_name, csv_file in injection_order:
            logger.info(f"ğŸ“Š Processing {entity_name} from {csv_file}...")
            
            file_path = self.data_dir / csv_file
            if not file_path.exists():
                logger.warning(f"âš ï¸ File not found: {csv_file}")
                continue
            
            success, count = await self._inject_csv_data(entity_name, file_path)
            total_success += success
            total_records += count
            
            logger.info(f"âœ… {entity_name}: {success}/{count} records injected")
            
            # ê° íŒŒì¼ ì²˜ë¦¬ í›„ ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€)
            await asyncio.sleep(1.0)
        
        # ìµœì¢… í†µê³„
        elapsed_time = time.time() - self.stats["start_time"]
        logger.info(f"ğŸ¯ Data injection completed!")
        logger.info(f"ğŸ“ˆ Total: {total_success}/{total_records} records in {elapsed_time:.2f}s")
        logger.info(f"âš¡ Rate: {total_success/elapsed_time:.2f} records/sec")
        
        return total_success == total_records
    
    async def _inject_csv_data(self, entity_name: str, file_path: Path) -> Tuple[int, int]:
        """CSV íŒŒì¼ ë°ì´í„° ë°°ì¹˜ ì£¼ì…"""
        try:
            df = pd.read_csv(file_path)
            total_count = len(df)
            success_count = 0
            
            # ë°°ì¹˜ í¬ê¸° (API ë¶€í•˜ ê³ ë ¤)
            batch_size = 50
            
            for i in range(0, total_count, batch_size):
                batch = df.iloc[i:i+batch_size]
                batch_success = await self._inject_batch(entity_name, batch)
                success_count += batch_success
                
                # ì§„í–‰ìƒí™© í‘œì‹œ
                if i % (batch_size * 10) == 0:
                    progress = (i / total_count) * 100
                    logger.info(f"   Progress: {progress:.1f}% ({i}/{total_count})")
                
                # API ë¶€í•˜ ë°©ì§€
                await asyncio.sleep(0.1)
            
            return success_count, total_count
            
        except Exception as e:
            logger.error(f"âŒ Error processing {file_path}: {e}")
            return 0, 0
    
    async def _inject_batch(self, entity_name: str, batch_df: pd.DataFrame) -> int:
        """ë°°ì¹˜ ë‹¨ìœ„ ë°ì´í„° ì£¼ì…"""
        success_count = 0
        
        for _, row in batch_df.iterrows():
            try:
                # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                instance_data = {k: (None if pd.isna(v) else v) for k, v in row.to_dict().items()}
                
                success = await self._create_instance(entity_name, instance_data)
                if success:
                    success_count += 1
                    self.stats["instances_created"] += 1
                else:
                    self.stats["errors"] += 1
                    
            except Exception as e:
                logger.error(f"âŒ Error injecting instance: {e}")
                self.stats["errors"] += 1
        
        return success_count
    
    async def _create_instance(self, entity_name: str, instance_data: Dict) -> bool:
        """ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Event Sourcing)"""
        url = f"{self.base_url}/api/v1/instances/{self.db_name}/async/{entity_name}/create"
        
        payload = {
            "data": instance_data
        }
        
        try:
            async with self.session.post(url, json=payload) as response:
                if response.status == 202:  # Event Sourcing ì„±ê³µ
                    return True
                else:
                    # ì‹¤íŒ¨í•œ ê²½ìš° ê°€ë” ë¡œê·¸ (ë„ˆë¬´ ë§ìœ¼ë©´ ë¡œê·¸ í­ì¦)
                    if self.stats["errors"] % 100 == 0:
                        error_text = await response.text()
                        logger.warning(f"âŒ Instance creation failed: {response.status} - {error_text[:200]}")
                    return False
                    
        except Exception as e:
            # ì˜ˆì™¸ë„ ê°€ë”ë§Œ ë¡œê·¸
            if self.stats["errors"] % 100 == 0:
                logger.warning(f"âŒ Instance creation exception: {e}")
            return False
    
    def print_final_statistics(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        elapsed = time.time() - self.stats["start_time"] if self.stats["start_time"] else 0
        
        print("\n" + "="*60)
        print("ğŸ† SPICE HARVESTER 3PL Synthetic Data Injection Complete!")
        print("="*60)
        print(f"ğŸ“Š Ontologies Created: {self.stats['ontologies_created']}")
        print(f"ğŸ“Š Instances Created: {self.stats['instances_created']}")
        print(f"âŒ Errors: {self.stats['errors']}")
        print(f"â±ï¸ Total Time: {elapsed:.2f} seconds")
        print(f"âš¡ Processing Rate: {self.stats['instances_created']/elapsed:.2f} instances/sec")
        print(f"âœ… Success Rate: {(self.stats['instances_created']/(self.stats['instances_created']+self.stats['errors'])*100):.2f}%")
        print("="*60)

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ SPICE HARVESTER 3PL Synthetic Data Injection Starting...")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    data_dir = "/Users/isihyeon/Desktop/SPICE HARVESTER/test_data/spice_harvester_synthetic_3pl"
    if not Path(data_dir).exists():
        logger.error(f"âŒ Data directory not found: {data_dir}")
        return
    
    async with SyntheticDataInjector(data_dir=data_dir) as injector:
        # Phase 1: ì˜¨í†¨ë¡œì§€ ìƒì„±
        logger.info("ğŸ—ï¸ Phase 1: Creating ontology schemas...")
        ontology_success = await injector.create_all_ontologies()
        
        if not ontology_success:
            logger.error("âŒ Ontology creation failed. Stopping.")
            return
        
        # ì˜¨í†¨ë¡œì§€ ìƒì„± ì™„ë£Œ ëŒ€ê¸°
        logger.info("â³ Waiting for ontology creation to complete...")
        await asyncio.sleep(10)
        
        # Phase 2: ë°ì´í„° ì£¼ì…
        logger.info("ğŸ“Š Phase 2: Injecting synthetic data...")
        data_success = await injector.inject_all_data()
        
        # ìµœì¢… í†µê³„
        injector.print_final_statistics()
        
        if ontology_success and data_success:
            logger.info("ğŸ‰ All phases completed successfully!")
        else:
            logger.error("âŒ Some phases failed. Check logs for details.")

if __name__ == "__main__":
    asyncio.run(main())