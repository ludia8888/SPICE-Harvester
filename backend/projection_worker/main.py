"""
Projection Worker Service
InstanceÏôÄ Ontology Ïù¥Î≤§Ìä∏Î•º ElasticsearchÏóê ÌîÑÎ°úÏ†ùÏÖòÌïòÎäî ÏõåÏª§ ÏÑúÎπÑÏä§
"""

import asyncio
import json
import logging
import os
import signal
from contextlib import suppress
from datetime import datetime, timezone
from typing import Optional, Dict, Any, List

from confluent_kafka import Consumer, Producer, KafkaError, KafkaException, TopicPartition

from shared.config.service_config import ServiceConfig
from shared.config.search_config import (
    get_instances_index_name,
    get_ontologies_index_name,
    DEFAULT_INDEX_SETTINGS
)
from shared.config.app_config import AppConfig
from shared.config.settings import ApplicationSettings
from shared.models.event_envelope import EventEnvelope
from shared.models.events import (
    BaseEvent, EventType,
    InstanceEvent,
    OntologyEvent
)
from shared.services.redis_service import RedisService, create_redis_service
from shared.services.elasticsearch_service import ElasticsearchService, create_elasticsearch_service
from shared.services.projection_manager import ProjectionManager
from shared.services.processed_event_registry import ClaimDecision, ProcessedEventRegistry
from shared.services.lineage_store import LineageStore
from shared.services.audit_log_store import AuditLogStore
from shared.utils.chaos import maybe_crash

# Observability imports
from shared.observability.tracing import get_tracing_service
from shared.observability.metrics import get_metrics_collector
from shared.observability.context_propagation import ContextPropagator

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class _InProgressLeaseError(RuntimeError):
    """Raised when another worker holds the processed_events lease for this event."""


class ProjectionWorker:
    """InstanceÏôÄ Ontology Ïù¥Î≤§Ìä∏Î•º ElasticsearchÏóê ÌîÑÎ°úÏ†ùÏÖòÌïòÎäî ÏõåÏª§

    Kafka message contract:
    - Projection topics carry EventEnvelope JSON (metadata.kind == "domain")
    """

    def __init__(self):
        self.running = False
        self.kafka_servers = ServiceConfig.get_kafka_bootstrap_servers()
        self.consumer: Optional[Consumer] = None
        self.producer: Optional[Producer] = None
        self.redis_service: Optional[RedisService] = None
        self.elasticsearch_service: Optional[ElasticsearchService] = None
        self.projection_manager: Optional[ProjectionManager] = None
        self.tracing_service = None
        self.metrics_collector = None
        self.context_propagator = ContextPropagator()

        # Durable idempotency (Postgres)
        self.enable_processed_event_registry = (
            os.getenv("ENABLE_PROCESSED_EVENT_REGISTRY", "true").lower() == "true"
        )
        self.processed_event_registry: Optional[ProcessedEventRegistry] = None

        # First-class provenance/audit (fail-open by default)
        self.enable_lineage = os.getenv("ENABLE_LINEAGE", "true").strip().lower() in {"1", "true", "yes", "on"}
        self.enable_audit_logs = os.getenv("ENABLE_AUDIT_LOGS", "true").strip().lower() in {"1", "true", "yes", "on"}
        self.lineage_store: Optional[LineageStore] = None
        self.audit_store: Optional[AuditLogStore] = None

        # ÏÉùÏÑ±Îêú Ïù∏Îç±Ïä§ Ï∫êÏãú (Ï§ëÎ≥µ ÏÉùÏÑ± Î∞©ÏßÄ)
        self.created_indices = set()

        # DLQ ÌÜ†ÌîΩ
        self.dlq_topic = AppConfig.PROJECTION_DLQ_TOPIC

        # Ïû¨ÏãúÎèÑ ÏÑ§Ï†ï
        self.max_retries = int(os.getenv("PROJECTION_WORKER_MAX_RETRIES", "5"))
        self.retry_count = {}

        # Cache Stampede Î∞©ÏßÄ Î™®ÎãàÌÑ∞ÎßÅ Î©îÌä∏Î¶≠
        self.cache_metrics = {
            "cache_hits": 0,
            "cache_misses": 0,
            "negative_cache_hits": 0,
            "lock_acquisitions": 0,
            "lock_failures": 0,
            "elasticsearch_queries": 0,
            "fallback_queries": 0,
            "total_lock_wait_time": 0.0,
        }

    @staticmethod
    def _is_es_version_conflict(error: Exception) -> bool:
        status = getattr(error, "status_code", None)
        meta = getattr(error, "meta", None)
        if meta is not None:
            status = getattr(meta, "status", status)
        return status == 409

    @staticmethod
    def _parse_sequence(value: Any) -> Optional[int]:
        if value is None:
            return None
        try:
            return int(value)
        except Exception:
            return None

    @staticmethod
    def _extract_envelope_metadata(event_data: Dict[str, Any]) -> Dict[str, Optional[str]]:
        metadata = event_data.get("metadata")
        if not isinstance(metadata, dict):
            metadata = {}
        command_id = metadata.get("command_id")
        trace_id = metadata.get("trace_id")
        correlation_id = metadata.get("correlation_id")
        service = metadata.get("service")
        return {
            "command_id": str(command_id) if command_id else None,
            "trace_id": str(trace_id) if trace_id else None,
            "correlation_id": str(correlation_id) if correlation_id else None,
            "origin_service": str(service) if service else None,
        }

    async def _record_es_side_effect(
        self,
        *,
        event_id: str,
        event_data: Dict[str, Any],
        db_name: str,
        index_name: str,
        doc_id: str,
        operation: str,
        status: str,
        record_lineage: bool,
        skip_reason: Optional[str] = None,
        error: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Record projection side-effects for provenance (lineage) + audit.

        - Lineage: domain event -> ES artifact (only when record_lineage=True)
        - Audit: structured log (success/failure, skip_reason, ids)
        """
        occurred_at = datetime.now(timezone.utc)
        meta = self._extract_envelope_metadata(event_data)
        seq = self._parse_sequence(event_data.get("sequence_number"))

        if self.audit_store:
            try:
                action = "PROJECTION_ES_INDEX" if operation == "index" else "PROJECTION_ES_DELETE"
                audit_metadata = {
                    "db_name": db_name,
                    "index": index_name,
                    "doc_id": doc_id,
                    "operation": operation,
                    "event_type": event_data.get("event_type"),
                    "aggregate_id": event_data.get("aggregate_id"),
                    "sequence_number": seq,
                    "skipped": bool(skip_reason),
                    "skip_reason": skip_reason,
                    "origin_service": meta.get("origin_service"),
                    "run_id": os.getenv("PIPELINE_RUN_ID") or os.getenv("RUN_ID") or os.getenv("EXECUTION_ID"),
                    "code_sha": os.getenv("CODE_SHA") or os.getenv("GIT_SHA") or os.getenv("COMMIT_SHA"),
                }
                if isinstance(extra_metadata, dict) and extra_metadata:
                    audit_metadata.update(extra_metadata)
                await self.audit_store.log(
                    partition_key=f"db:{db_name}",
                    actor="projection_worker",
                    action=action,
                    status=status,
                    resource_type="es_document",
                    resource_id=f"{index_name}/{doc_id}",
                    event_id=str(event_id) if event_id else None,
                    command_id=meta.get("command_id"),
                    trace_id=meta.get("trace_id"),
                    correlation_id=meta.get("correlation_id"),
                    metadata=audit_metadata,
                    error=error,
                    occurred_at=occurred_at,
                )
            except Exception as e:
                logger.debug(f"Audit record failed (non-fatal): {e}")

        if self.lineage_store and record_lineage:
            try:
                edge_type = "event_deleted_es_document" if operation == "delete" else "event_materialized_es_document"
                await self.lineage_store.record_link(
                    from_node_id=self.lineage_store.node_event(str(event_id)),
                    to_node_id=self.lineage_store.node_artifact("es", index_name, doc_id),
                    edge_type=edge_type,
                    occurred_at=occurred_at,
                    to_label=f"es:{index_name}/{doc_id}",
                    edge_metadata={
                        "db_name": db_name,
                        "index": index_name,
                        "doc_id": doc_id,
                        "operation": operation,
                        "sequence_number": seq,
                    },
                )
            except Exception as e:
                logger.debug(f"Lineage record failed (non-fatal): {e}")

    async def _heartbeat_loop(self, *, handler: str, event_id: str) -> None:
        if not self.processed_event_registry:
            return
        interval = int(os.getenv("PROCESSED_EVENT_HEARTBEAT_INTERVAL_SECONDS", "30"))
        while True:
            await asyncio.sleep(interval)
            ok = await self.processed_event_registry.heartbeat(handler=handler, event_id=event_id)
            if not ok:
                return
        
    async def initialize(self):
        """ÏõåÏª§ Ï¥àÍ∏∞Ìôî"""
        # Kafka Consumer ÏÑ§Ï†ï (Î©ÄÌã∞ ÌÜ†ÌîΩ Íµ¨ÎèÖ)
        self.consumer = Consumer({
            'bootstrap.servers': self.kafka_servers,
            'group.id': 'projection-worker-group',
            'auto.offset.reset': 'earliest',
            'enable.auto.commit': False,
            'max.poll.interval.ms': 300000,  # 5Î∂Ñ
            'session.timeout.ms': 45000,  # 45Ï¥à
        })
        
        # Kafka Producer ÏÑ§Ï†ï (Ïã§Ìå® Ïù¥Î≤§Ìä∏ Î∞úÌñâÏö©)
        self.producer = Producer({
            'bootstrap.servers': self.kafka_servers,
            'client.id': 'projection-worker',
            'acks': 'all',
            'retries': 3,
            'compression.type': 'snappy',
        })
        
        # Redis Ïó∞Í≤∞ ÏÑ§Ï†ï (Ïò®ÌÜ®Î°úÏßÄ Ï∫êÏã±Ïö©)
        settings = ApplicationSettings()
        self.redis_service = create_redis_service(settings)
        await self.redis_service.connect()
        logger.info("Redis connection established")
        
        # Elasticsearch Ïó∞Í≤∞ ÏÑ§Ï†ï
        self.elasticsearch_service = create_elasticsearch_service(settings)
        await self.elasticsearch_service.connect()
        logger.info("Elasticsearch connection established")

        # Durable processed-events registry (idempotency + ordering guard)
        if self.enable_processed_event_registry:
            self.processed_event_registry = ProcessedEventRegistry()
            await self.processed_event_registry.connect()
            logger.info("‚úÖ ProcessedEventRegistry connected (Postgres)")
        else:
            logger.warning("‚ö†Ô∏è ProcessedEventRegistry disabled (duplicates may re-apply side-effects)")

        # First-class lineage/audit (best-effort; do not fail the worker)
        if self.enable_lineage:
            try:
                self.lineage_store = LineageStore()
                await self.lineage_store.initialize()
                logger.info("‚úÖ LineageStore connected (Postgres)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è LineageStore unavailable (continuing without lineage): {e}")
                self.lineage_store = None

        if self.enable_audit_logs:
            try:
                self.audit_store = AuditLogStore()
                await self.audit_store.initialize()
                logger.info("‚úÖ AuditLogStore connected (Postgres)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è AuditLogStore unavailable (continuing without audit logs): {e}")
                self.audit_store = None
        
        # Ïù∏Îç±Ïä§ ÏÉùÏÑ± Î∞è Îß§Ìïë ÏÑ§Ï†ï
        await self._setup_indices()
        
        # ÌÜ†ÌîΩ Íµ¨ÎèÖ
        topics = [AppConfig.INSTANCE_EVENTS_TOPIC, AppConfig.ONTOLOGY_EVENTS_TOPIC]
        self.consumer.subscribe(topics)
        logger.info(f"Subscribed to topics: {topics}")
        
        # Initialize OpenTelemetry
        self.tracing_service = get_tracing_service("projection-worker")
        self.metrics_collector = get_metrics_collector("projection-worker")
        
        # üéØ Initialize ProjectionManager for materialized views
        try:
            # ProjectionManagerÎäî GraphFederationServiceWOQLÏù¥ ÌïÑÏöîÌïòÎØÄÎ°ú
            # Ïã§Ï†ú ÌîÑÎ°úÎçïÏÖòÏóêÏÑúÎäî Î≥ÑÎèÑÎ°ú Ï¥àÍ∏∞ÌôîÌïòÎèÑÎ°ù ÏÑ§Í≥ÑÎê®
            # Ïó¨Í∏∞ÏÑúÎäî Ïä§ÏºàÎ†àÌÜ§Îßå Ï§ÄÎπÑ
            logger.info("üéØ ProjectionManager ready for initialization when graph service is available")
            # TODO: Initialize ProjectionManager when GraphFederationServiceWOQL is available
            # self.projection_manager = ProjectionManager(
            #     graph_service=graph_service,
            #     es_service=self.elasticsearch_service,
            #     redis_service=self.redis_service
            # )
        except Exception as e:
            logger.warning(f"ProjectionManager initialization skipped: {e}")
        
    async def _setup_indices(self):
        """Îß§Ìïë ÌååÏùº Î°úÎìú (Ïù∏Îç±Ïä§Îäî DBÎ≥ÑÎ°ú ÎèôÏ†Å ÏÉùÏÑ±)"""
        try:
            # Îß§Ìïë ÌååÏùºÎßå ÎØ∏Î¶¨ Î°úÎìú
            self.instances_mapping = await self._load_mapping('instances_mapping.json')
            self.ontologies_mapping = await self._load_mapping('ontologies_mapping.json')
            logger.info("Loaded index mappings successfully")
                
        except Exception as e:
            logger.error(f"Failed to load mappings: {e}")
            raise
            
    async def _ensure_index_exists(self, db_name: str, index_type: str = "instances"):
        """ÌäπÏ†ï Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïùò Ïù∏Îç±Ïä§Í∞Ä Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥† ÏóÜÏúºÎ©¥ ÏÉùÏÑ±"""
        if index_type == "instances":
            index_name = get_instances_index_name(db_name)
            mapping = self.instances_mapping
        else:
            index_name = get_ontologies_index_name(db_name)
            mapping = self.ontologies_mapping
            
        # Ïù¥ÎØ∏ ÏÉùÏÑ±Îêú Ïù∏Îç±Ïä§Îäî Ïä§ÌÇµ
        if index_name in self.created_indices:
            return index_name
            
        try:
            if not await self.elasticsearch_service.index_exists(index_name):
                # ÏÑ§Ï†ï Î≥ëÌï© (Îß§Ìïë ÌååÏùº ÏÑ§Ï†ï + Í∏∞Î≥∏ ÏÑ§Ï†ï)
                settings = mapping.get('settings', {}).copy()
                settings.update(DEFAULT_INDEX_SETTINGS)
                
                await self.elasticsearch_service.create_index(
                    index_name,
                    mappings=mapping['mappings'],
                    settings=settings
                )
                logger.info(f"Created index: {index_name} for database: {db_name}")
                
            self.created_indices.add(index_name)
            return index_name
            
        except Exception as e:
            logger.error(f"Failed to ensure index exists for {db_name}: {e}")
            raise
            
    async def _load_mapping(self, filename: str) -> Dict[str, Any]:
        """Îß§Ìïë ÌååÏùº Î°úÎìú"""
        mapping_path = os.path.join(
            os.path.dirname(__file__), 
            "mappings", 
            filename
        )
        try:
            with open(mapping_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load mapping {filename}: {e}")
            raise
            
    async def run(self):
        """Î©îÏù∏ Ïã§Ìñâ Î£®ÌîÑ"""
        self.running = True
        logger.info("Projection Worker started")
        
        try:
            while self.running:
                msg = self.consumer.poll(timeout=1.0)
                if msg is None:
                    continue
                    
                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        continue
                    else:
                        logger.error(f"Kafka error: {msg.error()}")
                        continue
                        
                try:
                    # Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨
                    await self._process_event(msg)
                    # ÏÑ±Í≥µ Ïãú Ïò§ÌîÑÏÖã Ïª§Î∞ã
                    maybe_crash("projection_worker:before_commit", logger=logger)
                    self.consumer.commit(msg)
                    # Clear retry state for this offset
                    key = f"{msg.topic()}:{msg.partition()}:{msg.offset()}"
                    self.retry_count.pop(key, None)
                    
                except Exception as e:
                    logger.error(f"Failed to process event: {e}")
                    # Ïû¨ÏãúÎèÑ Î°úÏßÅ
                    await self._handle_retry(msg, e)
                    
        except KeyboardInterrupt:
            logger.info("Received keyboard interrupt")
        except Exception as e:
            logger.error(f"Unexpected error in main loop: {e}")
        finally:
            await self._shutdown()
            
    async def _process_event(self, msg):
        """Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            registry_event_id = None
            registry_aggregate_id = None
            registry_sequence = None
            registry_claimed = False

            try:
                envelope = EventEnvelope.model_validate_json(msg.value())
            except Exception as e:
                raise ValueError(f"Invalid EventEnvelope JSON: {e}") from e

            kind = envelope.metadata.get("kind") if isinstance(envelope.metadata, dict) else None
            if kind != "domain":
                raise ValueError(f"Unexpected envelope kind for projection topic {msg.topic()}: {kind}")

            event_data = envelope.model_dump(mode="json")
            event_type = envelope.event_type
            topic = msg.topic()
            
            logger.info(f"Processing event: {event_type} from topic: {topic}")
            
            # Durable idempotency + ordering guard (Postgres)
            registry_event_id = envelope.event_id
            registry_aggregate_id = envelope.aggregate_id
            registry_sequence = envelope.sequence_number
            handler = f"projection_worker:{topic}"

            if self.processed_event_registry and registry_event_id:
                claim = await self.processed_event_registry.claim(
                    handler=handler,
                    event_id=str(registry_event_id),
                    aggregate_id=str(registry_aggregate_id) if registry_aggregate_id else None,
                    sequence_number=int(registry_sequence) if registry_sequence is not None else None,
                )
                if claim.decision in {ClaimDecision.DUPLICATE_DONE, ClaimDecision.STALE}:
                    logger.info(
                        f"Skipping {claim.decision.value} event_id={registry_event_id} "
                        f"(aggregate_id={registry_aggregate_id}, seq={registry_sequence})"
                    )
                    return
                if claim.decision == ClaimDecision.IN_PROGRESS:
                    raise _InProgressLeaseError(
                        f"Event {registry_event_id} is already in progress elsewhere (lease not expired)"
                    )
                registry_claimed = True
                maybe_crash("projection_worker:after_claim", logger=logger)

            heartbeat_task = None
            if registry_claimed and self.processed_event_registry and registry_event_id:
                heartbeat_task = asyncio.create_task(
                    self._heartbeat_loop(handler=handler, event_id=str(registry_event_id))
                )
            
            try:
                maybe_crash("projection_worker:before_side_effect", logger=logger)
                if topic == AppConfig.INSTANCE_EVENTS_TOPIC:
                    await self._handle_instance_event(event_data)
                elif topic == AppConfig.ONTOLOGY_EVENTS_TOPIC:
                    await self._handle_ontology_event(event_data)
                else:
                    logger.warning(f"Unknown topic: {topic}")
                maybe_crash("projection_worker:after_side_effect", logger=logger)

                if registry_claimed and self.processed_event_registry and registry_event_id:
                    maybe_crash("projection_worker:before_mark_done", logger=logger)
                    await self.processed_event_registry.mark_done(
                        handler=handler,
                        event_id=str(registry_event_id),
                        aggregate_id=str(registry_aggregate_id) if registry_aggregate_id else None,
                        sequence_number=int(registry_sequence) if registry_sequence is not None else None,
                    )
            finally:
                if heartbeat_task:
                    heartbeat_task.cancel()
                    with suppress(asyncio.CancelledError):
                        await heartbeat_task
                
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse event JSON: {e}")
            raise
        except Exception as e:
            logger.error(f"Error processing event: {e}")
            if (
                registry_claimed
                and self.processed_event_registry
                and registry_event_id
                and "handler" in locals()
            ):
                try:
                    await self.processed_event_registry.mark_failed(
                        handler=handler,
                        event_id=str(registry_event_id),
                        error=str(e),
                    )
                except Exception as reg_err:
                    logger.warning(f"Failed to mark event failed in registry: {reg_err}")
            raise
            
    async def _handle_instance_event(self, event_data: Dict[str, Any]):
        """Ïù∏Ïä§ÌÑ¥Ïä§ Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            event_type = event_data.get('event_type')
            instance_data = event_data.get('data', {})
            event_id = event_data.get('event_id')
            
            if event_type == EventType.INSTANCE_CREATED.value:
                await self._handle_instance_created(instance_data, event_id, event_data)
            elif event_type == EventType.INSTANCE_UPDATED.value:
                await self._handle_instance_updated(instance_data, event_id, event_data)
            elif event_type == EventType.INSTANCE_DELETED.value:
                await self._handle_instance_deleted(instance_data, event_id, event_data)
            else:
                logger.warning(f"Unknown instance event type: {event_type}")
                
        except Exception as e:
            logger.error(f"Error handling instance event: {e}")
            raise
            
    async def _handle_ontology_event(self, event_data: Dict[str, Any]):
        """Ïò®ÌÜ®Î°úÏßÄ Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            event_type = event_data.get('event_type')
            ontology_data = event_data.get('data', {})
            event_id = event_data.get('event_id')
            
            if event_type == EventType.ONTOLOGY_CLASS_CREATED.value:
                await self._handle_ontology_class_created(ontology_data, event_id, event_data)
            elif event_type == EventType.ONTOLOGY_CLASS_UPDATED.value:
                await self._handle_ontology_class_updated(ontology_data, event_id, event_data)
            elif event_type == EventType.ONTOLOGY_CLASS_DELETED.value:
                await self._handle_ontology_class_deleted(ontology_data, event_id, event_data)
            elif event_type == EventType.DATABASE_CREATED.value:
                await self._handle_database_created(ontology_data, event_id, event_data)
            elif event_type == EventType.DATABASE_DELETED.value:
                await self._handle_database_deleted(ontology_data, event_id, event_data)
            else:
                logger.warning(f"Unknown ontology event type: {event_type}")
                
        except Exception as e:
            logger.error(f"Error handling ontology event: {e}")
            raise
            
    async def _handle_instance_created(self, instance_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïù¥Î¶Ñ Ï∂îÏ∂ú
            db_name = event_data.get('db_name') or instance_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for instance creation")
                
            # Ïù∏Îç±Ïä§ ÌôïÏù∏ Î∞è ÏÉùÏÑ±
            index_name = await self._ensure_index_exists(db_name, "instances")
            
            # ÌÅ¥ÎûòÏä§ ÎùºÎ≤® Ï°∞Ìöå (Redis Ï∫êÏãú ÌôúÏö©)
            class_label = await self._get_class_label(instance_data.get('class_id'), db_name)
            
            # Elasticsearch Î¨∏ÏÑú Íµ¨ÏÑ±
            instance_id = instance_data.get('instance_id')
            if not instance_id:
                raise ValueError("instance_id is required for instance creation")

            incoming_seq = self._parse_sequence(event_data.get("sequence_number"))
            if incoming_seq is None:
                existing_doc = await self.elasticsearch_service.get_document(index_name, instance_id)
                if existing_doc:
                    if existing_doc.get("event_id") == event_id:
                        logger.info(
                            f"Skipping duplicate instance create event (event_id={event_id}, instance_id={instance_id})"
                        )
                        await self._record_es_side_effect(
                            event_id=str(event_id),
                            event_data=event_data,
                            db_name=db_name,
                            index_name=index_name,
                            doc_id=str(instance_id),
                            operation="index",
                            status="success",
                            record_lineage=True,
                            skip_reason="duplicate",
                        )
                        return
                    logger.info(
                        f"Instance already exists; skipping create without sequence_number (instance_id={instance_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(instance_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="already_exists_no_sequence",
                    )
                    return

            doc = {
                'instance_id': instance_id,
                'class_id': instance_data.get('class_id'),
                'class_label': class_label,
                'properties': self._normalize_properties(instance_data.get('properties', [])),
                'data': instance_data,  # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ (enabled: false)
                'event_id': event_id,
                'event_sequence': incoming_seq,
                'event_timestamp': event_data.get('occurred_at') or event_data.get('timestamp'),
                'version': int(incoming_seq) if incoming_seq is not None else 1,
                'db_name': db_name,
                'branch': instance_data.get('branch'),
                'created_at': datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat()
            }
            
            # instance_idÎ•º Î¨∏ÏÑú IDÎ°ú ÏÇ¨Ïö© (ÏóÖÎç∞Ïù¥Ìä∏/ÏÇ≠Ï†ú Ï†ïÌï©ÏÑ±)
            try:
                await self.elasticsearch_service.index_document(
                    index_name,
                    doc,
                    doc_id=instance_id,
                    refresh=True,
                    version=incoming_seq,
                    version_type="external_gte" if incoming_seq is not None else None,
                    op_type="create" if incoming_seq is None else None,
                )
            except Exception as e:
                if incoming_seq is None and self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping instance create due to ES create conflict "
                        f"(instance_id={instance_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(instance_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="es_create_conflict",
                    )
                    return
                if incoming_seq is not None and self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping stale instance create event via ES version conflict "
                        f"(seq={incoming_seq}, instance_id={instance_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(instance_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="stale_version_conflict",
                    )
                    return
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(instance_id),
                    operation="index",
                    status="failure",
                    record_lineage=False,
                    error=str(e),
                )
                raise
            
            logger.info(f"Instance created in Elasticsearch: {instance_id} in index: {index_name}")
            await self._record_es_side_effect(
                event_id=str(event_id),
                event_data=event_data,
                db_name=db_name,
                index_name=index_name,
                doc_id=str(instance_id),
                operation="index",
                status="success",
                record_lineage=True,
            )
            
        except Exception as e:
            logger.error(f"Failed to handle instance created: {e}")
            raise
            
    async def _handle_instance_updated(self, instance_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """Ïù∏Ïä§ÌÑ¥Ïä§ ÏóÖÎç∞Ïù¥Ìä∏ Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïù¥Î¶Ñ Ï∂îÏ∂ú
            db_name = event_data.get('db_name') or instance_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for instance update")
                
            # Ïù∏Îç±Ïä§ ÌôïÏù∏ Î∞è ÏÉùÏÑ±
            index_name = await self._ensure_index_exists(db_name, "instances")
            
            # ÌÅ¥ÎûòÏä§ ÎùºÎ≤® Ï°∞Ìöå
            class_label = await self._get_class_label(instance_data.get('class_id'), db_name)
            
            # Í∏∞Ï°¥ Î¨∏ÏÑú Ï°∞Ìöå
            instance_id = instance_data.get('instance_id')
            if not instance_id:
                raise ValueError("instance_id is required for instance update")

            existing_doc = await self.elasticsearch_service.get_document(
                index_name,
                instance_id
            )

            incoming_seq = self._parse_sequence(event_data.get("sequence_number"))
            if existing_doc:
                if existing_doc.get("event_id") == event_id:
                    logger.info(
                        f"Skipping duplicate instance update event (event_id={event_id}, instance_id={instance_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(instance_id),
                        operation="index",
                        status="success",
                        record_lineage=True,
                        skip_reason="duplicate",
                    )
                    return

            if incoming_seq is None and existing_doc:
                logger.warning(
                    f"Refusing to update instance without sequence_number (instance_id={instance_id})"
                )
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(instance_id),
                    operation="index",
                    status="success",
                    record_lineage=False,
                    skip_reason="missing_sequence_number",
                )
                return
            
            version = 1
            if incoming_seq is not None:
                version = int(incoming_seq)

            created_at = None
            if existing_doc:
                created_at = existing_doc.get("created_at")
            
            # ÏóÖÎç∞Ïù¥Ìä∏ Î¨∏ÏÑú Íµ¨ÏÑ±
            doc = {
                'instance_id': instance_id,
                'class_id': instance_data.get('class_id'),
                'class_label': class_label,
                'properties': self._normalize_properties(instance_data.get('properties', [])),
                'data': instance_data,
                'event_id': event_id,
                'event_sequence': incoming_seq,
                'event_timestamp': event_data.get('occurred_at') or event_data.get('timestamp'),
                'version': version,
                'db_name': db_name,
                'branch': instance_data.get('branch'),
                'created_at': created_at or datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat()
            }
            
            try:
                if incoming_seq is not None:
                    await self.elasticsearch_service.index_document(
                        index_name,
                        doc,
                        doc_id=instance_id,
                        refresh=True,
                        version=incoming_seq,
                        version_type="external_gte",
                    )
                else:
                    await self.elasticsearch_service.index_document(
                        index_name,
                        doc,
                        doc_id=instance_id,
                        refresh=True,
                        op_type="create",
                    )
            except Exception as e:
                if incoming_seq is not None and self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping stale instance update event via ES version conflict "
                        f"(seq={incoming_seq}, instance_id={instance_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(instance_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="stale_version_conflict",
                    )
                    return
                if incoming_seq is None and self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping instance update create due to ES conflict "
                        f"(instance_id={instance_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(instance_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="es_create_conflict",
                    )
                    return
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(instance_id),
                    operation="index",
                    status="failure",
                    record_lineage=False,
                    error=str(e),
                )
                raise
            
            logger.info(f"Instance updated in Elasticsearch: {instance_id} in index: {index_name}")
            await self._record_es_side_effect(
                event_id=str(event_id),
                event_data=event_data,
                db_name=db_name,
                index_name=index_name,
                doc_id=str(instance_id),
                operation="index",
                status="success",
                record_lineage=True,
            )
            
        except Exception as e:
            logger.error(f"Failed to handle instance updated: {e}")
            raise
            
    async def _handle_instance_deleted(self, instance_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """Ïù∏Ïä§ÌÑ¥Ïä§ ÏÇ≠Ï†ú Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïù¥Î¶Ñ Ï∂îÏ∂ú
            db_name = event_data.get('db_name') or instance_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for instance deletion")
                
            # Ïù∏Îç±Ïä§ Ïù¥Î¶Ñ Í≤∞Ï†ï
            index_name = get_instances_index_name(db_name)
            instance_id = instance_data.get('instance_id')
            if not instance_id:
                raise ValueError("instance_id is required for instance deletion")

            incoming_seq = self._parse_sequence(event_data.get("sequence_number"))
            if incoming_seq is None:
                existing_doc = await self.elasticsearch_service.get_document(index_name, instance_id)
                if not existing_doc:
                    logger.info(
                        f"Instance already deleted (instance_id={instance_id}); treating delete as idempotent success"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(instance_id),
                        operation="delete",
                        status="success",
                        record_lineage=False,
                        skip_reason="already_deleted_no_sequence",
                    )
                    return
                if existing_doc.get("event_id") == event_id:
                    logger.info(
                        f"Skipping duplicate instance delete event (event_id={event_id}, instance_id={instance_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(instance_id),
                        operation="delete",
                        status="success",
                        record_lineage=False,
                        skip_reason="duplicate",
                    )
                    return

                logger.warning(
                    f"Refusing to delete instance without sequence_number (instance_id={instance_id})"
                )
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(instance_id),
                    operation="delete",
                    status="success",
                    record_lineage=False,
                    skip_reason="missing_sequence_number",
                )
                return

            # Î¨∏ÏÑú ÏÇ≠Ï†ú (external version guard)
            try:
                success = await self.elasticsearch_service.delete_document(
                    index_name,
                    instance_id,
                    refresh=True,
                    version=incoming_seq,
                    version_type="external_gte",
                )
            except Exception as e:
                if self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping stale instance delete event via ES version conflict "
                        f"(seq={incoming_seq}, instance_id={instance_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(instance_id),
                        operation="delete",
                        status="success",
                        record_lineage=False,
                        skip_reason="stale_version_conflict",
                    )
                    return
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(instance_id),
                    operation="delete",
                    status="failure",
                    record_lineage=False,
                    error=str(e),
                )
                raise
            
            if success:
                logger.info(f"Instance deleted from Elasticsearch: {instance_id} from index: {index_name}")
            else:
                logger.warning(f"Instance not found for deletion: {instance_id} in index: {index_name}")
            await self._record_es_side_effect(
                event_id=str(event_id),
                event_data=event_data,
                db_name=db_name,
                index_name=index_name,
                doc_id=str(instance_id),
                operation="delete",
                status="success",
                record_lineage=True,
                extra_metadata={"deleted": bool(success)},
            )
                
        except Exception as e:
            logger.error(f"Failed to handle instance deleted: {e}")
            raise
            
    async def _handle_ontology_class_created(self, ontology_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """Ïò®ÌÜ®Î°úÏßÄ ÌÅ¥ÎûòÏä§ ÏÉùÏÑ± Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïù¥Î¶Ñ Ï∂îÏ∂ú
            db_name = event_data.get('db_name') or ontology_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for ontology class creation")
                
            # Ïù∏Îç±Ïä§ ÌôïÏù∏ Î∞è ÏÉùÏÑ±
            index_name = await self._ensure_index_exists(db_name, "ontologies")
            
            # Elasticsearch Î¨∏ÏÑú Íµ¨ÏÑ±
            class_id = ontology_data.get('class_id') or ontology_data.get('id')
            if not class_id:
                raise ValueError("class_id is required for ontology class creation")

            incoming_seq = self._parse_sequence(event_data.get("sequence_number"))
            if incoming_seq is None:
                existing_doc = await self.elasticsearch_service.get_document(index_name, class_id)
                if existing_doc:
                    if existing_doc.get("event_id") == event_id:
                        logger.info(
                            f"Skipping duplicate ontology create event (event_id={event_id}, class_id={class_id})"
                        )
                        await self._record_es_side_effect(
                            event_id=str(event_id),
                            event_data=event_data,
                            db_name=db_name,
                            index_name=index_name,
                            doc_id=str(class_id),
                            operation="index",
                            status="success",
                            record_lineage=True,
                            skip_reason="duplicate",
                        )
                        return
                    logger.info(
                        f"Ontology already exists; skipping create without sequence_number (class_id={class_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(class_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="already_exists_no_sequence",
                    )
                    return

            doc = {
                'class_id': class_id,
                'label': ontology_data.get('label'),
                'description': ontology_data.get('description'),
                'properties': ontology_data.get('properties', []),
                'relationships': ontology_data.get('relationships', []),
                'parent_classes': ontology_data.get('parent_classes', []),
                'child_classes': ontology_data.get('child_classes', []),
                'db_name': db_name,
                'branch': ontology_data.get('branch'),
                'version': int(incoming_seq) if incoming_seq is not None else 1,
                'event_id': event_id,
                'event_sequence': incoming_seq,
                'event_timestamp': event_data.get('occurred_at') or event_data.get('timestamp'),
                'created_at': datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat()
            }
            
            # Ïù∏Îç±Ïã± (external version guard)
            try:
                await self.elasticsearch_service.index_document(
                    index_name,
                    doc,
                    doc_id=class_id,
                    refresh=True,
                    version=incoming_seq,
                    version_type="external_gte" if incoming_seq is not None else None,
                    op_type="create" if incoming_seq is None else None,
                )
            except Exception as e:
                if incoming_seq is None and self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping ontology create due to ES create conflict "
                        f"(class_id={class_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(class_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="es_create_conflict",
                    )
                    return
                if incoming_seq is not None and self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping stale ontology create event via ES version conflict "
                        f"(seq={incoming_seq}, class_id={class_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(class_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="stale_version_conflict",
                    )
                    return
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(class_id),
                    operation="index",
                    status="failure",
                    record_lineage=False,
                    error=str(e),
                )
                raise
            
            # RedisÏóê ÌÅ¥ÎûòÏä§ ÎùºÎ≤® Ï∫êÏã± (DBÎ≥ÑÎ°ú ÌÇ§ Íµ¨Î∂Ñ)
            await self._cache_class_label(
                class_id,
                ontology_data.get('label'),
                db_name
            )
            
            logger.info(f"Ontology class created in Elasticsearch: {class_id} in index: {index_name}")
            await self._record_es_side_effect(
                event_id=str(event_id),
                event_data=event_data,
                db_name=db_name,
                index_name=index_name,
                doc_id=str(class_id),
                operation="index",
                status="success",
                record_lineage=True,
            )
            
        except Exception as e:
            logger.error(f"Failed to handle ontology class created: {e}")
            raise
            
    async def _handle_ontology_class_updated(self, ontology_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """Ïò®ÌÜ®Î°úÏßÄ ÌÅ¥ÎûòÏä§ ÏóÖÎç∞Ïù¥Ìä∏ Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïù¥Î¶Ñ Ï∂îÏ∂ú
            db_name = event_data.get('db_name') or ontology_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for ontology class update")
                
            # Ïù∏Îç±Ïä§ ÌôïÏù∏ Î∞è ÏÉùÏÑ±
            index_name = await self._ensure_index_exists(db_name, "ontologies")

            class_id = ontology_data.get('class_id') or ontology_data.get('id')
            
            # Í∏∞Ï°¥ Î¨∏ÏÑú Ï°∞Ìöå
            existing_doc = await self.elasticsearch_service.get_document(
                index_name,
                class_id
            )

            incoming_seq = self._parse_sequence(event_data.get("sequence_number"))
            if existing_doc:
                if existing_doc.get("event_id") == event_id:
                    logger.info(f"Skipping duplicate ontology update event (event_id={event_id}, class_id={class_id})")
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(class_id),
                        operation="index",
                        status="success",
                        record_lineage=True,
                        skip_reason="duplicate",
                    )
                    return

            if incoming_seq is None and existing_doc:
                logger.warning(
                    f"Refusing to update ontology class without sequence_number (class_id={class_id})"
                )
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(class_id),
                    operation="index",
                    status="success",
                    record_lineage=False,
                    skip_reason="missing_sequence_number",
                )
                return
            
            version = 1
            if incoming_seq is not None:
                version = int(incoming_seq)

            created_at = None
            if existing_doc:
                created_at = existing_doc.get("created_at")
            
            # ÏóÖÎç∞Ïù¥Ìä∏ Î¨∏ÏÑú Íµ¨ÏÑ±
            doc = {
                'class_id': class_id,
                'label': ontology_data.get('label'),
                'description': ontology_data.get('description'),
                'properties': ontology_data.get('properties', []),
                'relationships': ontology_data.get('relationships', []),
                'parent_classes': ontology_data.get('parent_classes', []),
                'child_classes': ontology_data.get('child_classes', []),
                'db_name': db_name,
                'branch': ontology_data.get('branch'),
                'version': version,
                'event_id': event_id,
                'event_sequence': incoming_seq,
                'event_timestamp': event_data.get('occurred_at') or event_data.get('timestamp'),
                'created_at': created_at or datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat()
            }
            
            try:
                if incoming_seq is not None:
                    await self.elasticsearch_service.index_document(
                        index_name,
                        doc,
                        doc_id=class_id,
                        refresh=True,
                        version=incoming_seq,
                        version_type="external_gte",
                    )
                else:
                    await self.elasticsearch_service.index_document(
                        index_name,
                        doc,
                        doc_id=class_id,
                        refresh=True,
                        op_type="create",
                    )
            except Exception as e:
                if incoming_seq is not None and self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping stale ontology update event via ES version conflict "
                        f"(seq={incoming_seq}, class_id={class_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(class_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="stale_version_conflict",
                    )
                    return
                if incoming_seq is None and self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping ontology update create due to ES conflict "
                        f"(class_id={class_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(class_id),
                        operation="index",
                        status="success",
                        record_lineage=False,
                        skip_reason="es_create_conflict",
                    )
                    return
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(class_id),
                    operation="index",
                    status="failure",
                    record_lineage=False,
                    error=str(e),
                )
                raise
            
            # Redis Ï∫êÏãú ÏóÖÎç∞Ïù¥Ìä∏ (DBÎ≥ÑÎ°ú ÌÇ§ Íµ¨Î∂Ñ)
            await self._cache_class_label(
                class_id,
                ontology_data.get('label'),
                db_name
            )
            
            logger.info(f"Ontology class updated in Elasticsearch: {class_id} in index: {index_name}")
            await self._record_es_side_effect(
                event_id=str(event_id),
                event_data=event_data,
                db_name=db_name,
                index_name=index_name,
                doc_id=str(class_id),
                operation="index",
                status="success",
                record_lineage=True,
            )
            
        except Exception as e:
            logger.error(f"Failed to handle ontology class updated: {e}")
            raise
            
    async def _handle_ontology_class_deleted(self, ontology_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """Ïò®ÌÜ®Î°úÏßÄ ÌÅ¥ÎûòÏä§ ÏÇ≠Ï†ú Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïù¥Î¶Ñ Ï∂îÏ∂ú
            db_name = event_data.get('db_name') or ontology_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for ontology class deletion")
                
            # Ïù∏Îç±Ïä§ Ïù¥Î¶Ñ Í≤∞Ï†ï
            index_name = get_ontologies_index_name(db_name)
            class_id = ontology_data.get('class_id') or ontology_data.get('id')
            if not class_id:
                raise ValueError("class_id is required for ontology class deletion")

            incoming_seq = self._parse_sequence(event_data.get("sequence_number"))
            if incoming_seq is None:
                existing_doc = await self.elasticsearch_service.get_document(index_name, class_id)
                if not existing_doc:
                    logger.info(
                        f"Ontology class already deleted (class_id={class_id}); treating delete as idempotent success"
                    )
                    await self.redis_service.delete(AppConfig.get_class_label_key(db_name, class_id))
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(class_id),
                        operation="delete",
                        status="success",
                        record_lineage=False,
                        skip_reason="already_deleted_no_sequence",
                    )
                    return
                if existing_doc.get("event_id") == event_id:
                    logger.info(
                        f"Skipping duplicate ontology delete event (event_id={event_id}, class_id={class_id})"
                    )
                    await self.redis_service.delete(AppConfig.get_class_label_key(db_name, class_id))
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(class_id),
                        operation="delete",
                        status="success",
                        record_lineage=False,
                        skip_reason="duplicate",
                    )
                    return

                logger.warning(
                    f"Refusing to delete ontology class without sequence_number (class_id={class_id})"
                )
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(class_id),
                    operation="delete",
                    status="success",
                    record_lineage=False,
                    skip_reason="missing_sequence_number",
                )
                return

            # Î¨∏ÏÑú ÏÇ≠Ï†ú (external version guard)
            try:
                success = await self.elasticsearch_service.delete_document(
                    index_name,
                    class_id,
                    refresh=True,
                    version=incoming_seq,
                    version_type="external_gte",
                )
            except Exception as e:
                if self._is_es_version_conflict(e):
                    logger.info(
                        f"Skipping stale ontology delete event via ES version conflict "
                        f"(seq={incoming_seq}, class_id={class_id})"
                    )
                    await self._record_es_side_effect(
                        event_id=str(event_id),
                        event_data=event_data,
                        db_name=db_name,
                        index_name=index_name,
                        doc_id=str(class_id),
                        operation="delete",
                        status="success",
                        record_lineage=False,
                        skip_reason="stale_version_conflict",
                    )
                    return
                await self._record_es_side_effect(
                    event_id=str(event_id),
                    event_data=event_data,
                    db_name=db_name,
                    index_name=index_name,
                    doc_id=str(class_id),
                    operation="delete",
                    status="failure",
                    record_lineage=False,
                    error=str(e),
                )
                raise
            
            # Redis Ï∫êÏãú ÏÇ≠Ï†ú (DBÎ≥ÑÎ°ú ÌÇ§ Íµ¨Î∂Ñ)
            await self.redis_service.delete(AppConfig.get_class_label_key(db_name, class_id))
            
            if success:
                logger.info(f"Ontology class deleted from Elasticsearch: {class_id} from index: {index_name}")
            else:
                logger.warning(f"Ontology class not found for deletion: {class_id} in index: {index_name}")
            await self._record_es_side_effect(
                event_id=str(event_id),
                event_data=event_data,
                db_name=db_name,
                index_name=index_name,
                doc_id=str(class_id),
                operation="delete",
                status="success",
                record_lineage=True,
                extra_metadata={"deleted": bool(success)},
            )
                
        except Exception as e:
            logger.error(f"Failed to handle ontology class deleted: {e}")
            raise
            
    async def _handle_database_created(self, db_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉùÏÑ± Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            db_name = db_data.get('db_name') or event_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for database creation")
                
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉùÏÑ± Ïãú Í∏∞Î≥∏ Ïù∏Îç±Ïä§Îì§ÏùÑ ÎØ∏Î¶¨ Ï§ÄÎπÑ
            logger.info(f"Database created: {db_name}, preparing Elasticsearch indices")
            
            # Ïù∏Ïä§ÌÑ¥Ïä§ÏôÄ Ïò®ÌÜ®Î°úÏßÄ Ïù∏Îç±Ïä§Î•º ÎØ∏Î¶¨ ÏÉùÏÑ±
            await self._ensure_index_exists(db_name, "instances")
            await self._ensure_index_exists(db_name, "ontologies")
            
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î¨∏ÏÑú ÏÉùÏÑ± (Í≤ÄÏÉâ Í∞ÄÎä•Ìïú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™©Î°ù Í¥ÄÎ¶¨)
            metadata_index = "spice_database_metadata"
            metadata_doc = {
                'database_name': db_name,
                'description': db_data.get('description', ''),
                'created_at': datetime.now(timezone.utc).isoformat(),
                'created_by': event_data.get('occurred_by', 'system'),
                'event_id': event_id,
                'status': 'active'
            }
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ïù∏Îç±Ïä§ ÌôïÏù∏ Î∞è ÏÉùÏÑ±
            if not await self.elasticsearch_service.index_exists(metadata_index):
                await self.elasticsearch_service.create_index(
                    metadata_index,
                    mappings={
                        "properties": {
                            "database_name": {"type": "keyword"},
                            "description": {"type": "text"},
                            "created_at": {"type": "date"},
                            "created_by": {"type": "keyword"},
                            "event_id": {"type": "keyword"},
                            "status": {"type": "keyword"}
                        }
                    },
                    settings=DEFAULT_INDEX_SETTINGS
                )
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î¨∏ÏÑú Ïù∏Îç±Ïã±
            await self.elasticsearch_service.index_document(
                metadata_index,
                metadata_doc,
                doc_id=db_name,
                refresh=True
            )
            
            logger.info(f"Database creation processed: {db_name}, indices prepared and metadata indexed")
            
        except Exception as e:
            logger.error(f"Failed to handle database created: {e}")
            raise
            
    async def _handle_database_deleted(self, db_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÇ≠Ï†ú Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨"""
        try:
            db_name = db_data.get('db_name') or event_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for database deletion")
                
            logger.info(f"Database deleted: {db_name}, cleaning up Elasticsearch indices")
            
            # Í¥ÄÎ†® Ïù∏Îç±Ïä§Îì§ ÏÇ≠Ï†ú
            instances_index = get_instances_index_name(db_name)
            ontologies_index = get_ontologies_index_name(db_name)
            
            # Ïù∏Îç±Ïä§ ÏÇ≠Ï†ú (Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞ÏóêÎßå)
            if await self.elasticsearch_service.index_exists(instances_index):
                await self.elasticsearch_service.delete_index(instances_index)
                logger.info(f"Deleted instances index: {instances_index}")
                
            if await self.elasticsearch_service.index_exists(ontologies_index):
                await self.elasticsearch_service.delete_index(ontologies_index)
                logger.info(f"Deleted ontologies index: {ontologies_index}")
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ (ÏôÑÏ†Ñ ÏÇ≠Ï†ú ÎåÄÏã† ÎπÑÌôúÏÑ±Ìôî)
            metadata_index = "spice_database_metadata"
            if await self.elasticsearch_service.index_exists(metadata_index):
                await self.elasticsearch_service.update_document(
                    metadata_index,
                    db_name,
                    doc={
                        'status': 'deleted',
                        'deleted_at': datetime.now(timezone.utc).isoformat(),
                        'deleted_by': event_data.get('occurred_by', 'system'),
                        'deletion_event_id': event_id
                    },
                    refresh=True
                )
            
            # ÏÉùÏÑ±Îêú Ïù∏Îç±Ïä§ Ï∫êÏãúÏóêÏÑú Ï†úÍ±∞
            self.created_indices.discard(instances_index)
            self.created_indices.discard(ontologies_index)
            
            logger.info(f"Database deletion processed: {db_name}, indices cleaned up and metadata updated")
            
        except Exception as e:
            logger.error(f"Failed to handle database deleted: {e}")
            raise
            
    async def _get_class_label(self, class_id: str, db_name: str) -> Optional[str]:
        """
        RedisÏóêÏÑú ÌÅ¥ÎûòÏä§ ÎùºÎ≤® Ï°∞Ìöå (Cache Stampede Î∞©ÏßÄ)
        
        Î∂ÑÏÇ∞ ÎùΩÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎèôÏãúÏóê Ïó¨Îü¨ ÏöîÏ≤≠Ïù¥ Îì§Ïñ¥ÏôÄÎèÑ 
        ElasticsearchÏóêÎäî Ìïú Î≤àÎßå ÏöîÏ≤≠ÌïòÎèÑÎ°ù ÏµúÏ†ÅÌôîÌï©ÎãàÎã§.
        """
        try:
            if not class_id or not db_name:
                return None
                
            cache_key = AppConfig.get_class_label_key(db_name, class_id)
            lock_key = f"lock:{cache_key}"
            
            # Ï∫êÏãú stampede Î∞©ÏßÄÎ•º ÏúÑÌïú Î∂ÑÏÇ∞ ÎùΩ Î©îÏª§ÎãàÏ¶ò
            max_wait_time = 5.0  # ÏµúÎåÄ 5Ï¥à ÎåÄÍ∏∞
            lock_timeout = 10    # ÎùΩ ÌÉÄÏûÑÏïÑÏõÉ 10Ï¥à
            retry_interval = 0.05  # 50ms Í∞ÑÍ≤©ÏúºÎ°ú Ïû¨ÏãúÎèÑ
            
            start_time = asyncio.get_event_loop().time()
            
            while (asyncio.get_event_loop().time() - start_time) < max_wait_time:
                # 1. Ï∫êÏãúÏóêÏÑú Ï°∞Ìöå ÏãúÎèÑ
                cached_label = await self.redis_service.client.get(cache_key)
                if cached_label:
                    # Negative caching Ï≤òÎ¶¨
                    if cached_label == "__NONE__":
                        self.cache_metrics['negative_cache_hits'] += 1
                        return None
                    self.cache_metrics['cache_hits'] += 1
                    return cached_label
                
                # 2. Î∂ÑÏÇ∞ ÎùΩ ÌöçÎìù ÏãúÎèÑ (SETNX with TTL)
                lock_acquired = await self.redis_service.client.set(
                    lock_key, 
                    "1", 
                    ex=lock_timeout,  # TTL ÏÑ§Ï†ïÏúºÎ°ú Îç∞ÎìúÎùΩ Î∞©ÏßÄ
                    nx=True  # SET if Not eXists
                )
                
                if lock_acquired:
                    # 3. ÎùΩÏùÑ ÌöçÎìùÌïú ÏöîÏ≤≠Îßå ElasticsearchÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå
                    self.cache_metrics['lock_acquisitions'] += 1
                    try:
                        # ÎùΩ ÌöçÎìù ÌõÑ Îã§Ïãú ÌïúÎ≤à Ï∫êÏãú ÌôïÏù∏ (Îã§Î•∏ ÏöîÏ≤≠Ïù¥ Ïù¥ÎØ∏ Ï†ÄÏû•ÌñàÏùÑ Ïàò ÏûàÏùå)
                        cached_label = await self.redis_service.client.get(cache_key)
                        if cached_label:
                            # Negative caching Ï≤òÎ¶¨
                            if cached_label == "__NONE__":
                                self.cache_metrics['negative_cache_hits'] += 1
                                return None
                            self.cache_metrics['cache_hits'] += 1
                            return cached_label
                        
                        # ElasticsearchÏóêÏÑú Ï°∞Ìöå
                        self.cache_metrics['cache_misses'] += 1
                        self.cache_metrics['elasticsearch_queries'] += 1
                        index_name = get_ontologies_index_name(db_name)
                        doc = await self.elasticsearch_service.get_document(
                            index_name,
                            class_id
                        )
                        
                        if doc:
                            label = doc.get('label')
                            if label:
                                # Ï∫êÏãúÏóê Ï†ÄÏû• (1ÏãúÍ∞Ñ TTL)
                                await self.redis_service.client.setex(
                                    cache_key,
                                    AppConfig.CLASS_LABEL_CACHE_TTL,
                                    label
                                )
                                logger.debug(f"Cached class label for {class_id} in {db_name}: {label}")
                                return label
                        
                        # Í≤∞Í≥ºÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ÎèÑ ÏßßÏùÄ ÏãúÍ∞Ñ Ï∫êÏã± (negative caching)
                        await self.redis_service.client.setex(
                            cache_key,
                            300,  # 5Î∂ÑÍ∞Ñ negative Ï∫êÏã±
                            "__NONE__"  # Îπà Í∞í ÌëúÏãúÏûê
                        )
                        return None
                        
                    finally:
                        # 4. ÎùΩ Ìï¥Ï†ú (Î∞òÎìúÏãú Ïã§Ìñâ)
                        await self.redis_service.client.delete(lock_key)
                        
                else:
                    # 5. ÎùΩ ÌöçÎìù Ïã§Ìå® Ïãú Ïû†Ïãú ÎåÄÍ∏∞ ÌõÑ Ïû¨ÏãúÎèÑ
                    self.cache_metrics['lock_failures'] += 1
                    self.cache_metrics['total_lock_wait_time'] += retry_interval
                    await asyncio.sleep(retry_interval)
                    
            # ÏµúÎåÄ ÎåÄÍ∏∞ ÏãúÍ∞Ñ Ï¥àÍ≥º Ïãú fallback (ÎùΩ ÏóÜÏù¥ ÏßÅÏ†ë Ï°∞Ìöå)
            logger.warning(f"Lock wait timeout for class_label {class_id} in {db_name}, falling back to direct query")
            return await self._get_class_label_fallback(class_id, db_name)
            
        except Exception as e:
            logger.error(f"Failed to get class label for {class_id} in {db_name}: {e}")
            return None
    
    async def _get_class_label_fallback(self, class_id: str, db_name: str) -> Optional[str]:
        """
        ÎùΩ ÌöçÎìù Ïã§Ìå® Ïãú fallback Ï°∞Ìöå (ÏÑ±Îä•Î≥¥Îã§ ÏïàÏ†ïÏÑ± Ïö∞ÏÑ†)
        """
        try:
            self.cache_metrics['fallback_queries'] += 1
            self.cache_metrics['elasticsearch_queries'] += 1
            
            index_name = get_ontologies_index_name(db_name)
            doc = await self.elasticsearch_service.get_document(
                index_name,
                class_id
            )
            
            if doc:
                label = doc.get('label')
                if label:
                    # ÏßßÏùÄ ÏãúÍ∞ÑÎßå Ï∫êÏã± (Í≤ΩÌï© ÏÉÅÌô©Ïù¥ÎØÄÎ°ú)
                    cache_key = AppConfig.get_class_label_key(db_name, class_id)
                    await self.redis_service.client.setex(
                        cache_key,
                        60,  # 1Î∂ÑÎßå Ï∫êÏã±
                        label
                    )
                    return label
                    
            return None
            
        except Exception as e:
            logger.error(f"Fallback query failed for {class_id} in {db_name}: {e}")
            return None
    
    def get_cache_efficiency_metrics(self) -> Dict[str, Any]:
        """
        Ï∫êÏãú Ìö®Ïú®ÏÑ± Î∞è ÎùΩ Í≤ΩÌï© Î©îÌä∏Î¶≠ Î∞òÌôò
        
        Returns:
            Î©îÌä∏Î¶≠ ÎîïÏÖîÎÑàÎ¶¨
        """
        total_requests = (
            self.cache_metrics['cache_hits'] + 
            self.cache_metrics['cache_misses'] + 
            self.cache_metrics['negative_cache_hits']
        )
        
        if total_requests == 0:
            return {
                'cache_hit_rate': 0.0,
                'elasticsearch_query_rate': 0.0,
                'lock_contention_rate': 0.0,
                'average_lock_wait_time': 0.0,
                **self.cache_metrics
            }
        
        cache_hit_rate = (
            self.cache_metrics['cache_hits'] + 
            self.cache_metrics['negative_cache_hits']
        ) / total_requests
        
        total_lock_attempts = (
            self.cache_metrics['lock_acquisitions'] + 
            self.cache_metrics['lock_failures']
        )
        
        lock_contention_rate = (
            self.cache_metrics['lock_failures'] / total_lock_attempts 
            if total_lock_attempts > 0 else 0.0
        )
        
        avg_lock_wait_time = (
            self.cache_metrics['total_lock_wait_time'] / self.cache_metrics['lock_failures']
            if self.cache_metrics['lock_failures'] > 0 else 0.0
        )
        
        return {
            'cache_hit_rate': round(cache_hit_rate * 100, 2),  # Î∞±Î∂ÑÏú®
            'elasticsearch_query_rate': round(
                (self.cache_metrics['elasticsearch_queries'] / total_requests) * 100, 2
            ),
            'lock_contention_rate': round(lock_contention_rate * 100, 2),
            'average_lock_wait_time': round(avg_lock_wait_time * 1000, 2),  # ms Îã®ÏúÑ
            'total_requests': total_requests,
            **self.cache_metrics
        }
    
    def log_cache_metrics(self):
        """Ï∫êÏãú Î©îÌä∏Î¶≠ÏùÑ Î°úÍ∑∏Î°ú Ï∂úÎ†•"""
        metrics = self.get_cache_efficiency_metrics()
        
        logger.info(
            f"Cache Efficiency Metrics - "
            f"Hit Rate: {metrics['cache_hit_rate']}%, "
            f"ES Query Rate: {metrics['elasticsearch_query_rate']}%, "
            f"Lock Contention: {metrics['lock_contention_rate']}%, "
            f"Avg Lock Wait: {metrics['average_lock_wait_time']}ms, "
            f"Total Requests: {metrics['total_requests']}"
        )
        
        if metrics['fallback_queries'] > 0:
            logger.warning(
                f"Fallback queries detected: {metrics['fallback_queries']} "
                f"(indicates high lock contention)"
            )
            
    async def _cache_class_label(self, class_id: str, label: str, db_name: str):
        """ÌÅ¥ÎûòÏä§ ÎùºÎ≤®ÏùÑ RedisÏóê Ï∫êÏã±"""
        try:
            if not class_id or not label or not db_name:
                return
                
            cache_key = AppConfig.get_class_label_key(db_name, class_id)
            await self.redis_service.client.setex(
                cache_key,
                3600,  # 1ÏãúÍ∞Ñ TTL
                label
            )
        except Exception as e:
            logger.error(f"Failed to cache class label for {class_id} in {db_name}: {e}")
            
    def _normalize_properties(self, properties: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ÏÜçÏÑ±ÏùÑ Í≤ÄÏÉâ ÏµúÏ†ÅÌôîÎêú ÌòïÌÉúÎ°ú Ï†ïÍ∑úÌôî"""
        normalized = []
        for prop in properties:
            normalized.append({
                'name': prop.get('name'),
                'value': str(prop.get('value', '')),
                'type': prop.get('type')
            })
        return normalized

    @staticmethod
    def _is_transient_infra_error(error: Exception) -> bool:
        """
        Return True for errors that are expected to recover via retry (e.g. ES outage).

        Projection is a read-model: for transient infra failures we prefer "retry until success"
        over DLQ/commit, to guarantee convergence after recovery.
        """
        msg = str(error).lower()
        transient_markers = [
            # Elasticsearch / aiohttp connection errors (common during ES restart)
            "cannot connect to host elasticsearch",
            "clientconnectorerror",
            "connectionerror",
            "connection error",
            "connect call failed",
            "connection refused",
            "connection reset",
            "temporarily unavailable",
            "service unavailable",
            "timeout",
        ]
        return any(marker in msg for marker in transient_markers)
        
    async def _handle_retry(self, msg, error):
        """Ïû¨ÏãúÎèÑ Ï≤òÎ¶¨"""
        try:
            key = f"{msg.topic()}:{msg.partition()}:{msg.offset()}"

            # Lease contention is not a "failure": another worker is already processing this event_id.
            # Never send this to DLQ; just retry later without consuming retry budget.
            if isinstance(error, _InProgressLeaseError):
                self.retry_count.pop(key, None)
                logger.info(f"Lease in progress elsewhere; retrying later: {key}")
                await asyncio.sleep(2)
                if self.consumer:
                    self.consumer.seek(TopicPartition(msg.topic(), msg.partition(), msg.offset()))
                return

            # Transient infra failures (e.g. Elasticsearch down): retry indefinitely with capped backoff.
            if self._is_transient_infra_error(error):
                retry_count = self.retry_count.get(key, 0) + 1
                self.retry_count[key] = retry_count
                backoff_s = min(max(1, retry_count * 2), 30)
                logger.warning(
                    f"Transient infra error; retrying without DLQ (attempt {retry_count}, backoff={backoff_s}s): {key}"
                )
                await asyncio.sleep(backoff_s)
                if self.consumer:
                    self.consumer.seek(TopicPartition(msg.topic(), msg.partition(), msg.offset()))
                return

            retry_count = self.retry_count.get(key, 0) + 1
            
            if retry_count <= self.max_retries:
                self.retry_count[key] = retry_count
                logger.warning(f"Retrying message (attempt {retry_count}/{self.max_retries}): {key}")
                await asyncio.sleep(min(retry_count * 2, 30))  # capped backoff (avoid max.poll.interval issues)
                # Rewind to the failed offset so we don't accidentally commit past it.
                if self.consumer:
                    self.consumer.seek(TopicPartition(msg.topic(), msg.partition(), msg.offset()))
                return
                
            # ÏµúÎåÄ Ïû¨ÏãúÎèÑ ÌöüÏàò Ï¥àÍ≥º Ïãú DLQÎ°ú Ï†ÑÏÜ°
            logger.error(f"Max retries exceeded for message: {key}, sending to DLQ")
            await self._send_to_dlq(msg, error)
            
            # Ïû¨ÏãúÎèÑ Ïπ¥Ïö¥Ìä∏ Ï†úÍ±∞
            if key in self.retry_count:
                del self.retry_count[key]
                
            # Ïò§ÌîÑÏÖã Ïª§Î∞ã (DLQ Ï†ÑÏÜ° ÌõÑ)
            self.consumer.commit(msg)
            
        except Exception as e:
            logger.error(f"Error in retry handling: {e}")
            
    async def _send_to_dlq(self, msg, error):
        """Ïã§Ìå®Ìïú Î©îÏãúÏßÄÎ•º DLQÎ°ú Ï†ÑÏÜ°"""
        try:
            dlq_message = {
                'original_topic': msg.topic(),
                'original_partition': msg.partition(),
                'original_offset': msg.offset(),
                'original_value': msg.value().decode('utf-8'),
                'error': str(error),
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'worker': 'projection-worker'
            }
            
            self.producer.produce(
                self.dlq_topic,
                key=f"{msg.topic()}:{msg.partition()}:{msg.offset()}",
                value=json.dumps(dlq_message)
            )
            self.producer.flush()
            
            logger.info(f"Message sent to DLQ: {dlq_message}")
            
        except Exception as e:
            logger.error(f"Failed to send message to DLQ: {e}")
            
    async def _shutdown(self):
        """ÏõåÏª§ Ï¢ÖÎ£å"""
        logger.info("Shutting down Projection Worker...")
        
        self.running = False
        
        if self.consumer:
            self.consumer.close()
            
        if self.producer:
            self.producer.flush()
            
        if self.elasticsearch_service:
            await self.elasticsearch_service.disconnect()

        if self.processed_event_registry:
            await self.processed_event_registry.close()
            
        if self.redis_service:
            await self.redis_service.disconnect()
            
        logger.info("Projection Worker stopped")


async def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    worker = ProjectionWorker()
    
    # ÏãúÍ∑∏ÎÑê Ìï∏Îì§Îü¨ ÏÑ§Ï†ï
    def signal_handler(sig, frame):
        logger.info(f"Received signal {sig}")
        worker.running = False
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        await worker.initialize()
        await worker.run()
    except Exception as e:
        logger.error(f"Worker failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
