"""
Projection Worker Service
Instanceì™€ Ontology ì´ë²¤íŠ¸ë¥¼ Elasticsearchì— í”„ë¡œì ì…˜í•˜ëŠ” ì›Œì»¤ ì„œë¹„ìŠ¤

ğŸ”¥ MIGRATION: Enhanced to support S3/MinIO Event Store
- Can read events directly from S3 when reference is provided
- Falls back to PostgreSQL payload for backward compatibility
- Gradual migration from embedded payloads to S3 references
"""

import asyncio
import json
import logging
import os
import signal
from datetime import datetime
from typing import Optional, Dict, Any, List
from uuid import uuid4

from confluent_kafka import Consumer, Producer, KafkaError, KafkaException
import aioboto3

from shared.config.service_config import ServiceConfig
from shared.config.search_config import (
    get_instances_index_name,
    get_ontologies_index_name,
    DEFAULT_INDEX_SETTINGS
)
from shared.config.app_config import AppConfig
from shared.config.settings import ApplicationSettings
from shared.models.events import (
    BaseEvent, EventType,
    InstanceEvent,
    OntologyEvent
)
from shared.services.redis_service import RedisService, create_redis_service
from shared.services.elasticsearch_service import ElasticsearchService, create_elasticsearch_service
from shared.services.projection_manager import ProjectionManager

# Observability imports
from shared.observability.tracing import get_tracing_service
from shared.observability.metrics import get_metrics_collector
from shared.observability.context_propagation import ContextPropagator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ProjectionWorker:
    """Instanceì™€ Ontology ì´ë²¤íŠ¸ë¥¼ Elasticsearchì— í”„ë¡œì ì…˜í•˜ëŠ” ì›Œì»¤
    
    ğŸ”¥ MIGRATION: Now supports reading from S3/MinIO Event Store
    """
    
    def __init__(self):
        self.running = False
        self.kafka_servers = ServiceConfig.get_kafka_bootstrap_servers()
        self.consumer: Optional[Consumer] = None
        self.producer: Optional[Producer] = None
        self.redis_service: Optional[RedisService] = None
        self.elasticsearch_service: Optional[ElasticsearchService] = None
        self.projection_manager: Optional[ProjectionManager] = None
        self.tracing_service = None
        self.metrics_collector = None
        self.context_propagator = ContextPropagator()
        
        # ğŸ”¥ S3/MinIO Event Store configuration
        self.s3_event_store_enabled = os.getenv("ENABLE_S3_EVENT_STORE", "false").lower() == "true"
        self.event_store_bucket = "spice-event-store"
        self.aioboto_session = None
        
        # ìƒì„±ëœ ì¸ë±ìŠ¤ ìºì‹œ (ì¤‘ë³µ ìƒì„± ë°©ì§€)
        self.created_indices = set()
        
        # DLQ í† í”½
        self.dlq_topic = AppConfig.PROJECTION_DLQ_TOPIC
        
        # ì¬ì‹œë„ ì„¤ì •
        self.max_retries = 5
        self.retry_count = {}
        
        # Cache Stampede ë°©ì§€ ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­
        self.cache_metrics = {
            'cache_hits': 0,
            'cache_misses': 0,
            'negative_cache_hits': 0,
            'lock_acquisitions': 0,
            'lock_failures': 0,
            'elasticsearch_queries': 0,
            'fallback_queries': 0,
            'total_lock_wait_time': 0.0,
            # ğŸ”¥ S3 Event Store metrics
            's3_reads': 0,
            's3_read_failures': 0,
            'payload_fallbacks': 0
        }
        
    async def initialize(self):
        """ì›Œì»¤ ì´ˆê¸°í™”"""
        # Kafka Consumer ì„¤ì • (ë©€í‹° í† í”½ êµ¬ë…)
        self.consumer = Consumer({
            'bootstrap.servers': self.kafka_servers,
            'group.id': 'projection-worker-group',
            'auto.offset.reset': 'earliest',
            'enable.auto.commit': False,
            'max.poll.interval.ms': 300000,  # 5ë¶„
            'session.timeout.ms': 45000,  # 45ì´ˆ
        })
        
        # Kafka Producer ì„¤ì • (ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œí–‰ìš©)
        self.producer = Producer({
            'bootstrap.servers': self.kafka_servers,
            'client.id': 'projection-worker',
            'acks': 'all',
            'retries': 3,
            'compression.type': 'snappy',
        })
        
        # Redis ì—°ê²° ì„¤ì • (ì˜¨í†¨ë¡œì§€ ìºì‹±ìš©)
        settings = ApplicationSettings()
        self.redis_service = create_redis_service(settings)
        await self.redis_service.connect()
        logger.info("Redis connection established")
        
        # Elasticsearch ì—°ê²° ì„¤ì •
        self.elasticsearch_service = create_elasticsearch_service(settings)
        await self.elasticsearch_service.connect()
        logger.info("Elasticsearch connection established")
        
        # ì¸ë±ìŠ¤ ìƒì„± ë° ë§¤í•‘ ì„¤ì •
        await self._setup_indices()
        
        # í† í”½ êµ¬ë…
        topics = [AppConfig.INSTANCE_EVENTS_TOPIC, AppConfig.ONTOLOGY_EVENTS_TOPIC]
        self.consumer.subscribe(topics)
        logger.info(f"Subscribed to topics: {topics}")
        
        # Initialize OpenTelemetry
        self.tracing_service = get_tracing_service("projection-worker")
        self.metrics_collector = get_metrics_collector("projection-worker")
        
        # ğŸ”¥ Initialize aioboto3 for async S3 operations (Event Store)
        if self.s3_event_store_enabled:
            self.aioboto_session = aioboto3.Session()
            logger.info(f"ğŸ”¥ S3/MinIO Event Store ENABLED - bucket: {self.event_store_bucket}")
            logger.info("ğŸ”¥ Projection Worker will read events from S3 when available")
        else:
            logger.info("âš ï¸ S3/MinIO Event Store DISABLED - using legacy payload mode")
        
        # ğŸ¯ Initialize ProjectionManager for materialized views
        try:
            # ProjectionManagerëŠ” GraphFederationServiceWOQLì´ í•„ìš”í•˜ë¯€ë¡œ
            # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ë³„ë„ë¡œ ì´ˆê¸°í™”í•˜ë„ë¡ ì„¤ê³„ë¨
            # ì—¬ê¸°ì„œëŠ” ìŠ¤ì¼ˆë ˆí†¤ë§Œ ì¤€ë¹„
            logger.info("ğŸ¯ ProjectionManager ready for initialization when graph service is available")
            # TODO: Initialize ProjectionManager when GraphFederationServiceWOQL is available
            # self.projection_manager = ProjectionManager(
            #     graph_service=graph_service,
            #     es_service=self.elasticsearch_service,
            #     redis_service=self.redis_service
            # )
        except Exception as e:
            logger.warning(f"ProjectionManager initialization skipped: {e}")
        
    async def read_event_from_s3(self, s3_reference: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ğŸ”¥ Read event from S3/MinIO Event Store
        
        Args:
            s3_reference: Dictionary with bucket, key, and endpoint
            
        Returns:
            Event payload from S3 or None if failed
        """
        if not self.s3_event_store_enabled or not self.aioboto_session:
            return None
            
        try:
            bucket = s3_reference.get('bucket', self.event_store_bucket)
            key = s3_reference.get('key')
            
            if not key:
                logger.warning("No S3 key provided in reference")
                return None
            
            async with self.aioboto_session.client(
                's3',
                endpoint_url=os.getenv('MINIO_ENDPOINT_URL', 'http://localhost:9000'),
                aws_access_key_id=os.getenv('MINIO_ACCESS_KEY', 'admin'),
                aws_secret_access_key=os.getenv('MINIO_SECRET_KEY', 'spice123!'),
                use_ssl=False
            ) as s3:
                response = await s3.get_object(Bucket=bucket, Key=key)
                content = await response['Body'].read()
                event_data = json.loads(content)
                
                self.cache_metrics['s3_reads'] += 1
                logger.info(f"ğŸ”¥ Read event from S3: {key}")
                return event_data
                
        except Exception as e:
            self.cache_metrics['s3_read_failures'] += 1
            logger.error(f"Failed to read event from S3: {e}")
            return None
    
    async def extract_payload_from_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ Extract payload from message, preferring S3 if available
        
        Supports both:
        - New format: Read from S3 using reference
        - Legacy format: Use embedded payload
        """
        # Check if this is the new format with S3 reference
        if 's3_reference' in message and self.s3_event_store_enabled:
            s3_ref = message['s3_reference']
            logger.info(f"ğŸ”¥ Message has S3 reference, attempting to read from Event Store")
            
            # Try to read from S3
            event_data = await self.read_event_from_s3(s3_ref)
            if event_data:
                # Return the payload from S3 event
                return event_data.get('payload', {})
            else:
                logger.warning("Failed to read from S3, falling back to embedded payload")
                self.cache_metrics['payload_fallbacks'] += 1
        
        # Fall back to embedded payload (legacy or fallback)
        if 'payload' in message:
            return message['payload']
        
        # Very old format - the message itself is the event
        return message
        
    async def _setup_indices(self):
        """ë§¤í•‘ íŒŒì¼ ë¡œë“œ (ì¸ë±ìŠ¤ëŠ” DBë³„ë¡œ ë™ì  ìƒì„±)"""
        try:
            # ë§¤í•‘ íŒŒì¼ë§Œ ë¯¸ë¦¬ ë¡œë“œ
            self.instances_mapping = await self._load_mapping('instances_mapping.json')
            self.ontologies_mapping = await self._load_mapping('ontologies_mapping.json')
            logger.info("Loaded index mappings successfully")
                
        except Exception as e:
            logger.error(f"Failed to load mappings: {e}")
            raise
            
    async def _ensure_index_exists(self, db_name: str, index_type: str = "instances"):
        """íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ì˜ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        if index_type == "instances":
            index_name = get_instances_index_name(db_name)
            mapping = self.instances_mapping
        else:
            index_name = get_ontologies_index_name(db_name)
            mapping = self.ontologies_mapping
            
        # ì´ë¯¸ ìƒì„±ëœ ì¸ë±ìŠ¤ëŠ” ìŠ¤í‚µ
        if index_name in self.created_indices:
            return index_name
            
        try:
            if not await self.elasticsearch_service.index_exists(index_name):
                # ì„¤ì • ë³‘í•© (ë§¤í•‘ íŒŒì¼ ì„¤ì • + ê¸°ë³¸ ì„¤ì •)
                settings = mapping.get('settings', {}).copy()
                settings.update(DEFAULT_INDEX_SETTINGS)
                
                await self.elasticsearch_service.create_index(
                    index_name,
                    mappings=mapping['mappings'],
                    settings=settings
                )
                logger.info(f"Created index: {index_name} for database: {db_name}")
                
            self.created_indices.add(index_name)
            return index_name
            
        except Exception as e:
            logger.error(f"Failed to ensure index exists for {db_name}: {e}")
            raise
            
    async def _load_mapping(self, filename: str) -> Dict[str, Any]:
        """ë§¤í•‘ íŒŒì¼ ë¡œë“œ"""
        mapping_path = os.path.join(
            os.path.dirname(__file__), 
            "mappings", 
            filename
        )
        try:
            with open(mapping_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load mapping {filename}: {e}")
            raise
            
    async def run(self):
        """ë©”ì¸ ì‹¤í–‰ ë£¨í”„"""
        self.running = True
        logger.info("Projection Worker started")
        
        try:
            while self.running:
                msg = self.consumer.poll(timeout=1.0)
                if msg is None:
                    continue
                    
                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        continue
                    else:
                        logger.error(f"Kafka error: {msg.error()}")
                        continue
                        
                try:
                    # ì´ë²¤íŠ¸ ì²˜ë¦¬
                    await self._process_event(msg)
                    # ì„±ê³µ ì‹œ ì˜¤í”„ì…‹ ì»¤ë°‹
                    self.consumer.commit(msg)
                    
                except Exception as e:
                    logger.error(f"Failed to process event: {e}")
                    # ì¬ì‹œë„ ë¡œì§
                    await self._handle_retry(msg, e)
                    
        except KeyboardInterrupt:
            logger.info("Received keyboard interrupt")
        except Exception as e:
            logger.error(f"Unexpected error in main loop: {e}")
        finally:
            await self._shutdown()
            
    async def _process_event(self, msg):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            raw_message = json.loads(msg.value().decode('utf-8'))
            
            # ğŸ”¥ MIGRATION: Handle both new and legacy message formats
            # New format has metadata with storage_mode
            storage_mode = raw_message.get('metadata', {}).get('storage_mode', 'legacy')
            logger.info(f"ğŸ”¥ Message storage mode: {storage_mode}")
            
            # Extract the actual event payload
            event_data = await self.extract_payload_from_message(raw_message)
            event_type = event_data.get('event_type')
            topic = msg.topic()
            
            logger.info(f"Processing event: {event_type} from topic: {topic}")
            
            if topic == AppConfig.INSTANCE_EVENTS_TOPIC:
                await self._handle_instance_event(event_data)
            elif topic == AppConfig.ONTOLOGY_EVENTS_TOPIC:
                await self._handle_ontology_event(event_data)
            else:
                logger.warning(f"Unknown topic: {topic}")
                
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse event JSON: {e}")
            raise
        except Exception as e:
            logger.error(f"Error processing event: {e}")
            raise
            
    async def _handle_instance_event(self, event_data: Dict[str, Any]):
        """ì¸ìŠ¤í„´ìŠ¤ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            event_type = event_data.get('event_type')
            instance_data = event_data.get('data', {})
            event_id = event_data.get('event_id')
            
            if event_type == EventType.INSTANCE_CREATED.value:
                await self._handle_instance_created(instance_data, event_id, event_data)
            elif event_type == EventType.INSTANCE_UPDATED.value:
                await self._handle_instance_updated(instance_data, event_id, event_data)
            elif event_type == EventType.INSTANCE_DELETED.value:
                await self._handle_instance_deleted(instance_data, event_id, event_data)
            else:
                logger.warning(f"Unknown instance event type: {event_type}")
                
        except Exception as e:
            logger.error(f"Error handling instance event: {e}")
            raise
            
    async def _handle_ontology_event(self, event_data: Dict[str, Any]):
        """ì˜¨í†¨ë¡œì§€ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            event_type = event_data.get('event_type')
            ontology_data = event_data.get('data', {})
            event_id = event_data.get('event_id')
            
            if event_type == EventType.ONTOLOGY_CLASS_CREATED.value:
                await self._handle_ontology_class_created(ontology_data, event_id, event_data)
            elif event_type == EventType.ONTOLOGY_CLASS_UPDATED.value:
                await self._handle_ontology_class_updated(ontology_data, event_id, event_data)
            elif event_type == EventType.ONTOLOGY_CLASS_DELETED.value:
                await self._handle_ontology_class_deleted(ontology_data, event_id, event_data)
            elif event_type == EventType.DATABASE_CREATED.value:
                await self._handle_database_created(ontology_data, event_id, event_data)
            elif event_type == EventType.DATABASE_DELETED.value:
                await self._handle_database_deleted(ontology_data, event_id, event_data)
            else:
                logger.warning(f"Unknown ontology event type: {event_type}")
                
        except Exception as e:
            logger.error(f"Error handling ontology event: {e}")
            raise
            
    async def _handle_instance_created(self, instance_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ì¶”ì¶œ
            db_name = event_data.get('db_name') or instance_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for instance creation")
                
            # ì¸ë±ìŠ¤ í™•ì¸ ë° ìƒì„±
            index_name = await self._ensure_index_exists(db_name, "instances")
            
            # í´ë˜ìŠ¤ ë¼ë²¨ ì¡°íšŒ (Redis ìºì‹œ í™œìš©)
            class_label = await self._get_class_label(instance_data.get('class_id'), db_name)
            
            # Elasticsearch ë¬¸ì„œ êµ¬ì„±
            doc = {
                'instance_id': instance_data.get('instance_id'),
                'class_id': instance_data.get('class_id'),
                'class_label': class_label,
                'properties': self._normalize_properties(instance_data.get('properties', [])),
                'data': instance_data,  # ì›ë³¸ ë°ì´í„° (enabled: false)
                'event_id': event_id,
                'event_timestamp': event_data.get('timestamp'),
                'version': 1,
                'db_name': db_name,
                'branch': instance_data.get('branch'),
                'created_at': datetime.utcnow().isoformat(),
                'updated_at': datetime.utcnow().isoformat()
            }
            
            # ë©±ë“±ì„±ì„ ìœ„í•´ event_idë¥¼ ë¬¸ì„œ IDë¡œ ì‚¬ìš©
            await self.elasticsearch_service.index_document(
                index_name,
                doc,
                doc_id=event_id,
                refresh=True
            )
            
            logger.info(f"Instance created in Elasticsearch: {instance_data.get('instance_id')} in index: {index_name}")
            
        except Exception as e:
            logger.error(f"Failed to handle instance created: {e}")
            raise
            
    async def _handle_instance_updated(self, instance_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """ì¸ìŠ¤í„´ìŠ¤ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ì¶”ì¶œ
            db_name = event_data.get('db_name') or instance_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for instance update")
                
            # ì¸ë±ìŠ¤ í™•ì¸ ë° ìƒì„±
            index_name = await self._ensure_index_exists(db_name, "instances")
            
            # í´ë˜ìŠ¤ ë¼ë²¨ ì¡°íšŒ
            class_label = await self._get_class_label(instance_data.get('class_id'), db_name)
            
            # ê¸°ì¡´ ë¬¸ì„œ ì¡°íšŒ
            existing_doc = await self.elasticsearch_service.get_document(
                index_name,
                instance_data.get('instance_id')
            )
            
            version = 1
            if existing_doc:
                version = existing_doc.get('version', 0) + 1
            
            # ì—…ë°ì´íŠ¸ ë¬¸ì„œ êµ¬ì„±
            doc = {
                'instance_id': instance_data.get('instance_id'),
                'class_id': instance_data.get('class_id'),
                'class_label': class_label,
                'properties': self._normalize_properties(instance_data.get('properties', [])),
                'data': instance_data,
                'event_id': event_id,
                'event_timestamp': event_data.get('timestamp'),
                'version': version,
                'db_name': db_name,
                'branch': instance_data.get('branch'),
                'updated_at': datetime.utcnow().isoformat()
            }
            
            # ë¬¸ì„œ ì—…ë°ì´íŠ¸
            await self.elasticsearch_service.update_document(
                index_name,
                instance_data.get('instance_id'),
                doc=doc,
                upsert=doc,
                refresh=True
            )
            
            logger.info(f"Instance updated in Elasticsearch: {instance_data.get('instance_id')} in index: {index_name}")
            
        except Exception as e:
            logger.error(f"Failed to handle instance updated: {e}")
            raise
            
    async def _handle_instance_deleted(self, instance_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """ì¸ìŠ¤í„´ìŠ¤ ì‚­ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ì¶”ì¶œ
            db_name = event_data.get('db_name') or instance_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for instance deletion")
                
            # ì¸ë±ìŠ¤ ì´ë¦„ ê²°ì •
            index_name = get_instances_index_name(db_name)
            instance_id = instance_data.get('instance_id')
            
            # ë¬¸ì„œ ì‚­ì œ
            success = await self.elasticsearch_service.delete_document(
                index_name,
                instance_id,
                refresh=True
            )
            
            if success:
                logger.info(f"Instance deleted from Elasticsearch: {instance_id} from index: {index_name}")
            else:
                logger.warning(f"Instance not found for deletion: {instance_id} in index: {index_name}")
                
        except Exception as e:
            logger.error(f"Failed to handle instance deleted: {e}")
            raise
            
    async def _handle_ontology_class_created(self, ontology_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ì¶”ì¶œ
            db_name = event_data.get('db_name') or ontology_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for ontology class creation")
                
            # ì¸ë±ìŠ¤ í™•ì¸ ë° ìƒì„±
            index_name = await self._ensure_index_exists(db_name, "ontologies")
            
            # Elasticsearch ë¬¸ì„œ êµ¬ì„±
            doc = {
                'class_id': ontology_data.get('id'),
                'label': ontology_data.get('label'),
                'description': ontology_data.get('description'),
                'properties': ontology_data.get('properties', []),
                'relationships': ontology_data.get('relationships', []),
                'parent_classes': ontology_data.get('parent_classes', []),
                'child_classes': ontology_data.get('child_classes', []),
                'db_name': db_name,
                'branch': ontology_data.get('branch'),
                'version': 1,
                'event_id': event_id,
                'event_timestamp': event_data.get('timestamp'),
                'created_at': datetime.utcnow().isoformat(),
                'updated_at': datetime.utcnow().isoformat()
            }
            
            # ì¸ë±ì‹±
            await self.elasticsearch_service.index_document(
                index_name,
                doc,
                doc_id=ontology_data.get('id'),
                refresh=True
            )
            
            # Redisì— í´ë˜ìŠ¤ ë¼ë²¨ ìºì‹± (DBë³„ë¡œ í‚¤ êµ¬ë¶„)
            await self._cache_class_label(
                ontology_data.get('id'),
                ontology_data.get('label'),
                db_name
            )
            
            logger.info(f"Ontology class created in Elasticsearch: {ontology_data.get('id')} in index: {index_name}")
            
        except Exception as e:
            logger.error(f"Failed to handle ontology class created: {e}")
            raise
            
    async def _handle_ontology_class_updated(self, ontology_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ì¶”ì¶œ
            db_name = event_data.get('db_name') or ontology_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for ontology class update")
                
            # ì¸ë±ìŠ¤ í™•ì¸ ë° ìƒì„±
            index_name = await self._ensure_index_exists(db_name, "ontologies")
            
            # ê¸°ì¡´ ë¬¸ì„œ ì¡°íšŒ
            existing_doc = await self.elasticsearch_service.get_document(
                index_name,
                ontology_data.get('id')
            )
            
            version = 1
            if existing_doc:
                version = existing_doc.get('version', 0) + 1
            
            # ì—…ë°ì´íŠ¸ ë¬¸ì„œ êµ¬ì„±
            doc = {
                'class_id': ontology_data.get('id'),
                'label': ontology_data.get('label'),
                'description': ontology_data.get('description'),
                'properties': ontology_data.get('properties', []),
                'relationships': ontology_data.get('relationships', []),
                'parent_classes': ontology_data.get('parent_classes', []),
                'child_classes': ontology_data.get('child_classes', []),
                'db_name': db_name,
                'branch': ontology_data.get('branch'),
                'version': version,
                'event_id': event_id,
                'event_timestamp': event_data.get('timestamp'),
                'updated_at': datetime.utcnow().isoformat()
            }
            
            # ë¬¸ì„œ ì—…ë°ì´íŠ¸
            await self.elasticsearch_service.update_document(
                index_name,
                ontology_data.get('id'),
                doc=doc,
                upsert=doc,
                refresh=True
            )
            
            # Redis ìºì‹œ ì—…ë°ì´íŠ¸ (DBë³„ë¡œ í‚¤ êµ¬ë¶„)
            await self._cache_class_label(
                ontology_data.get('id'),
                ontology_data.get('label'),
                db_name
            )
            
            logger.info(f"Ontology class updated in Elasticsearch: {ontology_data.get('id')} in index: {index_name}")
            
        except Exception as e:
            logger.error(f"Failed to handle ontology class updated: {e}")
            raise
            
    async def _handle_ontology_class_deleted(self, ontology_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ì‚­ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ì¶”ì¶œ
            db_name = event_data.get('db_name') or ontology_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for ontology class deletion")
                
            # ì¸ë±ìŠ¤ ì´ë¦„ ê²°ì •
            index_name = get_ontologies_index_name(db_name)
            class_id = ontology_data.get('id')
            
            # ë¬¸ì„œ ì‚­ì œ
            success = await self.elasticsearch_service.delete_document(
                index_name,
                class_id,
                refresh=True
            )
            
            # Redis ìºì‹œ ì‚­ì œ (DBë³„ë¡œ í‚¤ êµ¬ë¶„)
            await self.redis_service.delete(AppConfig.get_class_label_key(db_name, class_id))
            
            if success:
                logger.info(f"Ontology class deleted from Elasticsearch: {class_id} from index: {index_name}")
            else:
                logger.warning(f"Ontology class not found for deletion: {class_id} in index: {index_name}")
                
        except Exception as e:
            logger.error(f"Failed to handle ontology class deleted: {e}")
            raise
            
    async def _handle_database_created(self, db_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            db_name = db_data.get('db_name') or event_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for database creation")
                
            # ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹œ ê¸°ë³¸ ì¸ë±ìŠ¤ë“¤ì„ ë¯¸ë¦¬ ì¤€ë¹„
            logger.info(f"Database created: {db_name}, preparing Elasticsearch indices")
            
            # ì¸ìŠ¤í„´ìŠ¤ì™€ ì˜¨í†¨ë¡œì§€ ì¸ë±ìŠ¤ë¥¼ ë¯¸ë¦¬ ìƒì„±
            await self._ensure_index_exists(db_name, "instances")
            await self._ensure_index_exists(db_name, "ontologies")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ë©”íƒ€ë°ì´í„° ë¬¸ì„œ ìƒì„± (ê²€ìƒ‰ ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ê´€ë¦¬)
            metadata_index = "spice_database_metadata"
            metadata_doc = {
                'database_name': db_name,
                'description': db_data.get('description', ''),
                'created_at': datetime.utcnow().isoformat(),
                'created_by': event_data.get('occurred_by', 'system'),
                'event_id': event_id,
                'status': 'active'
            }
            
            # ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ í™•ì¸ ë° ìƒì„±
            if not await self.elasticsearch_service.index_exists(metadata_index):
                await self.elasticsearch_service.create_index(
                    metadata_index,
                    mappings={
                        "properties": {
                            "database_name": {"type": "keyword"},
                            "description": {"type": "text"},
                            "created_at": {"type": "date"},
                            "created_by": {"type": "keyword"},
                            "event_id": {"type": "keyword"},
                            "status": {"type": "keyword"}
                        }
                    },
                    settings=DEFAULT_INDEX_SETTINGS
                )
            
            # ë©”íƒ€ë°ì´í„° ë¬¸ì„œ ì¸ë±ì‹±
            await self.elasticsearch_service.index_document(
                metadata_index,
                metadata_doc,
                doc_id=db_name,
                refresh=True
            )
            
            logger.info(f"Database creation processed: {db_name}, indices prepared and metadata indexed")
            
        except Exception as e:
            logger.error(f"Failed to handle database created: {e}")
            raise
            
    async def _handle_database_deleted(self, db_data: Dict[str, Any], event_id: str, event_data: Dict[str, Any]):
        """ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            db_name = db_data.get('db_name') or event_data.get('db_name')
            if not db_name:
                raise ValueError("db_name is required for database deletion")
                
            logger.info(f"Database deleted: {db_name}, cleaning up Elasticsearch indices")
            
            # ê´€ë ¨ ì¸ë±ìŠ¤ë“¤ ì‚­ì œ
            instances_index = get_instances_index_name(db_name)
            ontologies_index = get_ontologies_index_name(db_name)
            
            # ì¸ë±ìŠ¤ ì‚­ì œ (ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ)
            if await self.elasticsearch_service.index_exists(instances_index):
                await self.elasticsearch_service.delete_index(instances_index)
                logger.info(f"Deleted instances index: {instances_index}")
                
            if await self.elasticsearch_service.index_exists(ontologies_index):
                await self.elasticsearch_service.delete_index(ontologies_index)
                logger.info(f"Deleted ontologies index: {ontologies_index}")
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì™„ì „ ì‚­ì œ ëŒ€ì‹  ë¹„í™œì„±í™”)
            metadata_index = "spice_database_metadata"
            if await self.elasticsearch_service.index_exists(metadata_index):
                await self.elasticsearch_service.update_document(
                    metadata_index,
                    db_name,
                    doc={
                        'status': 'deleted',
                        'deleted_at': datetime.utcnow().isoformat(),
                        'deleted_by': event_data.get('occurred_by', 'system'),
                        'deletion_event_id': event_id
                    },
                    refresh=True
                )
            
            # ìƒì„±ëœ ì¸ë±ìŠ¤ ìºì‹œì—ì„œ ì œê±°
            self.created_indices.discard(instances_index)
            self.created_indices.discard(ontologies_index)
            
            logger.info(f"Database deletion processed: {db_name}, indices cleaned up and metadata updated")
            
        except Exception as e:
            logger.error(f"Failed to handle database deleted: {e}")
            raise
            
    async def _get_class_label(self, class_id: str, db_name: str) -> Optional[str]:
        """
        Redisì—ì„œ í´ë˜ìŠ¤ ë¼ë²¨ ì¡°íšŒ (Cache Stampede ë°©ì§€)
        
        ë¶„ì‚° ë½ì„ ì‚¬ìš©í•˜ì—¬ ë™ì‹œì— ì—¬ëŸ¬ ìš”ì²­ì´ ë“¤ì–´ì™€ë„ 
        Elasticsearchì—ëŠ” í•œ ë²ˆë§Œ ìš”ì²­í•˜ë„ë¡ ìµœì í™”í•©ë‹ˆë‹¤.
        """
        try:
            if not class_id or not db_name:
                return None
                
            cache_key = AppConfig.get_class_label_key(db_name, class_id)
            lock_key = f"lock:{cache_key}"
            
            # ìºì‹œ stampede ë°©ì§€ë¥¼ ìœ„í•œ ë¶„ì‚° ë½ ë©”ì»¤ë‹ˆì¦˜
            max_wait_time = 5.0  # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
            lock_timeout = 10    # ë½ íƒ€ì„ì•„ì›ƒ 10ì´ˆ
            retry_interval = 0.05  # 50ms ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„
            
            start_time = asyncio.get_event_loop().time()
            
            while (asyncio.get_event_loop().time() - start_time) < max_wait_time:
                # 1. ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
                cached_label = await self.redis_service.client.get(cache_key)
                if cached_label:
                    # Negative caching ì²˜ë¦¬
                    if cached_label == "__NONE__":
                        self.cache_metrics['negative_cache_hits'] += 1
                        return None
                    self.cache_metrics['cache_hits'] += 1
                    return cached_label
                
                # 2. ë¶„ì‚° ë½ íšë“ ì‹œë„ (SETNX with TTL)
                lock_acquired = await self.redis_service.client.set(
                    lock_key, 
                    "1", 
                    ex=lock_timeout,  # TTL ì„¤ì •ìœ¼ë¡œ ë°ë“œë½ ë°©ì§€
                    nx=True  # SET if Not eXists
                )
                
                if lock_acquired:
                    # 3. ë½ì„ íšë“í•œ ìš”ì²­ë§Œ Elasticsearchì—ì„œ ë°ì´í„° ì¡°íšŒ
                    self.cache_metrics['lock_acquisitions'] += 1
                    try:
                        # ë½ íšë“ í›„ ë‹¤ì‹œ í•œë²ˆ ìºì‹œ í™•ì¸ (ë‹¤ë¥¸ ìš”ì²­ì´ ì´ë¯¸ ì €ì¥í–ˆì„ ìˆ˜ ìˆìŒ)
                        cached_label = await self.redis_service.client.get(cache_key)
                        if cached_label:
                            # Negative caching ì²˜ë¦¬
                            if cached_label == "__NONE__":
                                self.cache_metrics['negative_cache_hits'] += 1
                                return None
                            self.cache_metrics['cache_hits'] += 1
                            return cached_label
                        
                        # Elasticsearchì—ì„œ ì¡°íšŒ
                        self.cache_metrics['cache_misses'] += 1
                        self.cache_metrics['elasticsearch_queries'] += 1
                        index_name = get_ontologies_index_name(db_name)
                        doc = await self.elasticsearch_service.get_document(
                            index_name,
                            class_id
                        )
                        
                        if doc:
                            label = doc.get('label')
                            if label:
                                # ìºì‹œì— ì €ì¥ (1ì‹œê°„ TTL)
                                await self.redis_service.client.setex(
                                    cache_key,
                                    AppConfig.CLASS_LABEL_CACHE_TTL,
                                    label
                                )
                                logger.debug(f"Cached class label for {class_id} in {db_name}: {label}")
                                return label
                        
                        # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ë„ ì§§ì€ ì‹œê°„ ìºì‹± (negative caching)
                        await self.redis_service.client.setex(
                            cache_key,
                            300,  # 5ë¶„ê°„ negative ìºì‹±
                            "__NONE__"  # ë¹ˆ ê°’ í‘œì‹œì
                        )
                        return None
                        
                    finally:
                        # 4. ë½ í•´ì œ (ë°˜ë“œì‹œ ì‹¤í–‰)
                        await self.redis_service.client.delete(lock_key)
                        
                else:
                    # 5. ë½ íšë“ ì‹¤íŒ¨ ì‹œ ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    self.cache_metrics['lock_failures'] += 1
                    self.cache_metrics['total_lock_wait_time'] += retry_interval
                    await asyncio.sleep(retry_interval)
                    
            # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ ì‹œ fallback (ë½ ì—†ì´ ì§ì ‘ ì¡°íšŒ)
            logger.warning(f"Lock wait timeout for class_label {class_id} in {db_name}, falling back to direct query")
            return await self._get_class_label_fallback(class_id, db_name)
            
        except Exception as e:
            logger.error(f"Failed to get class label for {class_id} in {db_name}: {e}")
            return None
    
    async def _get_class_label_fallback(self, class_id: str, db_name: str) -> Optional[str]:
        """
        ë½ íšë“ ì‹¤íŒ¨ ì‹œ fallback ì¡°íšŒ (ì„±ëŠ¥ë³´ë‹¤ ì•ˆì •ì„± ìš°ì„ )
        """
        try:
            self.cache_metrics['fallback_queries'] += 1
            self.cache_metrics['elasticsearch_queries'] += 1
            
            index_name = get_ontologies_index_name(db_name)
            doc = await self.elasticsearch_service.get_document(
                index_name,
                class_id
            )
            
            if doc:
                label = doc.get('label')
                if label:
                    # ì§§ì€ ì‹œê°„ë§Œ ìºì‹± (ê²½í•© ìƒí™©ì´ë¯€ë¡œ)
                    cache_key = AppConfig.get_class_label_key(db_name, class_id)
                    await self.redis_service.client.setex(
                        cache_key,
                        60,  # 1ë¶„ë§Œ ìºì‹±
                        label
                    )
                    return label
                    
            return None
            
        except Exception as e:
            logger.error(f"Fallback query failed for {class_id} in {db_name}: {e}")
            return None
    
    def get_cache_efficiency_metrics(self) -> Dict[str, Any]:
        """
        ìºì‹œ íš¨ìœ¨ì„± ë° ë½ ê²½í•© ë©”íŠ¸ë¦­ ë°˜í™˜
        
        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        total_requests = (
            self.cache_metrics['cache_hits'] + 
            self.cache_metrics['cache_misses'] + 
            self.cache_metrics['negative_cache_hits']
        )
        
        if total_requests == 0:
            return {
                'cache_hit_rate': 0.0,
                'elasticsearch_query_rate': 0.0,
                'lock_contention_rate': 0.0,
                'average_lock_wait_time': 0.0,
                **self.cache_metrics
            }
        
        cache_hit_rate = (
            self.cache_metrics['cache_hits'] + 
            self.cache_metrics['negative_cache_hits']
        ) / total_requests
        
        total_lock_attempts = (
            self.cache_metrics['lock_acquisitions'] + 
            self.cache_metrics['lock_failures']
        )
        
        lock_contention_rate = (
            self.cache_metrics['lock_failures'] / total_lock_attempts 
            if total_lock_attempts > 0 else 0.0
        )
        
        avg_lock_wait_time = (
            self.cache_metrics['total_lock_wait_time'] / self.cache_metrics['lock_failures']
            if self.cache_metrics['lock_failures'] > 0 else 0.0
        )
        
        return {
            'cache_hit_rate': round(cache_hit_rate * 100, 2),  # ë°±ë¶„ìœ¨
            'elasticsearch_query_rate': round(
                (self.cache_metrics['elasticsearch_queries'] / total_requests) * 100, 2
            ),
            'lock_contention_rate': round(lock_contention_rate * 100, 2),
            'average_lock_wait_time': round(avg_lock_wait_time * 1000, 2),  # ms ë‹¨ìœ„
            'total_requests': total_requests,
            **self.cache_metrics
        }
    
    def log_cache_metrics(self):
        """ìºì‹œ ë©”íŠ¸ë¦­ì„ ë¡œê·¸ë¡œ ì¶œë ¥"""
        metrics = self.get_cache_efficiency_metrics()
        
        logger.info(
            f"Cache Efficiency Metrics - "
            f"Hit Rate: {metrics['cache_hit_rate']}%, "
            f"ES Query Rate: {metrics['elasticsearch_query_rate']}%, "
            f"Lock Contention: {metrics['lock_contention_rate']}%, "
            f"Avg Lock Wait: {metrics['average_lock_wait_time']}ms, "
            f"Total Requests: {metrics['total_requests']}"
        )
        
        if metrics['fallback_queries'] > 0:
            logger.warning(
                f"Fallback queries detected: {metrics['fallback_queries']} "
                f"(indicates high lock contention)"
            )
            
    async def _cache_class_label(self, class_id: str, label: str, db_name: str):
        """í´ë˜ìŠ¤ ë¼ë²¨ì„ Redisì— ìºì‹±"""
        try:
            if not class_id or not label or not db_name:
                return
                
            cache_key = AppConfig.get_class_label_key(db_name, class_id)
            await self.redis_service.client.setex(
                cache_key,
                3600,  # 1ì‹œê°„ TTL
                label
            )
        except Exception as e:
            logger.error(f"Failed to cache class label for {class_id} in {db_name}: {e}")
            
    def _normalize_properties(self, properties: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì†ì„±ì„ ê²€ìƒ‰ ìµœì í™”ëœ í˜•íƒœë¡œ ì •ê·œí™”"""
        normalized = []
        for prop in properties:
            normalized.append({
                'name': prop.get('name'),
                'value': str(prop.get('value', '')),
                'type': prop.get('type')
            })
        return normalized
        
    async def _handle_retry(self, msg, error):
        """ì¬ì‹œë„ ì²˜ë¦¬"""
        try:
            key = f"{msg.topic()}:{msg.partition()}:{msg.offset()}"
            retry_count = self.retry_count.get(key, 0) + 1
            
            if retry_count <= self.max_retries:
                self.retry_count[key] = retry_count
                logger.warning(f"Retrying message (attempt {retry_count}/{self.max_retries}): {key}")
                await asyncio.sleep(retry_count * 2)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                return
                
            # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ DLQë¡œ ì „ì†¡
            logger.error(f"Max retries exceeded for message: {key}, sending to DLQ")
            await self._send_to_dlq(msg, error)
            
            # ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì œê±°
            if key in self.retry_count:
                del self.retry_count[key]
                
            # ì˜¤í”„ì…‹ ì»¤ë°‹ (DLQ ì „ì†¡ í›„)
            self.consumer.commit(msg)
            
        except Exception as e:
            logger.error(f"Error in retry handling: {e}")
            
    async def _send_to_dlq(self, msg, error):
        """ì‹¤íŒ¨í•œ ë©”ì‹œì§€ë¥¼ DLQë¡œ ì „ì†¡"""
        try:
            dlq_message = {
                'original_topic': msg.topic(),
                'original_partition': msg.partition(),
                'original_offset': msg.offset(),
                'original_value': msg.value().decode('utf-8'),
                'error': str(error),
                'timestamp': datetime.utcnow().isoformat(),
                'worker': 'projection-worker'
            }
            
            self.producer.produce(
                self.dlq_topic,
                key=f"{msg.topic()}:{msg.partition()}:{msg.offset()}",
                value=json.dumps(dlq_message)
            )
            self.producer.flush()
            
            logger.info(f"Message sent to DLQ: {dlq_message}")
            
        except Exception as e:
            logger.error(f"Failed to send message to DLQ: {e}")
            
    async def _shutdown(self):
        """ì›Œì»¤ ì¢…ë£Œ"""
        logger.info("Shutting down Projection Worker...")
        
        # ğŸ”¥ Log S3 Event Store metrics
        if self.s3_event_store_enabled:
            logger.info("ğŸ”¥ S3/MinIO Event Store Metrics:")
            logger.info(f"  - S3 reads: {self.cache_metrics['s3_reads']}")
            logger.info(f"  - S3 read failures: {self.cache_metrics['s3_read_failures']}")
            logger.info(f"  - Payload fallbacks: {self.cache_metrics['payload_fallbacks']}")
            
            if self.cache_metrics['s3_reads'] > 0:
                success_rate = (self.cache_metrics['s3_reads'] - self.cache_metrics['s3_read_failures']) / self.cache_metrics['s3_reads'] * 100
                logger.info(f"  - S3 read success rate: {success_rate:.1f}%")
        
        self.running = False
        
        if self.consumer:
            self.consumer.close()
            
        if self.producer:
            self.producer.flush()
            
        if self.elasticsearch_service:
            await self.elasticsearch_service.disconnect()
            
        if self.redis_service:
            await self.redis_service.disconnect()
            
        logger.info("Projection Worker stopped")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    worker = ProjectionWorker()
    
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì •
    def signal_handler(sig, frame):
        logger.info(f"Received signal {sig}")
        worker.running = False
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        await worker.initialize()
        await worker.run()
    except Exception as e:
        logger.error(f"Worker failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())