"""
ì˜¨í†¨ë¡œì§€ CRUD ë¼ìš°í„°
ì˜¨í†¨ë¡œì§€ ìƒì„±, ì¡°íšŒ, ìˆ˜ì •, ì‚­ì œë¥¼ ë‹´ë‹¹
"""

import json
import logging
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, Depends, HTTPException, Query, Request, status
from pydantic import BaseModel

from shared.models.ontology import (
    OntologyBase,
    OntologyCreateRequestBFF,
    OntologyResponse,
    OntologyUpdateInput,
    Property,
    Relationship,
)

# Add shared path for common utilities
from shared.utils.language import get_accept_language

# Security validation imports
from shared.security.input_sanitizer import sanitize_input, validate_db_name, validate_class_id


# Schema suggestion request models
class SchemaFromDataRequest(BaseModel):
    """Request model for schema suggestion from data"""

    data: List[List[Any]]
    columns: List[str]
    class_name: Optional[str] = None
    include_complex_types: bool = False


class SchemaFromGoogleSheetsRequest(BaseModel):
    """Request model for schema suggestion from Google Sheets"""

    sheet_url: str
    worksheet_name: Optional[str] = None
    class_name: Optional[str] = None
    api_key: Optional[str] = None


class MappingSuggestionRequest(BaseModel):
    """Request model for mapping suggestions between schemas"""
    
    source_schema: List[Dict[str, Any]]  # New data schema
    target_schema: List[Dict[str, Any]]  # Existing ontology schema
    sample_data: Optional[List[Dict[str, Any]]] = None  # Sample values for pattern matching
    target_sample_data: Optional[List[Dict[str, Any]]] = None  # Target sample values for distribution matching


from fastapi import HTTPException

# Modernized dependency injection imports  
from bff.dependencies import (
    get_oms_client,
    OMSClientDep,
    LabelMapperDep,
    JSONLDConverterDep,
    TerminusServiceDep,
    TerminusService,
    LabelMapper,
    JSONToJSONLDConverter
)
from bff.services.adapter_service import BFFAdapterService
from bff.services.oms_client import OMSClient

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/database/{db_name}", tags=["Ontology Management"])


# Dependency for BFF Adapter Service
def get_bff_adapter(
    terminus_service: TerminusService = TerminusServiceDep,
    label_mapper: LabelMapper = LabelMapperDep
) -> BFFAdapterService:
    """Get BFF Adapter Service instance"""
    return BFFAdapterService(terminus_service, label_mapper)

# Type-safe dependency annotation for BFF Adapter Service
BFFAdapterServiceDep = Depends(get_bff_adapter)


@router.post("/ontology", response_model=OntologyResponse)
async def create_ontology(
    db_name: str,
    ontology: Dict[str, Any],  # Accept raw dict to preserve target field
    mapper: LabelMapper = LabelMapperDep,
    terminus: TerminusService = TerminusServiceDep,
    jsonld_conv: JSONToJSONLDConverter = JSONLDConverterDep,
):
    """
    ì˜¨í†¨ë¡œì§€ ìƒì„±

    ìƒˆë¡œìš´ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ë ˆì´ë¸” ê¸°ë°˜ìœ¼ë¡œ IDê°€ ìë™ ìƒì„±ë©ë‹ˆë‹¤.
    """
    # ğŸ”¥ ULTRA DEBUG! Force logging to check if route is called
    
    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        
        # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ (ì´ë¯¸ dict í˜•íƒœ)
        ontology_dict = ontology.copy()  # Make a copy to avoid modifying the original
        ontology_dict = sanitize_input(ontology_dict)

        # ID ìƒì„± ë˜ëŠ” ê²€ì¦
        if ontology_dict.get('id'):
            # ì‚¬ìš©ìê°€ IDë¥¼ ì œê³µí•œ ê²½ìš° ê²€ì¦
            class_id = validate_class_id(ontology_dict['id'])
            logger.info(f"Using provided class_id '{class_id}'")
        else:
            # IDê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ìƒì„±
            from shared.utils.id_generator import generate_simple_id
            class_id = generate_simple_id(
                label=ontology_dict.get('label', ''), use_timestamp_for_korean=True, default_fallback="UnnamedClass"
            )
            logger.info(f"Generated class_id '{class_id}' from label '{ontology_dict.get('label', '')}')")

        ontology_dict["id"] = class_id

        # ğŸ”¥ THINK ULTRA! Transform properties for OMS compatibility
        # Convert 'target' to 'linkTarget' for link-type properties
        def transform_properties_for_oms(data):
            # Extract string from language objects
            def extract_string_value(value, field_name="field"):
                """Extract string value from either a plain string or language object"""
                if isinstance(value, str):
                    return value
                elif isinstance(value, dict):
                    # Try to get Korean first, then English, then any available language
                    return value.get('ko') or value.get('en') or next(iter(value.values()), '')
                else:
                    return str(value) if value else ''
            
            # Transform top-level label and description
            if 'label' in data:
                data['label'] = extract_string_value(data['label'], 'label')
            if 'description' in data:
                data['description'] = extract_string_value(data['description'], 'description')
            
            if 'properties' in data and isinstance(data['properties'], list):
                for i, prop in enumerate(data['properties']):
                    if isinstance(prop, dict):
                        # Extract string values from language objects in properties
                        if 'label' in prop:
                            prop['label'] = extract_string_value(prop['label'], 'property label')
                        if 'description' in prop:
                            prop['description'] = extract_string_value(prop['description'], 'property description')
                        
                        # ğŸ”¥ ULTRA! BFF uses 'label' but OMS requires 'name'
                        if 'name' not in prop and 'label' in prop:
                            # Generate name from label
                            prop['name'] = generate_simple_id(prop['label'], use_timestamp_for_korean=False)
                            logger.info(f"ğŸ”¥ Generated property name '{prop['name']}' from label '{prop['label']}'")
                        
                        # ğŸ”¥ ULTRA! Convert STRING to xsd:string for OMS
                        if prop.get('type') == 'STRING':
                            prop['type'] = 'xsd:string'
                        elif prop.get('type') == 'INTEGER':
                            prop['type'] = 'xsd:integer'
                        elif prop.get('type') == 'DECIMAL':
                            prop['type'] = 'xsd:decimal'
                        elif prop.get('type') == 'BOOLEAN':
                            prop['type'] = 'xsd:boolean'
                        elif prop.get('type') == 'DATETIME':
                            prop['type'] = 'xsd:dateTime'
                        
                        # Convert target to linkTarget for link type properties
                        if prop.get('type') == 'link' and 'target' in prop:
                            prop['linkTarget'] = prop.pop('target')
                            logger.info(f"ğŸ”§ Converted property '{prop.get('name')}' target -> linkTarget: {prop.get('linkTarget')}")
                        
                        # Handle array properties with link items
                        if prop.get('type') == 'array' and 'items' in prop:
                            items = prop['items']
                            if isinstance(items, dict) and items.get('type') == 'link' and 'target' in items:
                                items['linkTarget'] = items.pop('target')
                                logger.info(f"ğŸ”§ Converted array property '{prop.get('name')}' items target -> linkTarget: {items.get('linkTarget')}")
            
            # Transform relationships
            if 'relationships' in data and isinstance(data['relationships'], list):
                for rel in data['relationships']:
                    if isinstance(rel, dict):
                        if 'label' in rel:
                            rel['label'] = extract_string_value(rel['label'], 'relationship label')
                        if 'description' in rel:
                            rel['description'] = extract_string_value(rel['description'], 'relationship description')
        
        # Apply transformation
        transform_properties_for_oms(ontology_dict)
        
        # ğŸ”¥ ULTRA DEBUG! Write to file to verify transformation
        debug_file = f"/tmp/bff_debug_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(debug_file, 'w') as f:
            f.write(f"BEFORE transformation: {json.dumps(ontology, ensure_ascii=False, indent=2)}\n\n")
            f.write(f"AFTER transformation: {json.dumps(ontology_dict, ensure_ascii=False, indent=2)}\n")
        
        # Log transformed data for debugging
        logger.info(f"ğŸ”¥ ULTRA DEBUG! Sending to OMS: {json.dumps(ontology_dict, ensure_ascii=False, indent=2)}")
        
        # ì˜¨í†¨ë¡œì§€ ìƒì„±
        result = await terminus.create_class(db_name, ontology_dict)
        logger.info(f"OMS create result: {result}")

        # ë ˆì´ë¸” ë§¤í•‘ ë“±ë¡
        await mapper.register_class(db_name, class_id, ontology_dict.get('label', ''), ontology_dict.get('description'))

        # ì†ì„± ë ˆì´ë¸” ë§¤í•‘
        for prop in ontology_dict.get('properties', []):
            if isinstance(prop, dict):
                await mapper.register_property(db_name, class_id, prop.get('name', ''), prop.get('label', ''))

        # ê´€ê³„ ë ˆì´ë¸” ë§¤í•‘
        for rel in ontology_dict.get('relationships', []):
            if isinstance(rel, dict):
                await mapper.register_relationship(db_name, rel.get('predicate', ''), rel.get('label', ''))

        # OMS ì‘ë‹µì—ì„œ ìƒì„±ëœ ë°ì´í„° ì¶”ì¶œ
        if isinstance(result, dict):
            # OMSê°€ data í•„ë“œë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°
            if "data" in result and result.get("status") == "success":
                created_data = result["data"]
                # ìƒì„±ëœ ID ì‚¬ìš©
                if "id" in created_data:
                    class_id = created_data["id"]
            elif "id" in result:
                # ì§ì ‘ ë°ì´í„° í˜•ì‹ì¸ ê²½ìš°
                created_data = result
                class_id = result["id"]

        # ì‘ë‹µ ìƒì„±
        # Validate properties and relationships to ensure they're properly formatted
        validated_properties = []
        for prop in ontology_dict.get('properties', []):
            if isinstance(prop, dict):
                validated_properties.append(Property(**prop))
                
        validated_relationships = []
        for rel in ontology_dict.get('relationships', []):
            if isinstance(rel, dict):
                validated_relationships.append(Relationship(**rel))
        
        # Create response
        return OntologyResponse(
            id=class_id,
            label=ontology_dict.get('label', ''),
            description=ontology_dict.get('description'),
            parent_class=ontology_dict.get('parent_class'),
            abstract=ontology_dict.get('abstract', False),
            properties=validated_properties,
            relationships=validated_relationships,
            metadata={"created": True, "database": db_name}
        )

    except HTTPException as he:
        # ğŸ”¥ ULTRA! Properly propagate HTTP exceptions with correct status codes
        logger.error(f"HTTP exception in create_ontology: {he.status_code} - {he.detail}")
        raise he
    except Exception as e:
        logger.error(f"Failed to create ontology: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/ontology/list")
async def list_ontologies(
    db_name: str,
    request: Request,
    class_type: str = Query("sys:Class", description="í´ë˜ìŠ¤ íƒ€ì…"),
    limit: Optional[int] = Query(None, description="ê²°ê³¼ ê°œìˆ˜ ì œí•œ"),
    offset: int = Query(0, description="ì˜¤í”„ì…‹"),
    mapper: LabelMapper = LabelMapperDep,
    terminus: TerminusService = TerminusServiceDep,
):
    """
    ì˜¨í†¨ë¡œì§€ ëª©ë¡ ì¡°íšŒ

    ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ì˜¨í†¨ë¡œì§€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        
        # class_type íŒŒë¼ë¯¸í„° í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê²€ì¦ (Security Enhancement)
        allowed_class_types = {"sys:Class", "owl:Class", "rdfs:Class"}
        if class_type not in allowed_class_types:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid class_type. Allowed values: {', '.join(allowed_class_types)}"
            )
        
        # ì˜¨í†¨ë¡œì§€ ëª©ë¡ ì¡°íšŒ
        ontologies = await terminus.list_classes(db_name)
        
        # ğŸ”¥ ULTRA DEBUG!
        logger.info(f"ğŸ”¥ ontologies type: {type(ontologies)}")
        logger.info(f"ğŸ”¥ ontologies content: {ontologies[:2] if ontologies else 'empty'}")

        # ë°°ì¹˜ ë ˆì´ë¸” ì •ë³´ ì¶”ê°€ (N+1 ì¿¼ë¦¬ ë¬¸ì œ í•´ê²°)
        # ğŸ”¥ ULTRA! Skip label mapping for now to fix the issue
        labeled_ontologies = ontologies  # await mapper.convert_to_display_batch(db_name, ontologies, lang)

        return {
            "total": len(labeled_ontologies),
            "ontologies": labeled_ontologies,
            "offset": offset,
            "limit": limit,
        }

    except HTTPException as he:
        # ğŸ”¥ ULTRA! Properly propagate HTTP exceptions
        logger.error(f"HTTP exception in list_ontologies: {he.status_code} - {he.detail}")
        raise he
    except Exception as e:
        logger.error(f"Failed to list ontologies: {e}")
        # ğŸ”¥ ULTRA! Check if it's a 404 from OMS
        if "404" in str(e) or "not found" in str(e).lower():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ë°ì´í„°ë² ì´ìŠ¤ '{db_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/ontology/{class_label}", response_model=OntologyResponse)
async def get_ontology(
    db_name: str,
    class_label: str,
    request: Request,
    mapper: LabelMapper = LabelMapperDep,
    terminus: TerminusService = TerminusServiceDep,
):
    """
    ì˜¨í†¨ë¡œì§€ ì¡°íšŒ

    ë ˆì´ë¸” ë˜ëŠ” IDë¡œ ì˜¨í†¨ë¡œì§€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        class_label = sanitize_input(class_label)
        # URL íŒŒë¼ë¯¸í„° ë””ë²„ê¹…
        logger.info(
            f"Getting ontology - db_name: {db_name}, class_label: {class_label}, lang: {lang}"
        )

        # ë ˆì´ë¸”ë¡œ ID ì¡°íšŒ ì‹œë„
        class_id = await mapper.get_class_id(db_name, class_label, lang)
        logger.info(f"Mapped label '{class_label}' to class_id: {class_id}")

        # IDê°€ ì—†ìœ¼ë©´ ì…ë ¥ê°’ì„ IDë¡œ ê°„ì£¼
        if not class_id:
            class_id = class_label
            logger.info(f"No mapping found, using label as ID: {class_id}")

        # ì˜¨í†¨ë¡œì§€ ì¡°íšŒ
        result = await terminus.get_class(db_name, class_id)

        if not result:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ì˜¨í†¨ë¡œì§€ '{class_label}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            )

        logger.info(f"Raw result from terminus.get_class: {result}")

        # Extract ontology data (terminus.get_class already extracts from data field)
        ontology_data = result if isinstance(result, dict) else {}

        # ë ˆì´ë¸” ì •ë³´ ì¶”ê°€
        ontology_data = await mapper.convert_to_display(db_name, ontology_data, lang)
        logger.info(f"After convert_to_display: {ontology_data}")

        # Ensure result has required fields for OntologyResponse
        if not ontology_data.get("id"):
            ontology_data["id"] = class_id
        if not ontology_data.get("label"):
            # Try to get label from the mapper
            label = await mapper.get_class_label(db_name, class_id, lang)
            ontology_data["label"] = label or class_label

        # Remove extra fields that are not part of OntologyBase
        # Ensure label and description are simple strings
        label_value = ontology_data.get("label")
        if isinstance(label_value, dict):
            # Extract string from dict (fallback to English if available)
            label_value = label_value.get(lang) or label_value.get("en") or label_value.get("ko") or class_label
        elif not isinstance(label_value, str):
            label_value = str(label_value) if label_value else class_label
        
        desc_value = ontology_data.get("description")
        if isinstance(desc_value, dict):
            # Extract string from dict (fallback to English if available)
            desc_value = desc_value.get(lang) or desc_value.get("en") or desc_value.get("ko") or ""
        elif not isinstance(desc_value, str) and desc_value:
            desc_value = str(desc_value)
        
        ontology_base_data = {
            "id": ontology_data.get("id"),
            "label": label_value,
            "description": desc_value,
            "properties": ontology_data.get("properties", []),
            "relationships": ontology_data.get("relationships", []),
            "parent_class": ontology_data.get("parent_class"),
            "abstract": ontology_data.get("abstract", False),
            "metadata": ontology_data.get("metadata", {}),
        }

        # OntologyResponse inherits from OntologyBase, so pass fields directly
        return OntologyResponse(**ontology_base_data)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get ontology: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}",
        )


@router.put("/ontology/{class_label}")
async def update_ontology(
    db_name: str,
    class_label: str,
    ontology: OntologyUpdateInput,
    request: Request,
    mapper: LabelMapper = LabelMapperDep,
    terminus: TerminusService = TerminusServiceDep,
):
    """
    ì˜¨í†¨ë¡œì§€ ìˆ˜ì •

    ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        class_label = sanitize_input(class_label)
        # ë ˆì´ë¸”ë¡œ ID ì¡°íšŒ
        class_id = await mapper.get_class_id(db_name, class_label, lang)
        if not class_id:
            class_id = class_label

        # ì—…ë°ì´íŠ¸ ë°ì´í„° ì¤€ë¹„
        update_data = ontology.dict(exclude_unset=True)
        update_data["id"] = class_id

        # ì˜¨í†¨ë¡œì§€ ì—…ë°ì´íŠ¸
        await terminus.update_class(db_name, class_id, update_data)

        # ë ˆì´ë¸” ë§¤í•‘ ì—…ë°ì´íŠ¸
        await mapper.update_mappings(db_name, update_data)

        return {
            "message": f"ì˜¨í†¨ë¡œì§€ '{class_label}'ì´(ê°€) ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤",
            "id": class_id,
            "updated_fields": list(update_data.keys()),
        }

    except HTTPException as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ì˜¨í†¨ë¡œì§€ '{e.ontology_id}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
        )
    except HTTPException as e:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e.message}",
        )
    except Exception as e:
        logger.error(f"Failed to update ontology: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}",
        )


@router.delete("/ontology/{class_label}")
async def delete_ontology(
    db_name: str,
    class_label: str,
    request: Request,
    mapper: LabelMapper = LabelMapperDep,
    terminus: TerminusService = TerminusServiceDep,
):
    """
    ì˜¨í†¨ë¡œì§€ ì‚­ì œ

    ì˜¨í†¨ë¡œì§€ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    ì£¼ì˜: ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        class_label = sanitize_input(class_label)
        # ë ˆì´ë¸”ë¡œ ID ì¡°íšŒ
        class_id = await mapper.get_class_id(db_name, class_label, lang)
        if not class_id:
            class_id = class_label

        # ì˜¨í†¨ë¡œì§€ ì‚­ì œ
        await terminus.delete_class(db_name, class_id)

        # ë ˆì´ë¸” ë§¤í•‘ ì‚­ì œ
        await mapper.remove_class(db_name, class_id)

        return {"message": f"ì˜¨í†¨ë¡œì§€ '{class_label}'ì´(ê°€) ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤", "id": class_id}

    except HTTPException as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ì˜¨í†¨ë¡œì§€ '{e.ontology_id}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
        )
    except Exception as e:
        logger.error(f"Failed to delete ontology: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ì‚­ì œ ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/ontology/{class_id}/schema")
async def get_ontology_schema(
    db_name: str,
    class_id: str,
    request: Request,
    format: str = Query("json", description="ìŠ¤í‚¤ë§ˆ í˜•ì‹ (json, jsonld, owl)"),
    mapper: LabelMapper = LabelMapperDep,
    terminus: TerminusService = TerminusServiceDep,
    jsonld_conv: JSONToJSONLDConverter = JSONLDConverterDep,
):
    """
    ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ

    ì˜¨í†¨ë¡œì§€ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        class_id = sanitize_input(class_id)
        
        # format íŒŒë¼ë¯¸í„° í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê²€ì¦ (Security Enhancement)
        allowed_formats = {"json", "jsonld", "owl"}
        if format not in allowed_formats:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid format. Allowed values: {', '.join(allowed_formats)}"
            )
        
        # ë ˆì´ë¸”ë¡œ ID ì¡°íšŒ
        actual_id = await mapper.get_class_id(db_name, class_id, lang)
        if not actual_id:
            actual_id = class_id

        # ì˜¨í†¨ë¡œì§€ ì¡°íšŒ
        ontology = await terminus.get_class(db_name, actual_id)

        if not ontology:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ì˜¨í†¨ë¡œì§€ '{class_id}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            )

        # í˜•ì‹ë³„ ë³€í™˜
        if format == "jsonld":
            schema = jsonld_conv.convert_to_jsonld(ontology, db_name)
        elif format == "owl":
            # OWL í˜•ì‹ì€ JSON-LDë¡œ ëŒ€ì²´ (RDF í˜¸í™˜)
            schema = jsonld_conv.convert_to_jsonld(ontology, db_name)
            # OWL í˜•ì‹ í‘œì‹œë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if isinstance(schema, dict):
                schema["@comment"] = "This is a JSON-LD representation compatible with OWL"
                schema["@owl:versionInfo"] = "1.0"
        else:
            # ê¸°ë³¸ JSON
            schema = ontology

        return schema

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get ontology schema: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}",
        )


# ğŸ”¥ THINK ULTRA! BFF Enhanced Relationship Management Endpoints


@router.post("/ontology-advanced", response_model=OntologyResponse)
async def create_ontology_with_relationship_validation(
    db_name: str,
    ontology: OntologyCreateRequestBFF,
    adapter: BFFAdapterService = BFFAdapterServiceDep,
):
    """
    ğŸ”¥ ê³ ê¸‰ ê´€ê³„ ê²€ì¦ì„ í¬í•¨í•œ ì˜¨í†¨ë¡œì§€ ìƒì„± (BFF ë ˆì´ì–´ - ë¦¬íŒ©í† ë§ë¨)

    ì´ì œ ì¤‘ë³µ ë¡œì§ ì—†ì´ BFF Adapterë¥¼ í†µí•´ OMSë¡œ ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìœ„ì„í•©ë‹ˆë‹¤.
    
    Features:
    - ë ˆì´ë¸” ê¸°ë°˜ ìë™ ID ìƒì„±
    - ìë™ ì—­ê´€ê³„ ìƒì„±
    - ê´€ê³„ ê²€ì¦ ë° ë¬´ê²°ì„± ì²´í¬
    - ìˆœí™˜ ì°¸ì¡° íƒì§€
    - ë‹¤êµ­ì–´ ë ˆì´ë¸” ë§¤í•‘
    """
    try:
        # BFF Adapterë¥¼ í†µí•´ ëª¨ë“  ë¡œì§ ìœ„ì„ (ì¤‘ë³µ ì œê±°)
        return await adapter.create_advanced_ontology(
            db_name=db_name,
            ontology_data=ontology.dict(exclude_unset=True),
            language="ko"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to create ontology with advanced relationships: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê³ ê¸‰ ì˜¨í†¨ë¡œì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}",
        )


@router.post("/validate-relationships")
async def validate_ontology_relationships_bff(
    db_name: str,
    ontology: OntologyCreateRequestBFF,
    adapter: BFFAdapterService = BFFAdapterServiceDep,
):
    """
    ğŸ”¥ ì˜¨í†¨ë¡œì§€ ê´€ê³„ ê²€ì¦ (BFF ë ˆì´ì–´ - ë¦¬íŒ©í† ë§ë¨)

    ì´ì œ ì¤‘ë³µ ë¡œì§ ì—†ì´ BFF Adapterë¥¼ í†µí•´ OMSë¡œ ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìœ„ì„í•©ë‹ˆë‹¤.
    """
    try:
        # BFF Adapterë¥¼ í†µí•´ ëª¨ë“  ë¡œì§ ìœ„ì„ (ì¤‘ë³µ ì œê±°)
        return await adapter.validate_relationships(
            db_name=db_name,
            validation_data=ontology.dict(exclude_unset=True),
            language="ko"
        )

    except HTTPException:
        raise

    except Exception as e:
        logger.error(f"Failed to validate relationships: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"ê´€ê³„ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/check-circular-references")
async def check_circular_references_bff(
    db_name: str,
    ontology: Optional[OntologyCreateRequestBFF] = None,
    adapter: BFFAdapterService = BFFAdapterServiceDep,
):
    """
    ğŸ”¥ ìˆœí™˜ ì°¸ì¡° íƒì§€ (BFF ë ˆì´ì–´ - ë¦¬íŒ©í† ë§ë¨)

    ì´ì œ ì¤‘ë³µ ë¡œì§ ì—†ì´ BFF Adapterë¥¼ í†µí•´ OMSë¡œ ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìœ„ì„í•©ë‹ˆë‹¤.
    """
    try:
        # ìƒˆ ì˜¨í†¨ë¡œì§€ ë°ì´í„° ì¤€ë¹„
        detection_data = {}
        if ontology:
            detection_data = ontology.dict(exclude_unset=True)

        # BFF Adapterë¥¼ í†µí•´ ëª¨ë“  ë¡œì§ ìœ„ì„ (ì¤‘ë³µ ì œê±°)
        return await adapter.detect_circular_references(
            db_name=db_name,
            detection_data=detection_data,
            language="ko"
        )

    except Exception as e:
        logger.error(f"Failed to check circular references: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìˆœí™˜ ì°¸ì¡° íƒì§€ ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/relationship-network/analyze")
async def analyze_relationship_network_bff(
    db_name: str,
    request: Request,
    terminus: TerminusService = TerminusServiceDep,
    mapper: LabelMapper = LabelMapperDep,
):
    """
    ğŸ”¥ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ (BFF ë ˆì´ì–´)

    ì „ì²´ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ì˜ ê±´ê°•ì„±ì„ ë¶„ì„í•˜ê³  ì‚¬ìš©ì ì¹œí™”ì  ê²°ê³¼ ë°˜í™˜
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìˆ˜í–‰
        analysis_result = await terminus.analyze_relationship_network(db_name)

        # ì‚¬ìš©ì ì¹œí™”ì  í˜•íƒœë¡œ ë³€í™˜
        summary = analysis_result.get("relationship_summary", {})
        validation = analysis_result.get("validation_summary", {})
        cycles = analysis_result.get("cycle_analysis", {})
        graph = analysis_result.get("graph_summary", {})
        recommendations = analysis_result.get("recommendations", [])

        # ê±´ê°•ì„± ì ìˆ˜ ê³„ì‚° (0-100)
        health_score = 100
        if validation.get("errors", 0) > 0:
            health_score -= validation["errors"] * 10
        if cycles.get("critical_cycles", 0) > 0:
            health_score -= cycles["critical_cycles"] * 15
        if validation.get("warnings", 0) > 0:
            health_score -= validation["warnings"] * 2
        health_score = max(0, min(100, health_score))

        # ê±´ê°•ì„± ë“±ê¸‰ ê²°ì •
        if health_score >= 90:
            health_grade = "Excellent"
            health_color = "green"
        elif health_score >= 70:
            health_grade = "Good"
            health_color = "blue"
        elif health_score >= 50:
            health_grade = "Fair"
            health_color = "yellow"
        else:
            health_grade = "Poor"
            health_color = "red"

        return {
            "status": "success",
            "message": "ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "network_health": {
                "score": health_score,
                "grade": health_grade,
                "color": health_color,
                "description": f"ë„¤íŠ¸ì›Œí¬ ê±´ê°•ì„±ì´ {health_grade.lower()} ìƒíƒœì…ë‹ˆë‹¤",
            },
            "statistics": {
                "total_ontologies": analysis_result.get("ontology_count", 0),
                "total_relationships": summary.get("total_relationships", 0),
                "relationship_types": len(graph.get("relationship_types", [])),
                "total_entities": graph.get("total_entities", 0),
                "average_connections": round(graph.get("average_connections_per_entity", 0), 2),
            },
            "quality_metrics": {
                "validation_errors": validation.get("errors", 0),
                "validation_warnings": validation.get("warnings", 0),
                "critical_cycles": cycles.get("critical_cycles", 0),
                "total_cycles": cycles.get("total_cycles", 0),
                "inverse_coverage": summary.get("inverse_coverage", "0/0 (0%)"),
            },
            "recommendations": [
                {
                    "priority": "high" if "âŒ" in rec else "medium" if "âš ï¸" in rec else "low",
                    "message": rec.replace("âŒ ", "")
                    .replace("âš ï¸ ", "")
                    .replace("ğŸ“ ", "")
                    .replace("ğŸ”„ ", ""),
                }
                for rec in recommendations
            ],
            "metadata": {
                "analysis_timestamp": analysis_result.get("analysis_timestamp"),
                "database": db_name,
                "language": lang,
            },
        }

    except Exception as e:
        logger.error(f"Failed to analyze relationship network: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/relationship-paths")
async def find_relationship_paths_bff(
    request: Request,
    db_name: str,
    start_entity: str,
    end_entity: Optional[str] = Query(
        None, description="ëª©í‘œ ì—”í‹°í‹° (ì—†ìœ¼ë©´ ëª¨ë“  ë„ë‹¬ ê°€ëŠ¥í•œ ì—”í‹°í‹°)"
    ),
    max_depth: int = Query(3, ge=1, le=5, description="ìµœëŒ€ íƒìƒ‰ ê¹Šì´"),
    path_type: str = Query("shortest", description="ê²½ë¡œ íƒ€ì… (shortest, all, weighted, semantic)"),
    terminus: TerminusService = TerminusServiceDep,
    mapper: LabelMapper = LabelMapperDep,
):
    """
    ğŸ”¥ ê´€ê³„ ê²½ë¡œ íƒìƒ‰ (BFF ë ˆì´ì–´)

    ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ ê²½ë¡œë¥¼ ì°¾ì•„ ì‚¬ìš©ì ì¹œí™”ì  í˜•íƒœë¡œ ë°˜í™˜
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        start_entity = sanitize_input(start_entity)
        if end_entity:
            end_entity = sanitize_input(end_entity)
        
        # path_type íŒŒë¼ë¯¸í„° í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê²€ì¦ (Security Enhancement)
        allowed_path_types = {"shortest", "all", "weighted", "semantic"}
        if path_type not in allowed_path_types:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid path_type. Allowed values: {', '.join(allowed_path_types)}"
            )
        # ë ˆì´ë¸”ì„ IDë¡œ ë³€í™˜
        start_id = await mapper.get_class_id(db_name, start_entity, lang)
        if not start_id:
            start_id = start_entity

        end_id = None
        if end_entity:
            end_id = await mapper.get_class_id(db_name, end_entity, lang)
            if not end_id:
                end_id = end_entity

        # ê²½ë¡œ íƒìƒ‰ ìˆ˜í–‰
        path_result = await terminus.find_relationship_paths(
            db_name=db_name,
            start_entity=start_id,
            end_entity=end_id,
            max_depth=max_depth,
            path_type=path_type,
        )

        # IDë¥¼ ë ˆì´ë¸”ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©ì ì¹œí™”ì  ê²°ê³¼ ìƒì„±
        paths = path_result.get("paths", [])
        user_friendly_paths = []

        for path in paths:
            # ì—”í‹°í‹° IDë“¤ì„ ë ˆì´ë¸”ë¡œ ë³€í™˜
            entity_labels = []
            for entity_id in path.get("entities", []):
                label = await mapper.get_class_label(db_name, entity_id, lang)
                entity_labels.append(label or entity_id)

            user_friendly_paths.append(
                {
                    "start": entity_labels[0] if entity_labels else start_entity,
                    "end": (
                        entity_labels[-1]
                        if len(entity_labels) > 1
                        else entity_labels[0] if entity_labels else ""
                    ),
                    "path": " â†’ ".join(entity_labels),
                    "predicates": path.get("predicates", []),
                    "length": path.get("length", 0),
                    "weight": round(path.get("total_weight", 0), 2),
                    "confidence": path.get("confidence", 1.0),
                    "path_type": path.get("path_type", "unknown"),
                }
            )

        statistics = path_result.get("statistics", {})

        return {
            "status": "success",
            "message": f"ê´€ê³„ ê²½ë¡œ íƒìƒ‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({len(user_friendly_paths)}ê°œ ê²½ë¡œ ë°œê²¬)",
            "query": {
                "start_entity": start_entity,
                "end_entity": end_entity,
                "max_depth": max_depth,
                "path_type": path_type,
            },
            "paths": user_friendly_paths[:10],  # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜
            "statistics": {
                "total_paths_found": len(paths),
                "displayed_paths": min(len(user_friendly_paths), 10),
                "average_length": statistics.get("average_length", 0),
                "shortest_path_length": statistics.get("min_length", 0),
                "longest_path_length": statistics.get("max_length", 0),
            },
            "metadata": {"language": lang, "start_id": start_id, "end_id": end_id},
        }

    except Exception as e:
        logger.error(f"Failed to find relationship paths: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê´€ê³„ ê²½ë¡œ íƒìƒ‰ ì‹¤íŒ¨: {str(e)}",
        )


@router.post("/suggest-schema-from-data")
async def suggest_schema_from_data(
    db_name: str,
    request: SchemaFromDataRequest,
    terminus: TerminusService = TerminusServiceDep,
):
    """
    ğŸ”¥ ë°ì´í„°ì—ì„œ ìŠ¤í‚¤ë§ˆ ìë™ ì œì•ˆ

    ì œê³µëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ OMS ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ëŠ” Funnel ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ê¸°ëŠ¥ì„ í†µí•©í•œ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.
    """
    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        from bff.services.funnel_client import FunnelClient

        # Funnel í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ ìŠ¤í‚¤ë§ˆ ì œì•ˆ ìš”ì²­
        async with FunnelClient() as funnel_client:
            # ë°ì´í„° ë¶„ì„ ë° ìŠ¤í‚¤ë§ˆ ì œì•ˆì„ í•œ ë²ˆì— ìˆ˜í–‰
            result = await funnel_client.analyze_and_suggest_schema(
                data=request.data,
                columns=request.columns,
                class_name=request.class_name,
                include_complex_types=request.include_complex_types,
            )

            analysis = result.get("analysis", {})
            schema_suggestion = result.get("schema_suggestion", {})

            # ë¶„ì„ ê²°ê³¼ ìš”ì•½
            analyzed_columns = analysis.get("columns", [])
            high_confidence_types = [
                col
                for col in analyzed_columns
                if col.get("inferred_type", {}).get("confidence", 0) >= 0.7
            ]

            return {
                "status": "success",
                "message": f"ìŠ¤í‚¤ë§ˆ ì œì•ˆì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. {len(high_confidence_types)}/{len(analyzed_columns)} ì»¬ëŸ¼ì— ëŒ€í•´ ë†’ì€ ì‹ ë¢°ë„ íƒ€ì… ì¶”ë¡  ì„±ê³µ",
                "suggested_schema": schema_suggestion,
                "analysis_summary": {
                    "total_columns": len(analyzed_columns),
                    "high_confidence_columns": len(high_confidence_types),
                    "average_confidence": (
                        round(
                            sum(
                                col.get("inferred_type", {}).get("confidence", 0)
                                for col in analyzed_columns
                            )
                            / len(analyzed_columns),
                            2,
                        )
                        if analyzed_columns
                        else 0
                    ),
                    "detected_types": list(
                        set(
                            col.get("inferred_type", {}).get("type", "unknown")
                            for col in analyzed_columns
                        )
                    ),
                },
                "detailed_analysis": analysis,
            }

    except Exception as e:
        logger.error(f"Schema suggestion failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"ìŠ¤í‚¤ë§ˆ ì œì•ˆ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/suggest-mappings")
async def suggest_mappings(
    db_name: str,
    request: MappingSuggestionRequest,
) -> Dict[str, Any]:
    """
    ë‘ ìŠ¤í‚¤ë§ˆ ê°„ì˜ ë§¤í•‘ì„ ìë™ìœ¼ë¡œ ì œì•ˆ
    
    AI ê¸°ë°˜ ë§¤í•‘ ì œì•ˆ ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ìŠ¤í‚¤ë§ˆ(ìƒˆ ë°ì´í„°)ë¥¼
    íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆ(ê¸°ì¡´ ì˜¨í†¨ë¡œì§€)ì— ë§¤í•‘í•˜ëŠ” ì œì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Features:
    - ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­ (exact, fuzzy, phonetic)
    - íƒ€ì… ê¸°ë°˜ ë§¤ì¹­
    - ì˜ë¯¸ë¡ ì  ë§¤ì¹­ (ë„ë©”ì¸ ì§€ì‹)
    - ê°’ íŒ¨í„´ ê¸°ë°˜ ë§¤ì¹­
    - ì‹ ë¢°ë„ ì ìˆ˜ ì œê³µ
    """
    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        
        from bff.services.mapping_suggestion_service import MappingSuggestionService
        
        # ë§¤í•‘ ì œì•ˆ ì„œë¹„ìŠ¤ ì‚¬ìš© (êµ¬ì„± ê°€ëŠ¥í•œ ì„ê³„ê°’/ê°€ì¤‘ì¹˜)
        config = None
        try:
            # Try to load config from file
            import os
            config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'mapping_config.json')
            if os.path.exists(config_path):
                with open(config_path, 'r') as f:
                    config = json.load(f)
                logger.info(f"Loaded mapping config from {config_path}")
        except Exception as e:
            logger.warning(f"Failed to load mapping config: {e}, using defaults")
        
        suggestion_service = MappingSuggestionService(config=config)
        
        # Get target sample data if provided
        target_sample_data = request.target_sample_data
        
        # Note: For better accuracy, the frontend should fetch target sample data
        # when selecting a target ontology class and include it in the request.
        # This allows for value distribution similarity matching.
        
        if not target_sample_data:
            logger.info("No target sample data provided - value distribution matching will be skipped")
        
        suggestion = suggestion_service.suggest_mappings(
            source_schema=request.source_schema,
            target_schema=request.target_schema,
            sample_data=request.sample_data,
            target_sample_data=target_sample_data
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        return {
            "status": "success",
            "message": f"Found {len(suggestion.mappings)} mapping suggestions with {suggestion.overall_confidence:.2f} overall confidence",
            "mappings": [
                {
                    "source_field": m.source_field,
                    "target_field": m.target_field,
                    "confidence": m.confidence,
                    "match_type": m.match_type,
                    "reasons": m.reasons
                }
                for m in suggestion.mappings
            ],
            "unmapped_source_fields": suggestion.unmapped_source_fields,
            "unmapped_target_fields": suggestion.unmapped_target_fields,
            "overall_confidence": suggestion.overall_confidence,
            "statistics": {
                "total_source_fields": len(request.source_schema),
                "total_target_fields": len(request.target_schema),
                "mapped_fields": len(suggestion.mappings),
                "high_confidence_mappings": len([m for m in suggestion.mappings if m.confidence >= 0.8]),
                "medium_confidence_mappings": len([m for m in suggestion.mappings if 0.6 <= m.confidence < 0.8]),
            }
        }
    
    except Exception as e:
        logger.error(f"Mapping suggestion failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=f"ë§¤í•‘ ì œì•ˆ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/suggest-schema-from-google-sheets")
async def suggest_schema_from_google_sheets(
    db_name: str,
    request: SchemaFromGoogleSheetsRequest,
    terminus: TerminusService = TerminusServiceDep,
):
    """
    ğŸ”¥ Google Sheetsì—ì„œ ìŠ¤í‚¤ë§ˆ ìë™ ì œì•ˆ

    Google Sheets URLì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë¶„ì„í•˜ê³  OMS ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        from bff.services.funnel_client import FunnelClient

        # Funnel í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ Google Sheets ìŠ¤í‚¤ë§ˆ ì œì•ˆ ìš”ì²­
        async with FunnelClient() as funnel_client:
            result = await funnel_client.google_sheets_to_schema(
                sheet_url=request.sheet_url,
                worksheet_name=request.worksheet_name,
                class_name=request.class_name,
                api_key=request.api_key,
            )

            preview = result.get("preview", {})
            schema_suggestion = result.get("schema_suggestion", {})

            # ë¯¸ë¦¬ë³´ê¸° ì •ë³´ ìš”ì•½
            total_rows = preview.get("total_rows", 0)
            preview_rows = preview.get("preview_rows", 0)
            columns = preview.get("columns", [])

            return {
                "status": "success",
                "message": f"Google Sheets ìŠ¤í‚¤ë§ˆ ì œì•ˆì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. {total_rows}í–‰ ì¤‘ {preview_rows}í–‰ ë¶„ì„ë¨",
                "suggested_schema": schema_suggestion,
                "source_info": {
                    "sheet_url": request.sheet_url,
                    "worksheet_name": request.worksheet_name,
                    "total_rows": total_rows,
                    "preview_rows": preview_rows,
                    "columns": columns,
                },
                "preview_data": preview,
            }

    except Exception as e:
        logger.error(f"Google Sheets schema suggestion failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Google Sheets ìŠ¤í‚¤ë§ˆ ì œì•ˆ ì‹¤íŒ¨: {str(e)}",
        )


@router.post("/ontology/{class_id}/mapping-metadata")
async def save_mapping_metadata(
    db_name: str,
    class_id: str,
    metadata: Dict[str, Any],
    oms: OMSClient = OMSClientDep,
    mapper: LabelMapper = LabelMapperDep,
):
    """
    ë§¤í•‘ ë©”íƒ€ë°ì´í„°ë¥¼ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ì— ì €ì¥
    
    ë°ì´í„° ë§¤í•‘ ì´ë ¥ê³¼ í†µê³„ë¥¼ ClassMetadataì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        class_id = validate_class_id(class_id)
        sanitized_metadata = sanitize_input(metadata)
        
        # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        existing_metadata = await oms.get_class_metadata(db_name, class_id)
        
        # ë§¤í•‘ ì´ë ¥ ì—…ë°ì´íŠ¸
        mapping_history = existing_metadata.get("mapping_history", [])
        
        # ìƒˆ ë§¤í•‘ ì •ë³´ ì¶”ê°€
        new_mapping_entry = {
            "timestamp": sanitized_metadata.get("timestamp", datetime.now(timezone.utc).isoformat()),
            "source_file": sanitized_metadata.get("sourceFile", "unknown"),
            "mappings_count": sanitized_metadata.get("mappingsCount", 0),
            "average_confidence": sanitized_metadata.get("averageConfidence", 0),
            "mapping_details": sanitized_metadata.get("mappingDetails", [])
        }
        
        mapping_history.append(new_mapping_entry)
        
        # ìµœê·¼ 10ê°œì˜ ë§¤í•‘ ì´ë ¥ë§Œ ìœ ì§€
        if len(mapping_history) > 10:
            mapping_history = mapping_history[-10:]
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        updated_metadata = {
            **existing_metadata,
            "mapping_history": mapping_history,
            "last_mapping_date": new_mapping_entry["timestamp"],
            "total_mappings": sum(entry.get("mappings_count", 0) for entry in mapping_history),
            "mapping_sources": list(set(
                entry.get("source_file", "unknown") 
                for entry in mapping_history 
                if entry.get("source_file") != "unknown"
            ))
        }
        
        # OMSë¥¼ í†µí•´ ë©”íƒ€ë°ì´í„° ì €ì¥
        await oms.update_class_metadata(db_name, class_id, updated_metadata)
        
        logger.info(f"Saved mapping metadata for class {class_id}: {new_mapping_entry['mappings_count']} mappings from {new_mapping_entry['source_file']}")
        
        return {
            "status": "success",
            "message": "ë§¤í•‘ ë©”íƒ€ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤",
            "data": {
                "class_id": class_id,
                "mapping_entry": new_mapping_entry,
                "total_history_entries": len(mapping_history)
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to save mapping metadata: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë§¤í•‘ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}"
        )


# ==========================================
# CSV DATA IMPORT ENDPOINTS
# ==========================================
# TODO: Replace with actual Outbox Pattern + CQRS implementation
# ARCHITECTURE NOTES for future developers:
# ========================================
# 
# 1. OUTBOX PATTERN IMPLEMENTATION:
#    - Create PostgreSQL table: import_jobs
#    - Schema: job_id, database_name, class_id, csv_data, mappings, status, created_at
#    - Insert import job atomically with other operations
#    - Use database transactions to ensure consistency
#
# 2. EVENT STREAMING with KAFKA:
#    - Topic: "data-import-requests"
#    - Event Schema: { jobId, database, classId, csvData, mappings, timestamp }
#    - Producer: BFF service (this endpoint)
#    - Consumer: Import processing service
#
# 3. CQRS IMPLEMENTATION:
#    - COMMAND: Import job creation (handled by Kafka consumer)
#    - QUERY: Data retrieval from ElasticSearch + Cassandra
#    - Write Model: TerminusDB (source of truth)
#    - Read Models: ElasticSearch (search), Cassandra (bulk data)
#
# 4. SERVICES ARCHITECTURE:
#    BFF -> Outbox Table -> Kafka -> Import Service -> TerminusDB + ElasticSearch + Cassandra
#
# 5. PROGRESS TRACKING:
#    - WebSocket/SSE endpoint for real-time progress
#    - Redis for caching import job status
#    - Event-driven status updates
#
# 6. ERROR HANDLING:
#    - Dead letter queue for failed imports
#    - Retry mechanism with exponential backoff
#    - Detailed error logging and recovery

class CsvImportRequest(BaseModel):
    """Request model for CSV data import"""
    csv_data: List[Dict[str, Any]]
    mappings: Dict[str, Any]
    source_file: str
    total_records: int


@router.post("/import-csv-data")
async def import_csv_data(
    db_name: str,
    import_request: CsvImportRequest,
    oms: OMSClient = OMSClientDep
) -> Dict[str, Any]:
    """
    CSV ë°ì´í„°ë¥¼ ì˜¨í†¨ë¡œì§€ ì¸ìŠ¤í„´ìŠ¤ë¡œ ì„í¬íŠ¸
    
    TODO: ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Outbox Pattern + Event Streaming ì‚¬ìš©
    í˜„ì¬ëŠ” UX í”Œë¡œìš° ì™„ì„±ì„ ìœ„í•œ Placeholder êµ¬í˜„
    """
    try:
        # Input validation
        validate_db_name(db_name)
        
        if not import_request.csv_data:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="CSV ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"
            )
        
        logger.info(f"Starting CSV import for database {db_name}: {import_request.total_records} records from {import_request.source_file}")
        
        # TODO: Replace with Outbox Pattern
        # =====================================
        # STEP 1: Insert to outbox table (PostgreSQL)
        # import_job = await create_import_job({
        #     "database_name": db_name,
        #     "source_file": import_request.source_file,
        #     "total_records": import_request.total_records,
        #     "csv_data": import_request.csv_data,
        #     "mappings": import_request.mappings,
        #     "status": "pending",
        #     "created_at": datetime.utcnow()
        # })
        # 
        # STEP 2: Publish to Kafka
        # await kafka_producer.send("data-import-requests", {
        #     "job_id": import_job.id,
        #     "database": db_name,
        #     "class_id": import_request.mappings.get("target_class"),
        #     "csv_data": import_request.csv_data,
        #     "mappings": import_request.mappings,
        #     "timestamp": datetime.utcnow().isoformat()
        # })
        # 
        # STEP 3: Return job ID for progress tracking
        # return {
        #     "status": "accepted",
        #     "job_id": import_job.id,
        #     "message": "Import job queued successfully",
        #     "progress_url": f"/api/v1/import-progress/{import_job.id}"
        # }
        
        # PLACEHOLDER IMPLEMENTATION: Simulate async processing
        import uuid
        import asyncio
        
        job_id = str(uuid.uuid4())
        
        # Simulate processing delay
        await asyncio.sleep(0.1)
        
        # Mock processing results (95% success rate)
        success_count = int(import_request.total_records * 0.95)
        failed_count = import_request.total_records - success_count
        
        mock_errors = [
            f"Row {i}: Invalid data format" for i in range(1, failed_count + 1)
        ]
        
        logger.info(f"CSV import completed for job {job_id}: {success_count}/{import_request.total_records} records successful")
        
        return {
            "status": "completed",
            "job_id": job_id,
            "results": {
                "total_records": import_request.total_records,
                "success_count": success_count,
                "failed_count": failed_count,
                "errors": mock_errors[:3]  # Return first 3 errors
            },
            "message": f"Successfully imported {success_count} out of {import_request.total_records} records"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"CSV import failed for database {db_name}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"CSV ë°ì´í„° ì„í¬íŠ¸ ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/import-progress/{job_id}")
async def get_import_progress(
    db_name: str,
    job_id: str
) -> Dict[str, Any]:
    """
    ì„í¬íŠ¸ ì‘ì—… ì§„í–‰ ìƒí™© ì¡°íšŒ
    
    TODO: WebSocket/SSEë¡œ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì œê³µ
    Redisì—ì„œ ì§„í–‰ ìƒí™© ìºì‹œ ì¡°íšŒ
    """
    try:
        validate_db_name(db_name)
        
        # TODO: Replace with Redis cache lookup
        # progress_data = await redis_client.get(f"import_progress:{job_id}")
        # if not progress_data:
        #     raise HTTPException(404, "Import job not found")
        # return json.loads(progress_data)
        
        # PLACEHOLDER: Mock progress data
        return {
            "job_id": job_id,
            "status": "completed",
            "progress": 100,
            "current_step": "Import completed",
            "total_records": 500,
            "processed_records": 500,
            "success_count": 475,
            "failed_count": 25,
            "started_at": "2024-01-01T10:00:00Z",
            "completed_at": "2024-01-01T10:05:00Z"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get import progress for job {job_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„í¬íŠ¸ ì§„í–‰ ìƒí™© ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )
