"""
ì˜¨í†¨ë¡œì§€ CRUD ë¼ìš°í„°
ì˜¨í†¨ë¡œì§€ ìƒì„±, ì¡°íšŒ, ìˆ˜ì •, ì‚­ì œë¥¼ ë‹´ë‹¹
"""

import json
import logging
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, Depends, HTTPException, Query, Request, status
from pydantic import BaseModel

from shared.models.ontology import (
    OntologyBase,
    OntologyCreateRequestBFF,
    OntologyResponse,
    OntologyUpdateInput,
    Property,
    Relationship,
)

# Add shared path for common utilities
from shared.utils.language import get_accept_language

# Security validation imports
from shared.security.input_sanitizer import sanitize_input, validate_db_name, validate_class_id


# Schema suggestion request models
class SchemaFromDataRequest(BaseModel):
    """Request model for schema suggestion from data"""

    data: List[List[Any]]
    columns: List[str]
    class_name: Optional[str] = None
    include_complex_types: bool = False


class SchemaFromGoogleSheetsRequest(BaseModel):
    """Request model for schema suggestion from Google Sheets"""

    sheet_url: str
    worksheet_name: Optional[str] = None
    class_name: Optional[str] = None
    api_key: Optional[str] = None


from fastapi import HTTPException

from bff.dependencies import (
    JSONToJSONLDConverter,
    LabelMapper,
    TerminusService,
    get_jsonld_converter,
    get_label_mapper,
    get_terminus_service,
    get_oms_client,
)
from bff.services.adapter_service import BFFAdapterService
from bff.services.oms_client import OMSClient

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/database/{db_name}", tags=["Ontology Management"])


# Dependency for BFF Adapter Service
def get_bff_adapter(
    terminus_service: TerminusService = Depends(get_terminus_service),
    label_mapper: LabelMapper = Depends(get_label_mapper)
) -> BFFAdapterService:
    """Get BFF Adapter Service instance"""
    return BFFAdapterService(terminus_service, label_mapper)


@router.post("/ontology", response_model=OntologyResponse)
async def create_ontology(
    db_name: str,
    ontology: Dict[str, Any],  # Accept raw dict to preserve target field
    mapper: LabelMapper = Depends(get_label_mapper),
    terminus: TerminusService = Depends(get_terminus_service),
    jsonld_conv: JSONToJSONLDConverter = Depends(get_jsonld_converter),
):
    """
    ì˜¨í†¨ë¡œì§€ ìƒì„±

    ìƒˆë¡œìš´ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ë ˆì´ë¸” ê¸°ë°˜ìœ¼ë¡œ IDê°€ ìë™ ìƒì„±ë©ë‹ˆë‹¤.
    """
    # ğŸ”¥ ULTRA DEBUG! Force logging to check if route is called
    print(f"ğŸ”¥ğŸ”¥ğŸ”¥ CREATE_ONTOLOGY CALLED! db_name={db_name}, ontology={ontology}")
    logger.warning(f"ğŸ”¥ğŸ”¥ğŸ”¥ CREATE_ONTOLOGY CALLED! db_name={db_name}, ontology={ontology}")
    
    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        
        # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ (ì´ë¯¸ dict í˜•íƒœ)
        ontology_dict = ontology.copy()  # Make a copy to avoid modifying the original
        ontology_dict = sanitize_input(ontology_dict)

        # ID ìƒì„± ë˜ëŠ” ê²€ì¦
        if ontology_dict.get('id'):
            # ì‚¬ìš©ìê°€ IDë¥¼ ì œê³µí•œ ê²½ìš° ê²€ì¦
            class_id = validate_class_id(ontology_dict['id'])
            logger.info(f"Using provided class_id '{class_id}'")
        else:
            # IDê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ìƒì„±
            from shared.utils.id_generator import generate_simple_id
            class_id = generate_simple_id(
                label=ontology_dict.get('label', ''), use_timestamp_for_korean=True, default_fallback="UnnamedClass"
            )
            logger.info(f"Generated class_id '{class_id}' from label '{ontology_dict.get('label', '')}')")

        ontology_dict["id"] = class_id

        # ğŸ”¥ THINK ULTRA! Transform properties for OMS compatibility
        # Convert 'target' to 'linkTarget' for link-type properties
        def transform_properties_for_oms(data):
            if 'properties' in data and isinstance(data['properties'], list):
                for i, prop in enumerate(data['properties']):
                    if isinstance(prop, dict):
                        # ğŸ”¥ ULTRA! BFF uses 'label' but OMS requires 'name'
                        if 'name' not in prop and 'label' in prop:
                            # Generate name from label
                            prop['name'] = generate_simple_id(prop['label'], use_timestamp_for_korean=False)
                            logger.info(f"ğŸ”¥ Generated property name '{prop['name']}' from label '{prop['label']}'")
                        
                        # ğŸ”¥ ULTRA! Convert STRING to xsd:string for OMS
                        if prop.get('type') == 'STRING':
                            prop['type'] = 'xsd:string'
                        elif prop.get('type') == 'INTEGER':
                            prop['type'] = 'xsd:integer'
                        elif prop.get('type') == 'DECIMAL':
                            prop['type'] = 'xsd:decimal'
                        elif prop.get('type') == 'BOOLEAN':
                            prop['type'] = 'xsd:boolean'
                        elif prop.get('type') == 'DATETIME':
                            prop['type'] = 'xsd:dateTime'
                        
                        # Convert target to linkTarget for link type properties
                        if prop.get('type') == 'link' and 'target' in prop:
                            prop['linkTarget'] = prop.pop('target')
                            logger.info(f"ğŸ”§ Converted property '{prop.get('name')}' target -> linkTarget: {prop.get('linkTarget')}")
                        
                        # Handle array properties with link items
                        if prop.get('type') == 'array' and 'items' in prop:
                            items = prop['items']
                            if isinstance(items, dict) and items.get('type') == 'link' and 'target' in items:
                                items['linkTarget'] = items.pop('target')
                                logger.info(f"ğŸ”§ Converted array property '{prop.get('name')}' items target -> linkTarget: {items.get('linkTarget')}")
        
        # Apply transformation
        transform_properties_for_oms(ontology_dict)
        
        # ğŸ”¥ ULTRA DEBUG! Write to file to verify transformation
        import datetime
        debug_file = f"/tmp/bff_debug_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(debug_file, 'w') as f:
            f.write(f"BEFORE transformation: {json.dumps(ontology, ensure_ascii=False, indent=2)}\n\n")
            f.write(f"AFTER transformation: {json.dumps(ontology_dict, ensure_ascii=False, indent=2)}\n")
        
        # Log transformed data for debugging
        logger.info(f"ğŸ”¥ ULTRA DEBUG! Sending to OMS: {json.dumps(ontology_dict, ensure_ascii=False, indent=2)}")
        
        # ì˜¨í†¨ë¡œì§€ ìƒì„±
        result = await terminus.create_class(db_name, ontology_dict)
        logger.info(f"OMS create result: {result}")

        # ë ˆì´ë¸” ë§¤í•‘ ë“±ë¡
        await mapper.register_class(db_name, class_id, ontology_dict.get('label', ''), ontology_dict.get('description'))

        # ì†ì„± ë ˆì´ë¸” ë§¤í•‘
        for prop in ontology_dict.get('properties', []):
            if isinstance(prop, dict):
                await mapper.register_property(db_name, class_id, prop.get('name', ''), prop.get('label', ''))

        # ê´€ê³„ ë ˆì´ë¸” ë§¤í•‘
        for rel in ontology_dict.get('relationships', []):
            if isinstance(rel, dict):
                await mapper.register_relationship(db_name, rel.get('predicate', ''), rel.get('label', ''))

        # OMS ì‘ë‹µì—ì„œ ìƒì„±ëœ ë°ì´í„° ì¶”ì¶œ
        if isinstance(result, dict):
            # OMSê°€ data í•„ë“œë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°
            if "data" in result and result.get("status") == "success":
                created_data = result["data"]
                # ìƒì„±ëœ ID ì‚¬ìš©
                if "id" in created_data:
                    class_id = created_data["id"]
            elif "id" in result:
                # ì§ì ‘ ë°ì´í„° í˜•ì‹ì¸ ê²½ìš°
                created_data = result
                class_id = result["id"]

        # ì‘ë‹µ ìƒì„±
        # Validate properties and relationships to ensure they're properly formatted
        validated_properties = []
        for prop in ontology_dict.get('properties', []):
            if isinstance(prop, dict):
                validated_properties.append(Property(**prop))
                
        validated_relationships = []
        for rel in ontology_dict.get('relationships', []):
            if isinstance(rel, dict):
                validated_relationships.append(Relationship(**rel))
        
        # Create response
        return OntologyResponse(
            id=class_id,
            label=ontology_dict.get('label', ''),
            description=ontology_dict.get('description'),
            parent_class=ontology_dict.get('parent_class'),
            abstract=ontology_dict.get('abstract', False),
            properties=validated_properties,
            relationships=validated_relationships,
            metadata={"created": True, "database": db_name}
        )

    except HTTPException as he:
        # ğŸ”¥ ULTRA! Properly propagate HTTP exceptions with correct status codes
        logger.error(f"HTTP exception in create_ontology: {he.status_code} - {he.detail}")
        raise he
    except Exception as e:
        logger.error(f"Failed to create ontology: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/ontology/list")
async def list_ontologies(
    db_name: str,
    request: Request,
    class_type: str = Query("sys:Class", description="í´ë˜ìŠ¤ íƒ€ì…"),
    limit: Optional[int] = Query(None, description="ê²°ê³¼ ê°œìˆ˜ ì œí•œ"),
    offset: int = Query(0, description="ì˜¤í”„ì…‹"),
    mapper: LabelMapper = Depends(get_label_mapper),
    terminus: TerminusService = Depends(get_terminus_service),
):
    """
    ì˜¨í†¨ë¡œì§€ ëª©ë¡ ì¡°íšŒ

    ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ì˜¨í†¨ë¡œì§€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        # ì˜¨í†¨ë¡œì§€ ëª©ë¡ ì¡°íšŒ
        ontologies = await terminus.list_classes(db_name)
        
        # ğŸ”¥ ULTRA DEBUG!
        logger.info(f"ğŸ”¥ ontologies type: {type(ontologies)}")
        logger.info(f"ğŸ”¥ ontologies content: {ontologies[:2] if ontologies else 'empty'}")

        # ë°°ì¹˜ ë ˆì´ë¸” ì •ë³´ ì¶”ê°€ (N+1 ì¿¼ë¦¬ ë¬¸ì œ í•´ê²°)
        # ğŸ”¥ ULTRA! Skip label mapping for now to fix the issue
        labeled_ontologies = ontologies  # await mapper.convert_to_display_batch(db_name, ontologies, lang)

        return {
            "total": len(labeled_ontologies),
            "ontologies": labeled_ontologies,
            "offset": offset,
            "limit": limit,
        }

    except HTTPException as he:
        # ğŸ”¥ ULTRA! Properly propagate HTTP exceptions
        logger.error(f"HTTP exception in list_ontologies: {he.status_code} - {he.detail}")
        raise he
    except Exception as e:
        logger.error(f"Failed to list ontologies: {e}")
        # ğŸ”¥ ULTRA! Check if it's a 404 from OMS
        if "404" in str(e) or "not found" in str(e).lower():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ë°ì´í„°ë² ì´ìŠ¤ '{db_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/ontology/{class_label}", response_model=OntologyResponse)
async def get_ontology(
    db_name: str,
    class_label: str,
    request: Request,
    mapper: LabelMapper = Depends(get_label_mapper),
    terminus: TerminusService = Depends(get_terminus_service),
):
    """
    ì˜¨í†¨ë¡œì§€ ì¡°íšŒ

    ë ˆì´ë¸” ë˜ëŠ” IDë¡œ ì˜¨í†¨ë¡œì§€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        class_label = sanitize_input(class_label)
        # URL íŒŒë¼ë¯¸í„° ë””ë²„ê¹…
        logger.info(
            f"Getting ontology - db_name: {db_name}, class_label: {class_label}, lang: {lang}"
        )

        # ë ˆì´ë¸”ë¡œ ID ì¡°íšŒ ì‹œë„
        class_id = await mapper.get_class_id(db_name, class_label, lang)
        logger.info(f"Mapped label '{class_label}' to class_id: {class_id}")

        # IDê°€ ì—†ìœ¼ë©´ ì…ë ¥ê°’ì„ IDë¡œ ê°„ì£¼
        if not class_id:
            class_id = class_label
            logger.info(f"No mapping found, using label as ID: {class_id}")

        # ì˜¨í†¨ë¡œì§€ ì¡°íšŒ
        result = await terminus.get_class(db_name, class_id)

        if not result:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ì˜¨í†¨ë¡œì§€ '{class_label}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            )

        logger.info(f"Raw result from terminus.get_class: {result}")

        # Extract ontology data (terminus.get_class already extracts from data field)
        ontology_data = result if isinstance(result, dict) else {}

        # ë ˆì´ë¸” ì •ë³´ ì¶”ê°€
        ontology_data = await mapper.convert_to_display(db_name, ontology_data, lang)
        logger.info(f"After convert_to_display: {ontology_data}")

        # Ensure result has required fields for OntologyResponse
        if not ontology_data.get("id"):
            ontology_data["id"] = class_id
        if not ontology_data.get("label"):
            # Try to get label from the mapper
            label = await mapper.get_class_label(db_name, class_id, lang)
            ontology_data["label"] = label or class_label

        # Remove extra fields that are not part of OntologyBase
        # Ensure label and description are simple strings
        label_value = ontology_data.get("label")
        if isinstance(label_value, dict):
            # Extract string from dict (fallback to English if available)
            label_value = label_value.get(lang) or label_value.get("en") or label_value.get("ko") or class_label
        elif not isinstance(label_value, str):
            label_value = str(label_value) if label_value else class_label
        
        desc_value = ontology_data.get("description")
        if isinstance(desc_value, dict):
            # Extract string from dict (fallback to English if available)
            desc_value = desc_value.get(lang) or desc_value.get("en") or desc_value.get("ko") or ""
        elif not isinstance(desc_value, str) and desc_value:
            desc_value = str(desc_value)
        
        ontology_base_data = {
            "id": ontology_data.get("id"),
            "label": label_value,
            "description": desc_value,
            "properties": ontology_data.get("properties", []),
            "relationships": ontology_data.get("relationships", []),
            "parent_class": ontology_data.get("parent_class"),
            "abstract": ontology_data.get("abstract", False),
            "metadata": ontology_data.get("metadata", {}),
        }

        # OntologyResponse inherits from OntologyBase, so pass fields directly
        return OntologyResponse(**ontology_base_data)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get ontology: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}",
        )


@router.put("/ontology/{class_label}")
async def update_ontology(
    db_name: str,
    class_label: str,
    ontology: OntologyUpdateInput,
    request: Request,
    mapper: LabelMapper = Depends(get_label_mapper),
    terminus: TerminusService = Depends(get_terminus_service),
):
    """
    ì˜¨í†¨ë¡œì§€ ìˆ˜ì •

    ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        class_label = sanitize_input(class_label)
        # ë ˆì´ë¸”ë¡œ ID ì¡°íšŒ
        class_id = await mapper.get_class_id(db_name, class_label, lang)
        if not class_id:
            class_id = class_label

        # ì—…ë°ì´íŠ¸ ë°ì´í„° ì¤€ë¹„
        update_data = ontology.dict(exclude_unset=True)
        update_data["id"] = class_id

        # ì˜¨í†¨ë¡œì§€ ì—…ë°ì´íŠ¸
        await terminus.update_class(db_name, class_id, update_data)

        # ë ˆì´ë¸” ë§¤í•‘ ì—…ë°ì´íŠ¸
        await mapper.update_mappings(db_name, update_data)

        return {
            "message": f"ì˜¨í†¨ë¡œì§€ '{class_label}'ì´(ê°€) ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤",
            "id": class_id,
            "updated_fields": list(update_data.keys()),
        }

    except HTTPException as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ì˜¨í†¨ë¡œì§€ '{e.ontology_id}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
        )
    except HTTPException as e:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e.message}",
        )
    except Exception as e:
        logger.error(f"Failed to update ontology: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}",
        )


@router.delete("/ontology/{class_label}")
async def delete_ontology(
    db_name: str,
    class_label: str,
    request: Request,
    mapper: LabelMapper = Depends(get_label_mapper),
    terminus: TerminusService = Depends(get_terminus_service),
):
    """
    ì˜¨í†¨ë¡œì§€ ì‚­ì œ

    ì˜¨í†¨ë¡œì§€ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    ì£¼ì˜: ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        class_label = sanitize_input(class_label)
        # ë ˆì´ë¸”ë¡œ ID ì¡°íšŒ
        class_id = await mapper.get_class_id(db_name, class_label, lang)
        if not class_id:
            class_id = class_label

        # ì˜¨í†¨ë¡œì§€ ì‚­ì œ
        await terminus.delete_class(db_name, class_id)

        # ë ˆì´ë¸” ë§¤í•‘ ì‚­ì œ
        await mapper.remove_class(db_name, class_id)

        return {"message": f"ì˜¨í†¨ë¡œì§€ '{class_label}'ì´(ê°€) ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤", "id": class_id}

    except HTTPException as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ì˜¨í†¨ë¡œì§€ '{e.ontology_id}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
        )
    except Exception as e:
        logger.error(f"Failed to delete ontology: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ì‚­ì œ ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/ontology/{class_id}/schema")
async def get_ontology_schema(
    db_name: str,
    class_id: str,
    request: Request,
    format: str = Query("json", description="ìŠ¤í‚¤ë§ˆ í˜•ì‹ (json, jsonld, owl)"),
    mapper: LabelMapper = Depends(get_label_mapper),
    terminus: TerminusService = Depends(get_terminus_service),
    jsonld_conv: JSONToJSONLDConverter = Depends(get_jsonld_converter),
):
    """
    ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ

    ì˜¨í†¨ë¡œì§€ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        class_id = sanitize_input(class_id)
        # ë ˆì´ë¸”ë¡œ ID ì¡°íšŒ
        actual_id = await mapper.get_class_id(db_name, class_id, lang)
        if not actual_id:
            actual_id = class_id

        # ì˜¨í†¨ë¡œì§€ ì¡°íšŒ
        ontology = await terminus.get_class(db_name, actual_id)

        if not ontology:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ì˜¨í†¨ë¡œì§€ '{class_id}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            )

        # í˜•ì‹ë³„ ë³€í™˜
        if format == "jsonld":
            schema = jsonld_conv.convert_to_jsonld(ontology, db_name)
        elif format == "owl":
            # OWL í˜•ì‹ì€ JSON-LDë¡œ ëŒ€ì²´ (RDF í˜¸í™˜)
            schema = jsonld_conv.convert_to_jsonld(ontology, db_name)
            # OWL í˜•ì‹ í‘œì‹œë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if isinstance(schema, dict):
                schema["@comment"] = "This is a JSON-LD representation compatible with OWL"
                schema["@owl:versionInfo"] = "1.0"
        else:
            # ê¸°ë³¸ JSON
            schema = ontology

        return schema

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get ontology schema: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}",
        )


# ğŸ”¥ THINK ULTRA! BFF Enhanced Relationship Management Endpoints


@router.post("/ontology-advanced", response_model=OntologyResponse)
async def create_ontology_with_relationship_validation(
    db_name: str,
    ontology: OntologyCreateRequestBFF,
    adapter: BFFAdapterService = Depends(get_bff_adapter),
):
    """
    ğŸ”¥ ê³ ê¸‰ ê´€ê³„ ê²€ì¦ì„ í¬í•¨í•œ ì˜¨í†¨ë¡œì§€ ìƒì„± (BFF ë ˆì´ì–´ - ë¦¬íŒ©í† ë§ë¨)

    ì´ì œ ì¤‘ë³µ ë¡œì§ ì—†ì´ BFF Adapterë¥¼ í†µí•´ OMSë¡œ ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìœ„ì„í•©ë‹ˆë‹¤.
    
    Features:
    - ë ˆì´ë¸” ê¸°ë°˜ ìë™ ID ìƒì„±
    - ìë™ ì—­ê´€ê³„ ìƒì„±
    - ê´€ê³„ ê²€ì¦ ë° ë¬´ê²°ì„± ì²´í¬
    - ìˆœí™˜ ì°¸ì¡° íƒì§€
    - ë‹¤êµ­ì–´ ë ˆì´ë¸” ë§¤í•‘
    """
    try:
        # BFF Adapterë¥¼ í†µí•´ ëª¨ë“  ë¡œì§ ìœ„ì„ (ì¤‘ë³µ ì œê±°)
        return await adapter.create_advanced_ontology(
            db_name=db_name,
            ontology_data=ontology.dict(exclude_unset=True),
            language="ko"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to create ontology with advanced relationships: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê³ ê¸‰ ì˜¨í†¨ë¡œì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}",
        )


@router.post("/validate-relationships")
async def validate_ontology_relationships_bff(
    db_name: str,
    ontology: OntologyCreateRequestBFF,
    adapter: BFFAdapterService = Depends(get_bff_adapter),
):
    """
    ğŸ”¥ ì˜¨í†¨ë¡œì§€ ê´€ê³„ ê²€ì¦ (BFF ë ˆì´ì–´ - ë¦¬íŒ©í† ë§ë¨)

    ì´ì œ ì¤‘ë³µ ë¡œì§ ì—†ì´ BFF Adapterë¥¼ í†µí•´ OMSë¡œ ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìœ„ì„í•©ë‹ˆë‹¤.
    """
    try:
        # BFF Adapterë¥¼ í†µí•´ ëª¨ë“  ë¡œì§ ìœ„ì„ (ì¤‘ë³µ ì œê±°)
        return await adapter.validate_relationships(
            db_name=db_name,
            validation_data=ontology.dict(exclude_unset=True),
            language="ko"
        )

    except HTTPException:
        raise

    except Exception as e:
        logger.error(f"Failed to validate relationships: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"ê´€ê³„ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/check-circular-references")
async def check_circular_references_bff(
    db_name: str,
    ontology: Optional[OntologyCreateRequestBFF] = None,
    adapter: BFFAdapterService = Depends(get_bff_adapter),
):
    """
    ğŸ”¥ ìˆœí™˜ ì°¸ì¡° íƒì§€ (BFF ë ˆì´ì–´ - ë¦¬íŒ©í† ë§ë¨)

    ì´ì œ ì¤‘ë³µ ë¡œì§ ì—†ì´ BFF Adapterë¥¼ í†µí•´ OMSë¡œ ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìœ„ì„í•©ë‹ˆë‹¤.
    """
    try:
        # ìƒˆ ì˜¨í†¨ë¡œì§€ ë°ì´í„° ì¤€ë¹„
        detection_data = {}
        if ontology:
            detection_data = ontology.dict(exclude_unset=True)

        # BFF Adapterë¥¼ í†µí•´ ëª¨ë“  ë¡œì§ ìœ„ì„ (ì¤‘ë³µ ì œê±°)
        return await adapter.detect_circular_references(
            db_name=db_name,
            detection_data=detection_data,
            language="ko"
        )

    except Exception as e:
        logger.error(f"Failed to check circular references: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìˆœí™˜ ì°¸ì¡° íƒì§€ ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/relationship-network/analyze")
async def analyze_relationship_network_bff(
    db_name: str,
    request: Request,
    terminus: TerminusService = Depends(get_terminus_service),
    mapper: LabelMapper = Depends(get_label_mapper),
):
    """
    ğŸ”¥ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ (BFF ë ˆì´ì–´)

    ì „ì²´ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ì˜ ê±´ê°•ì„±ì„ ë¶„ì„í•˜ê³  ì‚¬ìš©ì ì¹œí™”ì  ê²°ê³¼ ë°˜í™˜
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìˆ˜í–‰
        analysis_result = await terminus.analyze_relationship_network(db_name)

        # ì‚¬ìš©ì ì¹œí™”ì  í˜•íƒœë¡œ ë³€í™˜
        summary = analysis_result.get("relationship_summary", {})
        validation = analysis_result.get("validation_summary", {})
        cycles = analysis_result.get("cycle_analysis", {})
        graph = analysis_result.get("graph_summary", {})
        recommendations = analysis_result.get("recommendations", [])

        # ê±´ê°•ì„± ì ìˆ˜ ê³„ì‚° (0-100)
        health_score = 100
        if validation.get("errors", 0) > 0:
            health_score -= validation["errors"] * 10
        if cycles.get("critical_cycles", 0) > 0:
            health_score -= cycles["critical_cycles"] * 15
        if validation.get("warnings", 0) > 0:
            health_score -= validation["warnings"] * 2
        health_score = max(0, min(100, health_score))

        # ê±´ê°•ì„± ë“±ê¸‰ ê²°ì •
        if health_score >= 90:
            health_grade = "Excellent"
            health_color = "green"
        elif health_score >= 70:
            health_grade = "Good"
            health_color = "blue"
        elif health_score >= 50:
            health_grade = "Fair"
            health_color = "yellow"
        else:
            health_grade = "Poor"
            health_color = "red"

        return {
            "status": "success",
            "message": "ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "network_health": {
                "score": health_score,
                "grade": health_grade,
                "color": health_color,
                "description": f"ë„¤íŠ¸ì›Œí¬ ê±´ê°•ì„±ì´ {health_grade.lower()} ìƒíƒœì…ë‹ˆë‹¤",
            },
            "statistics": {
                "total_ontologies": analysis_result.get("ontology_count", 0),
                "total_relationships": summary.get("total_relationships", 0),
                "relationship_types": len(graph.get("relationship_types", [])),
                "total_entities": graph.get("total_entities", 0),
                "average_connections": round(graph.get("average_connections_per_entity", 0), 2),
            },
            "quality_metrics": {
                "validation_errors": validation.get("errors", 0),
                "validation_warnings": validation.get("warnings", 0),
                "critical_cycles": cycles.get("critical_cycles", 0),
                "total_cycles": cycles.get("total_cycles", 0),
                "inverse_coverage": summary.get("inverse_coverage", "0/0 (0%)"),
            },
            "recommendations": [
                {
                    "priority": "high" if "âŒ" in rec else "medium" if "âš ï¸" in rec else "low",
                    "message": rec.replace("âŒ ", "")
                    .replace("âš ï¸ ", "")
                    .replace("ğŸ“ ", "")
                    .replace("ğŸ”„ ", ""),
                }
                for rec in recommendations
            ],
            "metadata": {
                "analysis_timestamp": analysis_result.get("analysis_timestamp"),
                "database": db_name,
                "language": lang,
            },
        }

    except Exception as e:
        logger.error(f"Failed to analyze relationship network: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
        )


@router.get("/relationship-paths")
async def find_relationship_paths_bff(
    request: Request,
    db_name: str,
    start_entity: str,
    end_entity: Optional[str] = Query(
        None, description="ëª©í‘œ ì—”í‹°í‹° (ì—†ìœ¼ë©´ ëª¨ë“  ë„ë‹¬ ê°€ëŠ¥í•œ ì—”í‹°í‹°)"
    ),
    max_depth: int = Query(3, ge=1, le=5, description="ìµœëŒ€ íƒìƒ‰ ê¹Šì´"),
    path_type: str = Query("shortest", description="ê²½ë¡œ íƒ€ì… (shortest, all, weighted, semantic)"),
    terminus: TerminusService = Depends(get_terminus_service),
    mapper: LabelMapper = Depends(get_label_mapper),
):
    """
    ğŸ”¥ ê´€ê³„ ê²½ë¡œ íƒìƒ‰ (BFF ë ˆì´ì–´)

    ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ ê²½ë¡œë¥¼ ì°¾ì•„ ì‚¬ìš©ì ì¹œí™”ì  í˜•íƒœë¡œ ë°˜í™˜
    """
    lang = get_accept_language(request)

    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        start_entity = sanitize_input(start_entity)
        if end_entity:
            end_entity = sanitize_input(end_entity)
        # ë ˆì´ë¸”ì„ IDë¡œ ë³€í™˜
        start_id = await mapper.get_class_id(db_name, start_entity, lang)
        if not start_id:
            start_id = start_entity

        end_id = None
        if end_entity:
            end_id = await mapper.get_class_id(db_name, end_entity, lang)
            if not end_id:
                end_id = end_entity

        # ê²½ë¡œ íƒìƒ‰ ìˆ˜í–‰
        path_result = await terminus.find_relationship_paths(
            db_name=db_name,
            start_entity=start_id,
            end_entity=end_id,
            max_depth=max_depth,
            path_type=path_type,
        )

        # IDë¥¼ ë ˆì´ë¸”ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©ì ì¹œí™”ì  ê²°ê³¼ ìƒì„±
        paths = path_result.get("paths", [])
        user_friendly_paths = []

        for path in paths:
            # ì—”í‹°í‹° IDë“¤ì„ ë ˆì´ë¸”ë¡œ ë³€í™˜
            entity_labels = []
            for entity_id in path.get("entities", []):
                label = await mapper.get_class_label(db_name, entity_id, lang)
                entity_labels.append(label or entity_id)

            user_friendly_paths.append(
                {
                    "start": entity_labels[0] if entity_labels else start_entity,
                    "end": (
                        entity_labels[-1]
                        if len(entity_labels) > 1
                        else entity_labels[0] if entity_labels else ""
                    ),
                    "path": " â†’ ".join(entity_labels),
                    "predicates": path.get("predicates", []),
                    "length": path.get("length", 0),
                    "weight": round(path.get("total_weight", 0), 2),
                    "confidence": path.get("confidence", 1.0),
                    "path_type": path.get("path_type", "unknown"),
                }
            )

        statistics = path_result.get("statistics", {})

        return {
            "status": "success",
            "message": f"ê´€ê³„ ê²½ë¡œ íƒìƒ‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({len(user_friendly_paths)}ê°œ ê²½ë¡œ ë°œê²¬)",
            "query": {
                "start_entity": start_entity,
                "end_entity": end_entity,
                "max_depth": max_depth,
                "path_type": path_type,
            },
            "paths": user_friendly_paths[:10],  # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜
            "statistics": {
                "total_paths_found": len(paths),
                "displayed_paths": min(len(user_friendly_paths), 10),
                "average_length": statistics.get("average_length", 0),
                "shortest_path_length": statistics.get("min_length", 0),
                "longest_path_length": statistics.get("max_length", 0),
            },
            "metadata": {"language": lang, "start_id": start_id, "end_id": end_id},
        }

    except Exception as e:
        logger.error(f"Failed to find relationship paths: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê´€ê³„ ê²½ë¡œ íƒìƒ‰ ì‹¤íŒ¨: {str(e)}",
        )


@router.post("/suggest-schema-from-data")
async def suggest_schema_from_data(
    db_name: str,
    request: SchemaFromDataRequest,
    terminus: TerminusService = Depends(get_terminus_service),
):
    """
    ğŸ”¥ ë°ì´í„°ì—ì„œ ìŠ¤í‚¤ë§ˆ ìë™ ì œì•ˆ

    ì œê³µëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ OMS ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ëŠ” Funnel ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ê¸°ëŠ¥ì„ í†µí•©í•œ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.
    """
    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        from bff.services.funnel_client import FunnelClient

        # Funnel í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ ìŠ¤í‚¤ë§ˆ ì œì•ˆ ìš”ì²­
        async with FunnelClient() as funnel_client:
            # ë°ì´í„° ë¶„ì„ ë° ìŠ¤í‚¤ë§ˆ ì œì•ˆì„ í•œ ë²ˆì— ìˆ˜í–‰
            result = await funnel_client.analyze_and_suggest_schema(
                data=request.data,
                columns=request.columns,
                class_name=request.class_name,
                include_complex_types=request.include_complex_types,
            )

            analysis = result.get("analysis", {})
            schema_suggestion = result.get("schema_suggestion", {})

            # ë¶„ì„ ê²°ê³¼ ìš”ì•½
            analyzed_columns = analysis.get("columns", [])
            high_confidence_types = [
                col
                for col in analyzed_columns
                if col.get("inferred_type", {}).get("confidence", 0) >= 0.7
            ]

            return {
                "status": "success",
                "message": f"ìŠ¤í‚¤ë§ˆ ì œì•ˆì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. {len(high_confidence_types)}/{len(analyzed_columns)} ì»¬ëŸ¼ì— ëŒ€í•´ ë†’ì€ ì‹ ë¢°ë„ íƒ€ì… ì¶”ë¡  ì„±ê³µ",
                "suggested_schema": schema_suggestion,
                "analysis_summary": {
                    "total_columns": len(analyzed_columns),
                    "high_confidence_columns": len(high_confidence_types),
                    "average_confidence": (
                        round(
                            sum(
                                col.get("inferred_type", {}).get("confidence", 0)
                                for col in analyzed_columns
                            )
                            / len(analyzed_columns),
                            2,
                        )
                        if analyzed_columns
                        else 0
                    ),
                    "detected_types": list(
                        set(
                            col.get("inferred_type", {}).get("type", "unknown")
                            for col in analyzed_columns
                        )
                    ),
                },
                "detailed_analysis": analysis,
            }

    except Exception as e:
        logger.error(f"Schema suggestion failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"ìŠ¤í‚¤ë§ˆ ì œì•ˆ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/suggest-schema-from-google-sheets")
async def suggest_schema_from_google_sheets(
    db_name: str,
    request: SchemaFromGoogleSheetsRequest,
    terminus: TerminusService = Depends(get_terminus_service),
):
    """
    ğŸ”¥ Google Sheetsì—ì„œ ìŠ¤í‚¤ë§ˆ ìë™ ì œì•ˆ

    Google Sheets URLì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë¶„ì„í•˜ê³  OMS ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # ì…ë ¥ ë°ì´í„° ë³´ì•ˆ ê²€ì¦
        db_name = validate_db_name(db_name)
        from bff.services.funnel_client import FunnelClient

        # Funnel í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ Google Sheets ìŠ¤í‚¤ë§ˆ ì œì•ˆ ìš”ì²­
        async with FunnelClient() as funnel_client:
            result = await funnel_client.google_sheets_to_schema(
                sheet_url=request.sheet_url,
                worksheet_name=request.worksheet_name,
                class_name=request.class_name,
                api_key=request.api_key,
            )

            preview = result.get("preview", {})
            schema_suggestion = result.get("schema_suggestion", {})

            # ë¯¸ë¦¬ë³´ê¸° ì •ë³´ ìš”ì•½
            total_rows = preview.get("total_rows", 0)
            preview_rows = preview.get("preview_rows", 0)
            columns = preview.get("columns", [])

            return {
                "status": "success",
                "message": f"Google Sheets ìŠ¤í‚¤ë§ˆ ì œì•ˆì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. {total_rows}í–‰ ì¤‘ {preview_rows}í–‰ ë¶„ì„ë¨",
                "suggested_schema": schema_suggestion,
                "source_info": {
                    "sheet_url": request.sheet_url,
                    "worksheet_name": request.worksheet_name,
                    "total_rows": total_rows,
                    "preview_rows": preview_rows,
                    "columns": columns,
                },
                "preview_data": preview,
            }

    except Exception as e:
        logger.error(f"Google Sheets schema suggestion failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Google Sheets ìŠ¤í‚¤ë§ˆ ì œì•ˆ ì‹¤íŒ¨: {str(e)}",
        )
