version: '3.8'

services:
  # PostgreSQL - 표준화된 인증 정보
  postgres:
    image: postgres:16-alpine
    container_name: spice_postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
    ports:
      - "${POSTGRES_PORT_HOST:-5433}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d
    networks:
      spice_network:
        aliases:
          - spice-minio
          - spice_minio
          - minio
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-spiceadmin} -d ${POSTGRES_DB:-spicedb}"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Redis - 표준화된 인증 정보
  redis:
    image: redis:7-alpine
    container_name: spice_redis
    command: redis-server --requirepass ${REDIS_PASSWORD:-spicepass123}
    ports:
      - "${REDIS_PORT_HOST:-6379}:6379"
    volumes:
      - redis_data:/data
    networks:
      - spice_network
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a ${REDIS_PASSWORD:-spicepass123} ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Elasticsearch - 표준화된 인증 정보
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.0
    container_name: spice_elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms256m -Xmx256m"
      - http.host=0.0.0.0
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "${ELASTICSEARCH_PORT_HOST:-9200}:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - spice_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: spice_kafka
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # Use 127.0.0.1 instead of localhost to avoid IPv6 (::1) bootstrap issues on some hosts/CI runners.
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://127.0.0.1:${KAFKA_PORT_HOST:-39092}
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    ports:
      - "${KAFKA_PORT_HOST:-39092}:9092"
    depends_on:
      - zookeeper
    networks:
      - spice_network
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "kafka:29092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: spice_zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "${ZOOKEEPER_PORT_HOST:-2181}:2181"
    networks:
      - spice_network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 5

  # MinIO - 표준화된 인증 정보
  minio:
    image: minio/minio:latest
    container_name: spice_minio
    environment:
      - MINIO_ROOT_USER=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_ROOT_PASSWORD=${MINIO_SECRET_KEY:-minioadmin123}
    ports:
      - "${MINIO_PORT_HOST:-9000}:9000"
      - "${MINIO_CONSOLE_PORT_HOST:-9001}:9001"
    volumes:
      - minio_data:/data
    networks:
      spice_network:
        aliases:
          - spice-minio
    command: server /data --console-address ":9001"
    healthcheck:
      # MinIO image doesn't ship with curl; use bash TCP check.
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/127.0.0.1/9000'"]
      interval: 30s
      timeout: 10s
      retries: 5

  # TerminusDB - 표준화된 인증 정보
  terminusdb:
    image: terminusdb/terminusdb-server:latest
    container_name: spice_terminusdb
    environment:
      - TERMINUSDB_ADMIN_PASS=${TERMINUS_KEY:-admin}
      - TERMINUSDB_SERVER_NAME=SpiceTerminusDB
      - TERMINUSDB_AUTOLOGIN=false
    ports:
      - "6363:6363"
    volumes:
      - terminusdb_data:/app/terminusdb/storage
    networks:
      - spice_network
    healthcheck:
      # terminusdb-server image doesn't ship with curl; use bash TCP check.
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/127.0.0.1/6363'"]
      interval: 30s
      timeout: 10s
      retries: 5

  # OMS (Ontology Management Service) - 표준화된 인증 정보
  oms:
    build:
      context: .
      dockerfile: ./oms/Dockerfile
    container_name: spice_oms
    environment:
      # TerminusDB
      - TERMINUS_SERVER_URL=http://terminusdb:6363
      - TERMINUS_USER=admin
      - TERMINUS_ACCOUNT=admin
      - TERMINUS_KEY=${TERMINUS_KEY:-admin}
      # PostgreSQL
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      # Kafka
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      # Redis
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-spicepass123}
      # Elasticsearch
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-}
      # Event Store (S3/MinIO)
      - MINIO_ENDPOINT_URL=http://spice-minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-minioadmin123}
      - EVENT_STORE_BUCKET=${EVENT_STORE_BUCKET:-spice-event-store}
      # Service
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
      - OMS_REQUIRE_AUTH=${OMS_REQUIRE_AUTH:-true}
      - ADMIN_TOKEN=${ADMIN_TOKEN:?ADMIN_TOKEN is required}
    ports:
      - "8000:8000"
    depends_on:
      - terminusdb
      - postgres
      - redis
      - elasticsearch
    networks:
      - spice_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # BFF
  bff:
    build:
      context: .
      dockerfile: ./bff/Dockerfile
    container_name: spice_bff
    environment:
      - OMS_BASE_URL=http://oms:8000
      - FUNNEL_BASE_URL=http://funnel:8003
      # Infra (needed for WebSocket notifications / search / rate limiting)
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-spicepass123}
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-}
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - OBJECTIFY_JOBS_TOPIC=${OBJECTIFY_JOBS_TOPIC:-objectify-jobs}
      - ENABLE_OBJECTIFY_OUTBOX_WORKER=true
      - OBJECTIFY_OUTBOX_POLL_SECONDS=5
      - OBJECTIFY_OUTBOX_BATCH=50
      - OBJECTIFY_OUTBOX_FLUSH_TIMEOUT_SECONDS=10
      - OBJECTIFY_OUTBOX_BACKOFF_BASE_SECONDS=2
      - OBJECTIFY_OUTBOX_BACKOFF_MAX_SECONDS=60
      - OBJECTIFY_OUTBOX_CLAIM_TIMEOUT_SECONDS=300
      - OBJECTIFY_OUTBOX_MAX_IN_FLIGHT=5
      - OBJECTIFY_OUTBOX_RETRIES=5
      - OBJECTIFY_OUTBOX_DELIVERY_TIMEOUT_MS=120000
      - OBJECTIFY_OUTBOX_REQUEST_TIMEOUT_MS=30000
      - OBJECTIFY_OUTBOX_PURGE_INTERVAL_SECONDS=3600
      - OBJECTIFY_OUTBOX_RETENTION_DAYS=7
      - OBJECTIFY_OUTBOX_PURGE_LIMIT=10000
      - ENABLE_OBJECTIFY_RECONCILER=true
      - OBJECTIFY_RECONCILER_POLL_SECONDS=60
      - OBJECTIFY_RECONCILER_STALE_SECONDS=600
      - OBJECTIFY_RECONCILER_ENQUEUED_STALE_SECONDS=900
      - OBJECTIFY_RECONCILER_LOCK_KEY=910215
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
      - BFF_REQUIRE_AUTH=${BFF_REQUIRE_AUTH:-true}
      - BFF_REQUIRE_DB_SCOPE=${BFF_REQUIRE_DB_SCOPE:-false}
      - ENABLE_DATASET_INGEST_RECONCILER=false
      - ADMIN_TOKEN=${ADMIN_TOKEN:?ADMIN_TOKEN is required}
      # Event Store / Dataset artifacts (S3/MinIO)
      - MINIO_ENDPOINT_URL=http://spice-minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-minioadmin123}
      # lakeFS (versioned storage + S3 gateway)
      - LAKEFS_API_URL=http://lakefs:8000
      - LAKEFS_S3_ENDPOINT_URL=http://lakefs:8000
      - LAKEFS_ARTIFACTS_REPOSITORY=${LAKEFS_ARTIFACTS_REPOSITORY:-pipeline-artifacts}
      - LAKEFS_RAW_REPOSITORY=${LAKEFS_RAW_REPOSITORY:-raw-datasets}
      - LAKEFS_CREDENTIALS_SOURCE=${LAKEFS_CREDENTIALS_SOURCE:-db}
      - LAKEFS_CREDENTIALS_ENCRYPTION_KEY=${LAKEFS_CREDENTIALS_ENCRYPTION_KEY:-WCgHDn1AOcBzLOuW2zM8kSTFocoQmT07kkgHCFc8qtM=}
      - LAKEFS_SERVICE_PRINCIPAL=bff
    ports:
      - "8002:8002"
    depends_on:
      - oms
      - funnel
    networks:
      - spice_network
    volumes:
      - bff_data:/app/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Connector Trigger Service (Foundry-style; polls + emits connector-updates)
  connector-trigger-service:
    build:
      context: .
      dockerfile: ./connector_trigger_service/Dockerfile
    container_name: spice_connector_trigger_service
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
      - CONNECTOR_UPDATES_TOPIC=connector-updates
      - CONNECTOR_TRIGGER_SOURCE_TYPE=google_sheets
      - CONNECTOR_TRIGGER_TICK_SECONDS=5
      - CONNECTOR_TRIGGER_POLL_CONCURRENCY=5
      # Optional (public sheets work without it)
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
    depends_on:
      - kafka
      - postgres
    networks:
      - spice_network
    restart: unless-stopped

  # Connector Sync Worker (Foundry-style; consumes connector-updates and submits writes)
  connector-sync-worker:
    build:
      context: .
      dockerfile: ./connector_sync_worker/Dockerfile
    container_name: spice_connector_sync_worker
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      # IMPORTANT: override ServiceConfig default (127.0.0.1) in Docker
      - BFF_BASE_URL=http://bff:8002
      - ADMIN_TOKEN=${ADMIN_TOKEN:?ADMIN_TOKEN is required}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
      - CONNECTOR_UPDATES_TOPIC=connector-updates
      - CONNECTOR_UPDATES_DLQ_TOPIC=connector-updates-dlq
      # Optional (public sheets work without it)
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
    depends_on:
      - bff
      - kafka
      - postgres
    networks:
      - spice_network
    restart: unless-stopped

  # Pipeline Worker (Spark-based execution)
  pipeline-worker:
    build:
      context: .
      dockerfile: ./pipeline_worker/Dockerfile
    container_name: spice_pipeline_worker
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      - MINIO_ENDPOINT_URL=http://spice-minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-minioadmin123}
      - LAKEFS_API_URL=http://lakefs:8000
      - LAKEFS_S3_ENDPOINT_URL=http://lakefs:8000
      - LAKEFS_ARTIFACTS_REPOSITORY=${LAKEFS_ARTIFACTS_REPOSITORY:-pipeline-artifacts}
      - LAKEFS_RAW_REPOSITORY=${LAKEFS_RAW_REPOSITORY:-raw-datasets}
      - LAKEFS_CREDENTIALS_SOURCE=${LAKEFS_CREDENTIALS_SOURCE:-db}
      - LAKEFS_CREDENTIALS_ENCRYPTION_KEY=${LAKEFS_CREDENTIALS_ENCRYPTION_KEY:-WCgHDn1AOcBzLOuW2zM8kSTFocoQmT07kkgHCFc8qtM=}
      - LAKEFS_SERVICE_PRINCIPAL=pipeline-worker
      - BFF_BASE_URL=http://bff:8002
      - ADMIN_TOKEN=${ADMIN_TOKEN:?ADMIN_TOKEN is required}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
      - PIPELINE_JOBS_TOPIC=pipeline-jobs
      - OBJECTIFY_JOBS_TOPIC=${OBJECTIFY_JOBS_TOPIC:-objectify-jobs}
    depends_on:
      - kafka
      - postgres
      - minio
    networks:
      - spice_network
    restart: unless-stopped

  # Pipeline Scheduler (cron-style)
  pipeline-scheduler:
    build:
      context: .
      dockerfile: ./pipeline_scheduler/Dockerfile
    container_name: spice_pipeline_scheduler
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      - LAKEFS_API_URL=http://lakefs:8000
      - LAKEFS_S3_ENDPOINT_URL=http://lakefs:8000
      - LAKEFS_ARTIFACTS_REPOSITORY=${LAKEFS_ARTIFACTS_REPOSITORY:-pipeline-artifacts}
      - LAKEFS_RAW_REPOSITORY=${LAKEFS_RAW_REPOSITORY:-raw-datasets}
      - LAKEFS_CREDENTIALS_SOURCE=${LAKEFS_CREDENTIALS_SOURCE:-db}
      - LAKEFS_CREDENTIALS_ENCRYPTION_KEY=${LAKEFS_CREDENTIALS_ENCRYPTION_KEY:-WCgHDn1AOcBzLOuW2zM8kSTFocoQmT07kkgHCFc8qtM=}
      - LAKEFS_SERVICE_PRINCIPAL=pipeline-scheduler
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
      - PIPELINE_JOBS_TOPIC=pipeline-jobs
    depends_on:
      - kafka
      - postgres
    networks:
      - spice_network
    restart: unless-stopped

  # Objectify Worker (Dataset -> Ontology instances)
  objectify-worker:
    build:
      context: .
      dockerfile: ./objectify_worker/Dockerfile
    container_name: spice_objectify_worker
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      - OMS_BASE_URL=http://oms:8000
      - LAKEFS_S3_ENDPOINT_URL=http://lakefs:8000
      - LAKEFS_ACCESS_KEY_ID=${LAKEFS_ACCESS_KEY_ID:-spice-lakefs-admin}
      - LAKEFS_SECRET_ACCESS_KEY=${LAKEFS_SECRET_ACCESS_KEY:-spice-lakefs-admin-secret}
      - OBJECTIFY_JOBS_TOPIC=${OBJECTIFY_JOBS_TOPIC:-objectify-jobs}
      - OBJECTIFY_JOBS_GROUP=${OBJECTIFY_JOBS_GROUP:-objectify-worker-group}
      - OBJECTIFY_JOBS_DLQ_TOPIC=${OBJECTIFY_JOBS_DLQ_TOPIC:-objectify-jobs-dlq}
      - OBJECTIFY_MAX_RETRIES=${OBJECTIFY_MAX_RETRIES:-5}
      - OBJECTIFY_BACKOFF_BASE_SECONDS=${OBJECTIFY_BACKOFF_BASE_SECONDS:-2}
      - OBJECTIFY_BACKOFF_MAX_SECONDS=${OBJECTIFY_BACKOFF_MAX_SECONDS:-60}
      - OBJECTIFY_ROW_BATCH_SIZE=${OBJECTIFY_ROW_BATCH_SIZE:-1000}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
      - ADMIN_TOKEN=${ADMIN_TOKEN:?ADMIN_TOKEN is required}
    depends_on:
      - kafka
      - postgres
      - lakefs
      - oms
    networks:
      - spice_network
    restart: unless-stopped

  # Dataset ingest reconciler worker (runs atomicity repair loop)
  ingest-reconciler-worker:
    build:
      context: .
      dockerfile: ./ingest_reconciler_worker/Dockerfile
    container_name: spice_ingest_reconciler_worker
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
      - ENABLE_DATASET_INGEST_RECONCILER=${ENABLE_DATASET_INGEST_RECONCILER:-true}
      - DATASET_INGEST_RECONCILER_POLL_SECONDS=${DATASET_INGEST_RECONCILER_POLL_SECONDS:-60}
      - DATASET_INGEST_RECONCILER_STALE_SECONDS=${DATASET_INGEST_RECONCILER_STALE_SECONDS:-3600}
      - DATASET_INGEST_RECONCILER_LIMIT=${DATASET_INGEST_RECONCILER_LIMIT:-200}
      - DATASET_INGEST_RECONCILER_LOCK_KEY=${DATASET_INGEST_RECONCILER_LOCK_KEY:-910214}
      - INGEST_RECONCILER_PORT=${INGEST_RECONCILER_PORT:-8012}
      - INGEST_RECONCILER_ALERT_WEBHOOK_URL=${INGEST_RECONCILER_ALERT_WEBHOOK_URL:-}
      - INGEST_RECONCILER_ALERT_PUBLISHED_THRESHOLD=${INGEST_RECONCILER_ALERT_PUBLISHED_THRESHOLD:-1}
      - INGEST_RECONCILER_ALERT_ABORTED_THRESHOLD=${INGEST_RECONCILER_ALERT_ABORTED_THRESHOLD:-1}
      - INGEST_RECONCILER_ALERT_ON_ERROR=${INGEST_RECONCILER_ALERT_ON_ERROR:-true}
      - INGEST_RECONCILER_ALERT_COOLDOWN_SECONDS=${INGEST_RECONCILER_ALERT_COOLDOWN_SECONDS:-300}
    ports:
      - "${INGEST_RECONCILER_PORT_HOST:-8012}:8012"
    depends_on:
      - postgres
    networks:
      - spice_network
    restart: unless-stopped

  # Funnel
  funnel:
    build:
      context: .
      dockerfile: ./funnel/Dockerfile
    container_name: spice_funnel
    environment:
      # Infra (rate limiting)
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-spicepass123}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
    ports:
      - "8003:8003"
    networks:
      - spice_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Message Relay - 표준화된 인증 정보
  message-relay:
    build:
      context: .
      dockerfile: ./message_relay/Dockerfile
    container_name: spice_message_relay
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - MINIO_ENDPOINT_URL=http://spice-minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-minioadmin123}
      - EVENT_STORE_BUCKET=${EVENT_STORE_BUCKET:-spice-event-store}
      - EVENT_PUBLISHER_BATCH_SIZE=${EVENT_PUBLISHER_BATCH_SIZE:-200}
      - EVENT_PUBLISHER_POLL_INTERVAL=${EVENT_PUBLISHER_POLL_INTERVAL:-1}
      - EVENT_PUBLISHER_LOOKBACK_SECONDS=${EVENT_PUBLISHER_LOOKBACK_SECONDS:-0}
      - EVENT_PUBLISHER_LOOKBACK_MAX_KEYS=${EVENT_PUBLISHER_LOOKBACK_MAX_KEYS:-0}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
    depends_on:
      - kafka
    networks:
      - spice_network
    restart: unless-stopped

  # Ontology Worker - 표준화된 인증 정보
  ontology-worker:
    build:
      context: .
      dockerfile: ./ontology_worker/Dockerfile
    container_name: spice_ontology_worker
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - TERMINUS_SERVER_URL=http://terminusdb:6363
      - TERMINUS_USER=admin
      - TERMINUS_ACCOUNT=admin
      - TERMINUS_KEY=${TERMINUS_KEY:-admin}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-spicepass123}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
    depends_on:
      - terminusdb
      - kafka
      - redis
    networks:
      - spice_network
    restart: unless-stopped

  # Instance Worker - 표준화된 인증 정보
  instance-worker:
    build:
      context: .
      dockerfile: ./instance_worker/Dockerfile
    container_name: spice_instance_worker
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - TERMINUS_SERVER_URL=http://terminusdb:6363
      - TERMINUS_USER=admin
      - TERMINUS_ACCOUNT=admin
      - TERMINUS_KEY=${TERMINUS_KEY:-admin}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-spicepass123}
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-}
      - MINIO_ENDPOINT_URL=http://spice-minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-minioadmin123}
      - INSTANCE_BUCKET=instance-events
      - EVENT_STORE_BUCKET=${EVENT_STORE_BUCKET:-spice-event-store}
      # Correctness/chaos tuning (defaults are production-safe)
      - PROCESSED_EVENT_LEASE_TIMEOUT_SECONDS=${PROCESSED_EVENT_LEASE_TIMEOUT_SECONDS:-900}
      - PROCESSED_EVENT_HEARTBEAT_INTERVAL_SECONDS=${PROCESSED_EVENT_HEARTBEAT_INTERVAL_SECONDS:-30}
      - INSTANCE_WORKER_MAX_RETRY_ATTEMPTS=${INSTANCE_WORKER_MAX_RETRY_ATTEMPTS:-5}
      - ENABLE_CHAOS_INJECTION=${ENABLE_CHAOS_INJECTION:-false}
      - CHAOS_CRASH_POINT=${CHAOS_CRASH_POINT:-}
      - CHAOS_CRASH_ONCE=${CHAOS_CRASH_ONCE:-true}
      - CHAOS_CRASH_EXIT_CODE=${CHAOS_CRASH_EXIT_CODE:-42}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
    depends_on:
      - terminusdb
      - kafka
      - elasticsearch
      - minio
    networks:
      - spice_network
    restart: unless-stopped

  # Projection Worker - 표준화된 인증 정보
  projection-worker:
    build:
      context: .
      dockerfile: ./projection_worker/Dockerfile
    container_name: spice_projection_worker
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-spiceadmin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-spicepass123}
      - POSTGRES_DB=${POSTGRES_DB:-spicedb}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-spicepass123}
      # Correctness/chaos tuning (defaults are production-safe)
      - PROCESSED_EVENT_LEASE_TIMEOUT_SECONDS=${PROCESSED_EVENT_LEASE_TIMEOUT_SECONDS:-900}
      - PROCESSED_EVENT_HEARTBEAT_INTERVAL_SECONDS=${PROCESSED_EVENT_HEARTBEAT_INTERVAL_SECONDS:-30}
      - PROJECTION_WORKER_MAX_RETRIES=${PROJECTION_WORKER_MAX_RETRIES:-5}
      - ENABLE_CHAOS_INJECTION=${ENABLE_CHAOS_INJECTION:-false}
      - CHAOS_CRASH_POINT=${CHAOS_CRASH_POINT:-}
      - CHAOS_CRASH_ONCE=${CHAOS_CRASH_ONCE:-true}
      - CHAOS_CRASH_EXIT_CODE=${CHAOS_CRASH_EXIT_CODE:-42}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
    depends_on:
      - kafka
      - elasticsearch
      - redis
    networks:
      - spice_network
    restart: unless-stopped

  # Search Projection Worker (optional instance indexing)
  search-projection-worker:
    build:
      context: .
      dockerfile: ./search_projection_worker/Dockerfile
    container_name: spice_search_projection_worker
    environment:
      - KAFKA_HOST=kafka
      - KAFKA_PORT=29092
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME:-}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-}
      - ENABLE_SEARCH_PROJECTION=${ENABLE_SEARCH_PROJECTION:-false}
      - SEARCH_INDEX=${SEARCH_INDEX:-objects}
      - INSTANCE_EVENTS_TOPIC=${INSTANCE_EVENTS_TOPIC:-instance_events}
      - SEARCH_PROJECTION_DLQ_TOPIC=${SEARCH_PROJECTION_DLQ_TOPIC:-projection_failures_dlq}
      - SEARCH_PROJECTION_MAX_RETRIES=${SEARCH_PROJECTION_MAX_RETRIES:-5}
      - SEARCH_PROJECTION_BACKOFF_BASE_SECONDS=${SEARCH_PROJECTION_BACKOFF_BASE_SECONDS:-2}
      - SEARCH_PROJECTION_BACKOFF_MAX_SECONDS=${SEARCH_PROJECTION_BACKOFF_MAX_SECONDS:-60}
      - PROCESSED_EVENT_LEASE_TIMEOUT_SECONDS=${PROCESSED_EVENT_LEASE_TIMEOUT_SECONDS:-900}
      - PROCESSED_EVENT_HEARTBEAT_INTERVAL_SECONDS=${PROCESSED_EVENT_HEARTBEAT_INTERVAL_SECONDS:-30}
      - LOG_LEVEL=INFO
      - DOCKER_CONTAINER=true
    depends_on:
      - kafka
      - elasticsearch
    networks:
      - spice_network
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  elasticsearch_data:
  minio_data:
  terminusdb_data:
  bff_data:

networks:
  spice_network:
    name: ${SPICE_NETWORK_NAME:-spice_network}
    driver: bridge
