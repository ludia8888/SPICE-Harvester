"""
Message Relay Service
Outbox í…Œì´ë¸”ì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì½ì–´ Kafkaë¡œ ì „ì†¡í•˜ëŠ” ì„œë¹„ìŠ¤

ğŸ”¥ MIGRATION: Enhanced to include S3/MinIO Event Store references
- Kafka messages now include S3 event references
- Consumers can gradually migrate to reading from S3
- PostgreSQL payload still included for backward compatibility
"""

import asyncio
import json
import logging
import os
import signal
from typing import Optional, Dict, Any
from datetime import datetime, timezone

import asyncpg
from confluent_kafka import Producer, KafkaError
from confluent_kafka.admin import AdminClient, NewTopic

from shared.config.service_config import ServiceConfig
from shared.config.app_config import AppConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class MessageRelay:
    """Outbox Pattern Message Relay Service
    
    ğŸ”¥ MIGRATION: Enhanced with S3/MinIO Event Store integration
    """
    
    def __init__(self):
        self.running = False
        self.postgres_url = ServiceConfig.get_postgres_url()
        self.kafka_servers = ServiceConfig.get_kafka_bootstrap_servers()
        self.producer: Optional[Producer] = None
        self.conn: Optional[asyncpg.Connection] = None
        
        # ğŸ”¥ S3/MinIO Event Store configuration
        self.s3_enabled = os.getenv("ENABLE_S3_EVENT_STORE", "false").lower() == "true"
        self.s3_bucket = "spice-event-store"
        self.minio_endpoint = ServiceConfig.get_minio_endpoint() if self.s3_enabled else None
        
    async def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # Kafka Producer ì„¤ì •
        self.producer = Producer({
            'bootstrap.servers': self.kafka_servers,
            'client.id': 'message-relay',
            'acks': 'all',  # ëª¨ë“  replicaê°€ ë©”ì‹œì§€ë¥¼ ë°›ì„ ë•Œê¹Œì§€ ëŒ€ê¸°
            'retries': 3,
            'max.in.flight.requests.per.connection': 1,  # ìˆœì„œ ë³´ì¥
            # REMOVED: 'compression.type': 'snappy',  # Python consumers can't read Snappy!
        })
        
        # Kafka í† í”½ ìƒì„± í™•ì¸
        await self.ensure_kafka_topics()
        
        # PostgreSQL ì—°ê²°
        self.conn = await asyncpg.connect(self.postgres_url)
        
        # ğŸ”¥ Log S3/MinIO Event Store configuration
        if self.s3_enabled:
            logger.info(f"ğŸ”¥ S3/MinIO Event Store ENABLED - endpoint: {self.minio_endpoint}, bucket: {self.s3_bucket}")
            logger.info("ğŸ”¥ Messages will include S3 references for gradual migration")
        else:
            logger.info("âš ï¸ S3/MinIO Event Store DISABLED - using legacy mode")
            
        logger.info("Message Relay initialized successfully")
        
    async def ensure_kafka_topics(self):
        """í•„ìš”í•œ Kafka í† í”½ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        admin_client = AdminClient({'bootstrap.servers': self.kafka_servers})
        
        # ìƒì„±í•  í† í”½ ë¦¬ìŠ¤íŠ¸
        topics = [
            NewTopic(
                topic=AppConfig.ONTOLOGY_EVENTS_TOPIC,
                num_partitions=3,
                replication_factor=1,
                config={
                    'retention.ms': '604800000',  # 7ì¼ ë³´ê´€
                    # REMOVED: 'compression.type': 'snappy'  # Python consumers can't read Snappy!
                }
            )
        ]
        
        # ê¸°ì¡´ í† í”½ í™•ì¸
        existing_topics = admin_client.list_topics(timeout=10)
        existing_topic_names = set(existing_topics.topics.keys())
        
        # ìƒˆë¡œìš´ í† í”½ë§Œ ìƒì„±
        new_topics = [t for t in topics if t.topic not in existing_topic_names]
        
        if new_topics:
            fs = admin_client.create_topics(new_topics)
            for topic, f in fs.items():
                try:
                    f.result()  # í† í”½ ìƒì„± ì™„ë£Œ ëŒ€ê¸°
                    logger.info(f"Created Kafka topic: {topic}")
                except Exception as e:
                    logger.error(f"Failed to create topic {topic}: {e}")
        else:
            logger.info("All required Kafka topics already exist")
            
    def delivery_report(self, err, msg):
        """Kafka ë©”ì‹œì§€ ì „ì†¡ ê²°ê³¼ ì½œë°±"""
        if err is not None:
            logger.error(f"Message delivery failed: {err}")
        else:
            logger.debug(f"Message delivered to {msg.topic()} [{msg.partition()}]")
    
    def _build_s3_reference(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ Build S3/MinIO Event Store reference for a message
        
        Returns a reference to where the event is stored in S3/MinIO
        """
        try:
            # Parse the payload to extract event details
            payload = json.loads(message['payload']) if isinstance(message['payload'], str) else message['payload']
            
            # Extract event metadata
            event_id = message['id']
            aggregate_type = payload.get('aggregate_type', 'unknown')
            aggregate_id = payload.get('aggregate_id', 'unknown')
            
            # Get timestamp from payload or use current time
            timestamp_str = payload.get('timestamp') or payload.get('created_at')
            if timestamp_str:
                # Parse timestamp (handle various formats)
                if isinstance(timestamp_str, str):
                    # Simple parsing - you might need more robust parsing
                    dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                else:
                    dt = datetime.now(timezone.utc)
            else:
                dt = datetime.now(timezone.utc)
            
            # Build S3 key path (matching event_store.py structure)
            s3_key = (
                f"events/{dt.year:04d}/{dt.month:02d}/{dt.day:02d}/"
                f"{aggregate_type}/{aggregate_id}/{event_id}.json"
            )
            
            return {
                "bucket": self.s3_bucket,
                "key": s3_key,
                "endpoint": self.minio_endpoint,
                "event_id": str(event_id)
            }
        except Exception as e:
            logger.warning(f"Failed to build S3 reference: {e}")
            return None
            
    async def process_events(self):
        """Outbox í…Œì´ë¸”ì—ì„œ ë©”ì‹œì§€(Command/Event)ë¥¼ ì½ì–´ Kafkaë¡œ ì „ì†¡"""
        batch_size = int(os.getenv("MESSAGE_RELAY_BATCH_SIZE", "100"))
        
        async with self.conn.transaction():
            # ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë©”ì‹œì§€ë¥¼ ì¡°íšŒí•˜ê³  ì ê¸ˆ
            messages = await self.conn.fetch(
                """
                SELECT id, message_type, topic, payload, retry_count
                FROM spice_outbox.outbox
                WHERE processed_at IS NULL
                  AND (retry_count < 5 OR retry_count IS NULL)
                  AND (last_retry_at IS NULL OR last_retry_at < NOW() - INTERVAL '1 minute' * POWER(2, retry_count))
                ORDER BY created_at
                LIMIT $1
                FOR UPDATE SKIP LOCKED
                """,
                batch_size
            )
            
            if not messages:
                return 0
                
            processed_count = 0
            
            for message in messages:
                try:
                    # ğŸ”¥ MIGRATION: Build enriched message with S3 reference
                    kafka_message = {
                        "message_type": message['message_type'],
                        "payload": json.loads(message['payload']) if isinstance(message['payload'], str) else message['payload'],
                        "metadata": {
                            "relay_timestamp": datetime.now(timezone.utc).isoformat(),
                            "retry_count": message['retry_count'] or 0
                        }
                    }
                    
                    # Add S3 reference if enabled
                    if self.s3_enabled:
                        s3_ref = self._build_s3_reference(message)
                        if s3_ref:
                            kafka_message["s3_reference"] = s3_ref
                            kafka_message["metadata"]["storage_mode"] = "dual_write"
                            logger.debug(f"Added S3 reference: {s3_ref['key']}")
                        else:
                            kafka_message["metadata"]["storage_mode"] = "postgres_only"
                    else:
                        kafka_message["metadata"]["storage_mode"] = "legacy"
                    
                    # Serialize to JSON
                    kafka_value = json.dumps(kafka_message)
                    
                    # Kafkaë¡œ ë©”ì‹œì§€ ë°œí–‰
                    self.producer.produce(
                        topic=message['topic'],
                        value=kafka_value,
                        key=str(message['id']).encode('utf-8'),
                        callback=self.delivery_report
                    )
                    
                    # ë°œí–‰ ì„±ê³µ ì‹œ ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                    await self.conn.execute(
                        """
                        UPDATE spice_outbox.outbox 
                        SET processed_at = NOW()
                        WHERE id = $1
                        """,
                        message['id']
                    )
                    
                    processed_count += 1
                    storage_mode = kafka_message["metadata"].get("storage_mode", "unknown")
                    logger.info(f"ğŸ”¥ Relayed {message['message_type']} {message['id']} to topic {message['topic']} (mode: {storage_mode})")
                    
                except Exception as e:
                    # ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€
                    logger.error(f"Failed to relay message {message['id']}: {e}")
                    
                    await self.conn.execute(
                        """
                        UPDATE spice_outbox.outbox 
                        SET retry_count = COALESCE(retry_count, 0) + 1,
                            last_retry_at = NOW()
                        WHERE id = $1
                        """,
                        message['id']
                    )
                    
            # ë‚¨ì€ ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ ëŒ€ê¸°
            self.producer.flush(timeout=10)
            
            return processed_count
            
    async def run(self):
        """ë©”ì¸ ì‹¤í–‰ ë£¨í”„"""
        self.running = True
        poll_interval = int(os.getenv("MESSAGE_RELAY_POLL_INTERVAL", "5"))
        
        logger.info(f"Message Relay started. Polling interval: {poll_interval}s")
        
        while self.running:
            try:
                # ì´ë²¤íŠ¸ ì²˜ë¦¬
                processed = await self.process_events()
                
                if processed > 0:
                    logger.info(f"Processed {processed} events")
                else:
                    # ì²˜ë¦¬í•  ì´ë²¤íŠ¸ê°€ ì—†ìœ¼ë©´ ëŒ€ê¸°
                    await asyncio.sleep(poll_interval)
                    
            except Exception as e:
                logger.error(f"Error in relay loop: {e}")
                await asyncio.sleep(poll_interval)
                
    async def shutdown(self):
        """ì„œë¹„ìŠ¤ ì¢…ë£Œ"""
        logger.info("Shutting down Message Relay...")
        self.running = False
        
        if self.producer:
            self.producer.flush()
            
        if self.conn:
            await self.conn.close()
            
        logger.info("Message Relay shut down successfully")


async def main():
    """ë©”ì¸ ì§„ì…ì """
    relay = MessageRelay()
    
    # ì¢…ë£Œ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬
    def signal_handler(sig, frame):
        logger.info(f"Received signal {sig}")
        asyncio.create_task(relay.shutdown())
        
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        await relay.initialize()
        await relay.run()
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        await relay.shutdown()
        raise


if __name__ == "__main__":
    asyncio.run(main())