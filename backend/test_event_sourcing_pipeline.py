#!/usr/bin/env python3
"""
Event Sourcing íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸
ì˜¬ë°”ë¥¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ ì „ì²´ ë°ì´í„° íë¦„ í™•ì¸
"""

import asyncio
import aiohttp
import asyncpg
import json
import os
import sys
import time
from datetime import datetime

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (Docker í™˜ê²½ì´ ì•„ë‹˜ì„ ëª…ì‹œ)
os.environ["POSTGRES_HOST"] = "localhost"
os.environ["POSTGRES_PORT"] = "5433"
os.environ["KAFKA_HOST"] = "localhost"
os.environ["REDIS_HOST"] = "localhost"
os.environ["ELASTICSEARCH_HOST"] = "localhost"
os.environ["TERMINUS_SERVER_URL"] = "http://localhost:6363"
os.environ["DOCKER_CONTAINER"] = "false"  # Docker í™˜ê²½ì´ ì•„ë‹˜ì„ ëª…ì‹œ

# backend ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, "/Users/isihyeon/Desktop/SPICE HARVESTER/backend")

import logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class EventSourcingPipelineTest:
    """Event Sourcing íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.api_base_url = "http://localhost:8000"
        self.postgres_url = "postgresql://spiceadmin:spicepass123@localhost:5433/spicedb"
        self.test_db_name = f"es_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.test_class_id = "TestProduct"
        
    async def test_database_creation(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸"""
        logger.info("=" * 60)
        logger.info("1ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸")
        logger.info("=" * 60)
        
        async with aiohttp.ClientSession() as session:
            # ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            async with session.post(
                f"{self.api_base_url}/api/v1/database/create",
                json={
                    "name": self.test_db_name,
                    "description": "Event Sourcing Pipeline Test"
                }
            ) as resp:
                status = resp.status
                data = await resp.json()
                
                if status in [200, 202]:
                    logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì„±ê³µ: {self.test_db_name}")
                    logger.info(f"   ì‘ë‹µ: {data}")
                    return True
                else:
                    logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {status}")
                    logger.error(f"   ì‘ë‹µ: {data}")
                    return False
    
    async def check_outbox_before(self):
        """Outbox í…Œì´ë¸” ì´ì „ ìƒíƒœ í™•ì¸"""
        logger.info("\nğŸ“Š Outbox í…Œì´ë¸” ì´ì „ ìƒíƒœ í™•ì¸...")
        
        conn = await asyncpg.connect(self.postgres_url)
        try:
            count = await conn.fetchval(
                "SELECT COUNT(*) FROM spice_outbox.outbox WHERE processed_at IS NULL"
            )
            logger.info(f"   ë¯¸ì²˜ë¦¬ ë©”ì‹œì§€ ìˆ˜: {count}")
            
            # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ í™•ì¸
            recent = await conn.fetch(
                """
                SELECT id, message_type, aggregate_type, created_at 
                FROM spice_outbox.outbox 
                ORDER BY created_at DESC 
                LIMIT 5
                """
            )
            
            if recent:
                logger.info("   ìµœê·¼ ë©”ì‹œì§€:")
                for msg in recent:
                    logger.info(f"   - {msg['message_type']}: {msg['aggregate_type']} ({msg['created_at']})")
            
            return count
            
        finally:
            await conn.close()
    
    async def test_instance_creation(self):
        """ì¸ìŠ¤í„´ìŠ¤ ìƒì„± Command í…ŒìŠ¤íŠ¸"""
        logger.info("\n=" * 60)
        logger.info("2ï¸âƒ£ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± Command í…ŒìŠ¤íŠ¸")
        logger.info("=" * 60)
        
        async with aiohttp.ClientSession() as session:
            # ë¹„ë™ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± API í˜¸ì¶œ
            test_data = {
                "data": {
                    "product_name": "ES Test Product",
                    "price": 99.99,
                    "test_timestamp": datetime.utcnow().isoformat()
                },
                "metadata": {
                    "test_id": "es_pipeline_test"
                }
            }
            
            async with session.post(
                f"{self.api_base_url}/api/v1/instances/{self.test_db_name}/async/{self.test_class_id}/create",
                json=test_data
            ) as resp:
                status = resp.status
                data = await resp.json()
                
                if status == 202:
                    logger.info(f"âœ… Command ìƒì„± ì„±ê³µ (202 Accepted)")
                    logger.info(f"   Command ID: {data.get('command_id')}")
                    logger.info(f"   ìƒíƒœ: {data.get('status')}")
                    return data.get('command_id')
                else:
                    logger.error(f"âŒ Command ìƒì„± ì‹¤íŒ¨: {status}")
                    logger.error(f"   ì‘ë‹µ: {data}")
                    return None
    
    async def check_outbox_after(self, before_count: int):
        """Outbox í…Œì´ë¸” ì´í›„ ìƒíƒœ í™•ì¸"""
        logger.info("\nğŸ“Š Outbox í…Œì´ë¸” ì´í›„ ìƒíƒœ í™•ì¸...")
        
        conn = await asyncpg.connect(self.postgres_url)
        try:
            # 3ì´ˆ ëŒ€ê¸° (Message Relayê°€ ì²˜ë¦¬í•  ì‹œê°„)
            await asyncio.sleep(3)
            
            after_count = await conn.fetchval(
                "SELECT COUNT(*) FROM spice_outbox.outbox WHERE processed_at IS NULL"
            )
            
            total_count = await conn.fetchval(
                "SELECT COUNT(*) FROM spice_outbox.outbox"
            )
            
            logger.info(f"   ì „ì²´ ë©”ì‹œì§€ ìˆ˜: {total_count}")
            logger.info(f"   ë¯¸ì²˜ë¦¬ ë©”ì‹œì§€ ìˆ˜: {after_count}")
            logger.info(f"   ë³€í™”: {before_count} â†’ {after_count}")
            
            # ìƒˆë¡œ ì¶”ê°€ëœ ë©”ì‹œì§€ í™•ì¸
            if total_count > before_count:
                new_messages = await conn.fetch(
                    """
                    SELECT id, message_type, aggregate_type, topic, processed_at
                    FROM spice_outbox.outbox 
                    ORDER BY created_at DESC 
                    LIMIT 5
                    """
                )
                
                logger.info("   ìµœì‹  ë©”ì‹œì§€:")
                for msg in new_messages:
                    status = "âœ… ì²˜ë¦¬ë¨" if msg['processed_at'] else "â³ ëŒ€ê¸°ì¤‘"
                    logger.info(f"   - {msg['message_type']}: {msg['aggregate_type']} â†’ {msg['topic']} [{status}]")
            
            return after_count
            
        finally:
            await conn.close()
    
    async def check_elasticsearch(self):
        """Elasticsearch ë°ì´í„° í™•ì¸"""
        logger.info("\nğŸ“Š Elasticsearch ë°ì´í„° í™•ì¸...")
        
        async with aiohttp.ClientSession() as session:
            # ì¸ë±ìŠ¤ í™•ì¸
            index_name = f"instances_{self.test_db_name.replace('-', '_')}"
            
            async with session.get(
                f"http://localhost:9200/{index_name}/_count"
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    count = data.get('count', 0)
                    logger.info(f"âœ… Elasticsearch ì¸ë±ìŠ¤ ì¡´ì¬: {index_name}")
                    logger.info(f"   ë¬¸ì„œ ìˆ˜: {count}")
                    
                    if count > 0:
                        # ë¬¸ì„œ ë‚´ìš© í™•ì¸
                        async with session.get(
                            f"http://localhost:9200/{index_name}/_search?size=1"
                        ) as search_resp:
                            search_data = await search_resp.json()
                            hits = search_data.get('hits', {}).get('hits', [])
                            if hits:
                                logger.info("   ìƒ˜í”Œ ë¬¸ì„œ:")
                                logger.info(f"   {json.dumps(hits[0]['_source'], indent=2)[:200]}...")
                    
                    return count > 0
                else:
                    logger.warning(f"âš ï¸  Elasticsearch ì¸ë±ìŠ¤ ì—†ìŒ: {index_name}")
                    return False
    
    async def test_command_status(self, command_id: str):
        """Command ìƒíƒœ í™•ì¸"""
        logger.info(f"\nğŸ“Š Command ìƒíƒœ í™•ì¸: {command_id}")
        
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{self.api_base_url}/api/v1/command/{command_id}/status"
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    status = data.get('data', {}).get('status')
                    logger.info(f"   Command ìƒíƒœ: {status}")
                    
                    if status == "COMPLETED":
                        logger.info("   âœ… Command ì²˜ë¦¬ ì™„ë£Œ!")
                    elif status == "FAILED":
                        logger.error(f"   âŒ Command ì‹¤íŒ¨: {data.get('data', {}).get('error')}")
                    else:
                        logger.info(f"   â³ Command ì²˜ë¦¬ì¤‘... ({status})")
                    
                    return status
                else:
                    logger.warning(f"âš ï¸  Command ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {resp.status}")
                    return None
    
    async def run_full_test(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ Event Sourcing íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info("=" * 60)
        
        try:
            # 1. ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            if not await self.test_database_creation():
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
                return
            
            await asyncio.sleep(2)
            
            # 2. Outbox ì´ì „ ìƒíƒœ í™•ì¸
            before_count = await self.check_outbox_before()
            
            # 3. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± Command ë°œí–‰
            command_id = await self.test_instance_creation()
            if not command_id:
                logger.error("Command ìƒì„± ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
                return
            
            # 4. Outbox ì´í›„ ìƒíƒœ í™•ì¸
            await self.check_outbox_after(before_count)
            
            # 5. Command ì²˜ë¦¬ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
            for i in range(10):
                await asyncio.sleep(1)
                status = await self.test_command_status(command_id)
                if status in ["COMPLETED", "FAILED"]:
                    break
            
            # 6. Elasticsearch ë°ì´í„° í™•ì¸
            await asyncio.sleep(2)
            es_success = await self.check_elasticsearch()
            
            # 7. ê²°ê³¼ ìš”ì•½
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ“Š íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±: {self.test_db_name}")
            logger.info(f"âœ… Command ìƒì„±: {command_id}")
            logger.info(f"{'âœ…' if es_success else 'âŒ'} Elasticsearch ë°ì´í„°: {'ìˆìŒ' if es_success else 'ì—†ìŒ'}")
            
            if es_success:
                logger.info("\nğŸ‰ Event Sourcing íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
            else:
                logger.info("\nâš ï¸  íŒŒì´í”„ë¼ì¸ ì¼ë¶€ êµ¬ì„± ìš”ì†Œê°€ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                logger.info("   ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
                logger.info("   - Message Relay ì„œë¹„ìŠ¤ ì‹¤í–‰ ì—¬ë¶€")
                logger.info("   - Instance Worker ì„œë¹„ìŠ¤ ì‹¤í–‰ ì—¬ë¶€")
                logger.info("   - Projection Worker ì„œë¹„ìŠ¤ ì‹¤í–‰ ì—¬ë¶€")
                logger.info("   - Kafka ì—°ê²° ìƒíƒœ")
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())


async def main():
    tester = EventSourcingPipelineTest()
    await tester.run_full_test()


if __name__ == "__main__":
    asyncio.run(main())