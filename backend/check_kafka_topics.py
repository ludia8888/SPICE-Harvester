#!/usr/bin/env python3
"""
Kafka í† í”½ì˜ ë©”ì‹œì§€ í™•ì¸
THINK ULTRA - ì‹¤ì œ ë©”ì‹œì§€ê°€ Kafkaì— ë„ë‹¬í–ˆëŠ”ì§€ ê²€ì¦
"""

import os
import sys
import json
from confluent_kafka import Consumer, KafkaError, TopicPartition
from confluent_kafka.admin import AdminClient

# í™˜ê²½ ë³€ìˆ˜
os.environ["KAFKA_HOST"] = "127.0.0.1"  # IPv4 ëª…ì‹œ
os.environ["KAFKA_PORT"] = "9092"

sys.path.insert(0, "/Users/isihyeon/Desktop/SPICE HARVESTER/backend")

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def check_kafka_topics():
    """Kafka í† í”½ ë° ë©”ì‹œì§€ í™•ì¸"""
    logger.info("=" * 60)
    logger.info("ğŸ” Kafka í† í”½ í™•ì¸")
    logger.info("=" * 60)
    
    # AdminClientë¡œ í† í”½ ëª©ë¡ í™•ì¸
    admin_client = AdminClient({
        'bootstrap.servers': '127.0.0.1:9092'  # IPv4 ëª…ì‹œ
    })
    
    try:
        # í† í”½ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        metadata = admin_client.list_topics(timeout=5)
        
        logger.info(f"\nğŸ“‹ Kafka í† í”½ ëª©ë¡:")
        topics_to_check = []
        for topic in metadata.topics:
            if not topic.startswith('_'):  # ë‚´ë¶€ í† í”½ ì œì™¸
                partitions = metadata.topics[topic].partitions
                logger.info(f"   - {topic} (íŒŒí‹°ì…˜: {len(partitions)}ê°œ)")
                topics_to_check.append(topic)
        
        # ê° í† í”½ì˜ ë©”ì‹œì§€ í™•ì¸
        for topic_name in topics_to_check:
            if 'command' in topic_name.lower() or 'event' in topic_name.lower():
                logger.info(f"\nğŸ“Œ í† í”½ '{topic_name}' ë©”ì‹œì§€ í™•ì¸:")
                check_topic_messages(topic_name)
                
    except Exception as e:
        logger.error(f"âŒ Kafka í† í”½ í™•ì¸ ì‹¤íŒ¨: {e}")


def check_topic_messages(topic_name):
    """íŠ¹ì • í† í”½ì˜ ë©”ì‹œì§€ í™•ì¸"""
    consumer = Consumer({
        'bootstrap.servers': '127.0.0.1:9092',
        'group.id': f'check-messages-{topic_name}',
        'auto.offset.reset': 'earliest',
        'enable.auto.commit': False,
        'session.timeout.ms': 6000,
        'max.poll.interval.ms': 300000
    })
    
    try:
        # í† í”½ êµ¬ë…
        consumer.subscribe([topic_name])
        
        # íŒŒí‹°ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        partitions = consumer.list_topics(topic=topic_name).topics[topic_name].partitions
        
        # ê° íŒŒí‹°ì…˜ì˜ ì˜¤í”„ì…‹ ì •ë³´ í™•ì¸
        topic_partitions = []
        for partition_id in partitions:
            tp = TopicPartition(topic_name, partition_id)
            topic_partitions.append(tp)
        
        # ì‹œì‘ê³¼ ë ì˜¤í”„ì…‹ ê°€ì ¸ì˜¤ê¸°
        for tp in topic_partitions:
            low, high = consumer.get_watermark_offsets(tp, timeout=5)
            logger.info(f"   íŒŒí‹°ì…˜ {tp.partition}: ì˜¤í”„ì…‹ ë²”ìœ„ [{low}, {high})")
            
            if high > low:
                logger.info(f"      â†’ {high - low}ê°œ ë©”ì‹œì§€ ì¡´ì¬")
                
                # ìµœê·¼ ë©”ì‹œì§€ ëª‡ ê°œ ì½ê¸°
                tp.offset = max(low, high - 5)  # ìµœê·¼ 5ê°œë§Œ
                consumer.assign([tp])
                
                msg_count = 0
                while msg_count < 5:
                    msg = consumer.poll(timeout=1.0)
                    if msg is None:
                        break
                    if msg.error():
                        if msg.error().code() == KafkaError._PARTITION_EOF:
                            break
                        else:
                            logger.error(f"      ë©”ì‹œì§€ ì—ëŸ¬: {msg.error()}")
                            break
                    
                    try:
                        value = msg.value().decode('utf-8')
                        data = json.loads(value)
                        
                        # ë©”ì‹œì§€ ìš”ì•½ ì¶œë ¥
                        msg_type = data.get('command_type') or data.get('event_type') or 'Unknown'
                        msg_id = data.get('command_id') or data.get('event_id') or 'Unknown'
                        timestamp = data.get('created_at') or data.get('occurred_at') or 'Unknown'
                        
                        logger.info(f"      ë©”ì‹œì§€ {msg_count + 1}:")
                        logger.info(f"         ID: {msg_id}")
                        logger.info(f"         Type: {msg_type}")
                        logger.info(f"         Time: {timestamp}")
                        
                        if 'aggregate_id' in data:
                            logger.info(f"         Aggregate: {data['aggregate_id']}")
                        
                        msg_count += 1
                        
                    except json.JSONDecodeError:
                        logger.error(f"      JSON íŒŒì‹± ì‹¤íŒ¨: {value[:100]}...")
                    except Exception as e:
                        logger.error(f"      ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            else:
                logger.info(f"      â†’ ë©”ì‹œì§€ ì—†ìŒ")
        
    except Exception as e:
        logger.error(f"   í† í”½ ë©”ì‹œì§€ í™•ì¸ ì‹¤íŒ¨: {e}")
    finally:
        consumer.close()


def main():
    # Kafka ì—°ê²° í…ŒìŠ¤íŠ¸
    logger.info("Kafka ì—°ê²° í…ŒìŠ¤íŠ¸...")
    
    try:
        # Producerë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸
        from confluent_kafka import Producer
        
        producer = Producer({
            'bootstrap.servers': '127.0.0.1:9092',
            'client.id': 'connection-test'
        })
        
        # ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì—°ê²° í™•ì¸)
        metadata = producer.list_topics(timeout=5)
        logger.info(f"âœ… Kafka ì—°ê²° ì„±ê³µ! ë¸Œë¡œì»¤ ìˆ˜: {len(metadata.brokers)}")
        
        # í† í”½ í™•ì¸
        check_kafka_topics()
        
    except Exception as e:
        logger.error(f"âŒ Kafka ì—°ê²° ì‹¤íŒ¨: {e}")
        logger.info("\ní•´ê²° ë°©ë²•:")
        logger.info("1. Kafka ì‹¤í–‰ í™•ì¸: docker-compose ps kafka")
        logger.info("2. í¬íŠ¸ í™•ì¸: lsof -i :9092")
        logger.info("3. Kafka ì¬ì‹œì‘: docker-compose restart kafka")


if __name__ == "__main__":
    main()